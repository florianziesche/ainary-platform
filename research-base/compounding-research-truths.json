{
  "version": "1.0",
  "generated": "2026-02-19T10:38:00Z",
  "last_updated": "2026-02-19T10:38:00Z",
  "truths": [
    {
      "id": "CT-001",
      "category": "calibration",
      "claim": "RLHF damage to calibration is regime-dependent, not absolute — calibratable vs non-calibratable regimes exist",
      "source": "ICML 2025, confirmed by Frontier Research Agent + Batch D Formalist",
      "confidence": 0.88,
      "verified_by": "frontier-research-agent + web_search",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["rlhf", "calibration", "basecal"]
    },
    {
      "id": "CT-002",
      "category": "calibration",
      "claim": "Self-consistency calibration reduces ECE from 42% to 27.3% in biomedical QA (PMC study, 9 models, 13 datasets)",
      "source": "PMC biomedical study, cited in AR-020-v3",
      "confidence": 0.85,
      "verified_by": "empiricist-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["consistency", "calibration", "ece", "biomedical"]
    },
    {
      "id": "CT-003",
      "category": "calibration",
      "claim": "Self-consistency cannot detect systematic bias — only reduces epistemic uncertainty component (~60-70% of miscalibration)",
      "source": "Batch D Formalist analysis §2.3",
      "confidence": 0.75,
      "verified_by": "formalist-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["consistency", "bias", "limitation"]
    },
    {
      "id": "CT-004",
      "category": "calibration",
      "claim": "ECE alone is insufficient as calibration metric — needs Brier Score + Reliability Diagram for completeness",
      "source": "Guo et al. 2017, confirmed by Formalist §2.4",
      "confidence": 0.92,
      "verified_by": "formalist-agent + literature",
      "date_added": "2026-02-19",
      "expiry_date": "2027-02-19",
      "status": "ACTIVE",
      "tags": ["ece", "metrics", "brier-score"]
    },
    {
      "id": "CT-005",
      "category": "calibration",
      "claim": "Budget-CoCoA achieves consistency calibration at ~$0.005/check using 3 small-model calls",
      "source": "AR-020-v3, verified by Replicator",
      "confidence": 0.80,
      "verified_by": "replicator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["cost", "consistency", "budget-cocoa"]
    },
    {
      "id": "CT-006",
      "category": "calibration",
      "claim": "Prompting alone is insufficient for good calibration — fine-tuning on ~1000 graded examples outperforms baselines",
      "source": "Dossier paper b3bf4ca8 (2024)",
      "confidence": 0.85,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["prompting", "fine-tuning", "calibration"]
    },
    {
      "id": "CT-007",
      "category": "calibration",
      "claim": "APRICOT enables black-box LLM calibration using only model output — no logit access needed",
      "source": "Dossier paper 3c45d3c1 (2024)",
      "confidence": 0.82,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["apricot", "black-box", "calibration"]
    },
    {
      "id": "CT-008",
      "category": "calibration",
      "claim": "LLM-based guard models produce overconfident predictions and show significant miscalibration under jailbreak attacks",
      "source": "Dossier paper e2f0bc45 (2024), 9 guard models, 12 benchmarks",
      "confidence": 0.87,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["guard-models", "overconfidence", "safety"]
    },
    {
      "id": "CT-009",
      "category": "calibration",
      "claim": "Atypical Presentations Recalibration reduces calibration errors by ~60% in medical QA, outperforming vanilla and CoT verbalized confidence",
      "source": "Dossier paper 8d978805 (2024)",
      "confidence": 0.80,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["medical", "recalibration", "domain-specific"]
    },
    {
      "id": "CT-010",
      "category": "calibration",
      "claim": "Auxiliary models outperform LLMs' internal probabilities and verbalized confidences for calibration",
      "source": "Dossier paper 3ae1d0fd (2025), 12 LLMs, 4 prompt styles",
      "confidence": 0.83,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["auxiliary-models", "calibration", "calib-n"]
    },
    {
      "id": "CT-011",
      "category": "calibration",
      "claim": "Confidence scores help calibrate human trust in AI, but trust calibration alone is insufficient to improve AI-assisted decision making",
      "source": "Dossier paper 5cc4100a (2020), human experiments",
      "confidence": 0.88,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2027-02-19",
      "status": "ACTIVE",
      "tags": ["human-trust", "decision-making", "confidence"]
    },
    {
      "id": "CT-012",
      "category": "calibration",
      "claim": "Current UQ practices for LLMs are not optimal for human users — community should adopt human-centered approach",
      "source": "Dossier paper c6f2d538 (2025)",
      "confidence": 0.80,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["uncertainty-quantification", "human-centered", "calibration"]
    },
    {
      "id": "CT-013",
      "category": "multi-agent",
      "claim": "For positively correlated agents, multiplicative confidence propagation systematically underestimates compound confidence",
      "source": "Batch D Formalist §2.2 (UNVERIFIED PROOF)",
      "confidence": 0.70,
      "verified_by": "formalist-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "NEEDS_VERIFICATION",
      "tags": ["multi-agent", "propagation", "correlation"]
    },
    {
      "id": "CT-014",
      "category": "multi-agent",
      "claim": "MLAs introduce critical trustworthiness challenges beyond traditional LLMs — multi-step execution involves nonlinear risk accumulation",
      "source": "MLA-Trust benchmark, Dossier paper 681714a9 (2025)",
      "confidence": 0.85,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["multi-agent", "trustworthiness", "mla-trust"]
    },
    {
      "id": "CT-015",
      "category": "market",
      "claim": "Calibration is a regulatory vacuum — EU AI Act Art. 15 requires 'accuracy metrics' but the word 'calibration' never appears",
      "source": "Batch D Regulator §1.1, verified against Official Journal",
      "confidence": 0.92,
      "verified_by": "regulator-agent + official-journal",
      "date_added": "2026-02-19",
      "expiry_date": "2027-02-19",
      "status": "ACTIVE",
      "tags": ["regulation", "eu-ai-act", "vacuum"]
    },
    {
      "id": "CT-016",
      "category": "market",
      "claim": "CEN/CENELEC harmonized standards expected 2027-2028 will define 'accuracy' technically — window to shape standards is NOW",
      "source": "Batch D Regulator §1.3",
      "confidence": 0.75,
      "verified_by": "regulator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["regulation", "standards", "opportunity"]
    },
    {
      "id": "CT-017",
      "category": "market",
      "claim": "No US federal law mandates AI confidence disclosure — NIST AI RMF 1.0 recommends but doesn't require UQ",
      "source": "Batch D Regulator §1.4",
      "confidence": 0.88,
      "verified_by": "regulator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["regulation", "us", "nist"]
    },
    {
      "id": "CT-018",
      "category": "market",
      "claim": "Full-stack calibration costs <$0.05/decision for single-turn — but multi-step agent workflows multiply this significantly",
      "source": "AR-020-v3 Exhibit 2, critique by Replicator + Red Team",
      "confidence": 0.78,
      "verified_by": "replicator-agent + red-team-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["cost", "multi-step", "economics"]
    },
    {
      "id": "CT-019",
      "category": "market",
      "claim": "The Three-Tier Architecture (Entropy → Consistency → Conformal) is a research synthesis, not an implementation guide — significant code gaps remain",
      "source": "Batch A Replicator assessment",
      "confidence": 0.85,
      "verified_by": "replicator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["implementation", "three-tier", "gaps"]
    },
    {
      "id": "CT-020",
      "category": "regulatory",
      "claim": "AI Agents are NOT a separate category in EU AI Act — classification depends on deployment domain, not technology",
      "source": "Batch D Regulator §1.2",
      "confidence": 0.90,
      "verified_by": "regulator-agent + official-journal",
      "date_added": "2026-02-19",
      "expiry_date": "2027-02-19",
      "status": "ACTIVE",
      "tags": ["regulation", "eu-ai-act", "classification"]
    },
    {
      "id": "CT-021",
      "category": "regulatory",
      "claim": "EU AI Act Article 14 (Human Oversight) functionally requires confidence signals — calibration is a de facto prerequisite",
      "source": "Batch D Regulator §1.1",
      "confidence": 0.78,
      "verified_by": "regulator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["regulation", "human-oversight", "article-14"]
    },
    {
      "id": "CT-022",
      "category": "regulatory",
      "claim": "ISO 42001:2023 requires accuracy monitoring process but does not define calibration technically — gap exists",
      "source": "Batch D Regulator §1.5",
      "confidence": 0.82,
      "verified_by": "regulator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2027-02-19",
      "status": "ACTIVE",
      "tags": ["iso", "standards", "gap"]
    },
    {
      "id": "CT-023",
      "category": "regulatory",
      "claim": "EU AI Act enforcement begins August 2026 for High-Risk (Annex III) and Transparency (Art. 50)",
      "source": "Batch D Regulator §1.3, EC timeline",
      "confidence": 0.95,
      "verified_by": "regulator-agent + official-journal",
      "date_added": "2026-02-19",
      "expiry_date": "2026-09-19",
      "status": "ACTIVE",
      "tags": ["regulation", "timeline", "enforcement"]
    },
    {
      "id": "CT-024",
      "category": "regulatory",
      "claim": "Texas and California offer Safe Harbor for companies implementing NIST AI RMF or ISO 42001",
      "source": "Batch D Regulator §1.4",
      "confidence": 0.80,
      "verified_by": "regulator-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["regulation", "us", "safe-harbor"]
    },
    {
      "id": "CT-025",
      "category": "calibration",
      "claim": "LLM sycophancy influences user trust — complimentary stance adaptation reduces perceived authenticity while neutral adaptation enhances trust",
      "source": "Dossier paper 3d04e8a8 (2025)",
      "confidence": 0.82,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["sycophancy", "trust", "rlhf"]
    },
    {
      "id": "CT-026",
      "category": "calibration",
      "claim": "Hallucination essence lies in absence of metacognition in LLMs — DMC framework separates metacognition from cognition",
      "source": "Dossier paper 5f591bfe (2025)",
      "confidence": 0.78,
      "verified_by": "prepare.py dossier extraction",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["hallucination", "metacognition", "dmc"]
    },
    {
      "id": "CT-027",
      "category": "multi-agent",
      "claim": "Compositionality of conformal prediction across dependent pipeline stages is an unsolved theoretical problem",
      "source": "Batch D Formalist §2.5 Problem 4",
      "confidence": 0.85,
      "verified_by": "formalist-agent",
      "date_added": "2026-02-19",
      "expiry_date": "2026-08-19",
      "status": "ACTIVE",
      "tags": ["conformal-prediction", "compositionality", "open-problem"]
    }
  ],
  "corrections": [
    {
      "id": "CX-001",
      "wrong": "No framework exists for multi-agent confidence propagation",
      "right": "SAUP (ACL 2025) and HTC (Jan 2026) address multi-agent propagation — partial solutions exist but don't compose across organizational boundaries",
      "source": "Frontier Research Agent + Red Team Finding #4",
      "date_added": "2026-02-19",
      "severity": "CRITICAL"
    },
    {
      "id": "CX-002",
      "wrong": "ECE of 27.3% for consistency calibration is a universal result",
      "right": "The 27.3% ECE figure is from biomedical QA only (PMC study). Domain-specific — cross-domain generalization unverified",
      "source": "Red Team Finding #1 (CRITICAL)",
      "date_added": "2026-02-19",
      "severity": "CRITICAL"
    },
    {
      "id": "CX-003",
      "wrong": "Enterprise hallucination losses are $67.4B",
      "right": "$67.4B figure is from AllAboutAI — single non-peer-reviewed source with no disclosed methodology. Use verified case studies (Mata v. Avianca, Air Canada) instead",
      "source": "Red Team Finding #3 (CRITICAL)",
      "date_added": "2026-02-19",
      "severity": "CRITICAL"
    },
    {
      "id": "CX-004",
      "wrong": "The Three-Tier Architecture is implementable on Monday morning by any engineer",
      "right": "Only Tier 3 (threshold routing) is trivially implementable. Tier 1 needs 1-2 days + ML experience. Tier 2 (conformal) requires statistician. Tier 1.5 (SAUP/HTC) is research-grade only",
      "source": "Replicator assessment",
      "date_added": "2026-02-19",
      "severity": "MAJOR"
    },
    {
      "id": "CX-005",
      "wrong": "Budget-CoCoA costs $0.005 per check",
      "right": "Actual cost with Haiku pricing (~$0.80/MTok) for 3 calls × 200 tokens ≈ $0.0005 — report estimate is ~10x too high (or assumes longer prompts, needs specification)",
      "source": "Replicator cost verification",
      "date_added": "2026-02-19",
      "severity": "MAJOR"
    },
    {
      "id": "CX-006",
      "wrong": "Self-consistency meta-calibration (5-prompt agreement) validates claim correctness",
      "right": "5-prompt self-consistency is epistemically circular — validates consistency-based calibration using consistency. Agreement rates likely inflated by ~10-20%",
      "source": "Red Team Finding #8 + Empiricist Experiment 3",
      "date_added": "2026-02-19",
      "severity": "MAJOR"
    },
    {
      "id": "CX-007",
      "wrong": "ECE < ε is sufficient for good calibration",
      "right": "ECE is necessary but not sufficient. Requires additionally: Sharpness (variance of confidence distribution) and Resolution (high confidence → higher accuracy). Brier Score combines all three",
      "source": "Formalist §2.4, Guo et al. 2017",
      "date_added": "2026-02-19",
      "severity": "MAJOR"
    }
  ]
}
