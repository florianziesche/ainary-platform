# Intelligence Brief â€” 13. Feb 2026

**Sources:** 5 articles from AI Supremacy, Latent Space, Lenny's Newsletter, Decision Intelligence, blogwatcher scan

---

## ðŸ”´ MUST-KNOW

**China's Open-Source Blitz Reshapes Competitive Landscape**  
GLM-5 (744B/40B active) just dropped as new SOTA open-weights model, beating closed alternatives on Intelligence Index (score 50) and claiming lowest hallucination rate. Zhipu AI explicitly GPU-starved, delaying rollout. Pattern: DeepSeek architecture (MLA + sparse attention) now powers "almost every frontier open LLM" â€” cost advantage 5-10x vs Western closed models for comparable quality. Implication for VC thesis: Open-source moat erosion accelerating, "operator-friendly AI" positioning window narrowing fast.

---

## ðŸ“Œ WORTH-KNOWING

**1. OpenAI/Cerebras GPT-5.3-Codex-Spark: 1000 tok/s Coding**  
4 weeks from partnership to production model. "Smaller" than GPT-5.3-Codex (128K context, text-only) but **10x faster** â€” enables "flow state" iterative coding. Simon Willison demo: complex SVG in seconds. Relevance: "Agentic coding" becomes practical for real-time workflows, not just batch tasks. Pricing TBA but expect premium for speed.

**2. Jeff Dean on AI Pareto Frontier Strategy (Latent Space)**  
Google's Chief AI Scientist on why you need BOTH frontier (Pro) and low-latency (Flash) models: distillation lets Flash_n+1 match Pro_n capability. Key insight: **energy (picojoules) > FLOPs** as bottleneck â€” moving data costs 1000x more than compute. Flash economic dominance (50T tokens) proves low-latency unlocks use cases, not just reduces cost. VC angle: Betting on "Flash-class moat" = betting on distillation IP, not raw model scale.

**3. Judgment Is the New Bottleneck (Cassie Kozyrkov)**  
"Imagine AI was perfect â€” what would you learn?" Answer: Decision-making. As tools hit "speed of thought," poor judgment amplifies faster. Traditional education trained "navigate fixed problems"; real decisions are messy, ambiguous. Risk: Leaders hide behind "send me a model output" instead of owning the question. Opportunity: "Decision intelligence" as competitive advantage when execution commoditizes. Meta-lesson: AI boom creates *adjacent* scarcity (steering skills, not compute).

---

## ðŸ’¡ CONTENT ANGLE

**"The Thoughtlessness Trap: Why Faster AI Demands Sharper Judgment"**  
Hook: Codex generates 1000 tokens/sec. Can you *think* at 1000 tok/s?  
Thesis: AI's promise = thoughtlessness enabler. You get results without effort. But "instant answer" â‰  "right answer" if you don't understand the question. As execution speed â†’ âˆž, judgment becomes the only moat.  
Payoff: Operators who master "wishing responsibly" (clear requirements, failure-mode anticipation, quality checks) win. Those who treat AI as magic lamp lose.  
CTA: "Before you prompt, ask: Do I know what I actually need? Because the genie won't."

---

**Next Brief:** Montag, 17. Feb 2026  
**Archive:** `memory/intelligence-briefs/`
