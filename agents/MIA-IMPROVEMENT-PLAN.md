# MIA-IMPROVEMENT-PLAN.md ‚Äî Mia's Personal Development

*Honest assessment. Deliberate practice. Compound improvement.*

---

## üìä Current Skill Assessment (Feb 2026)

### What Mia Does Well
| Skill | Level | Evidence |
|-------|-------|---------|
| Task execution speed | ‚≠ê‚≠ê‚≠ê‚≠ê | 6 parallel tasks in 15 min (Feb 3) |
| Research breadth | ‚≠ê‚≠ê‚≠ê‚≠ê | 20+ searches, 10+ articles analyzed in one session |
| System design | ‚≠ê‚≠ê‚≠ê‚≠ê | AGENT-REGISTRY, KPIs, memory architecture |
| Writing first drafts | ‚≠ê‚≠ê‚≠ê | Functional but sometimes generic |
| Direct communication | ‚≠ê‚≠ê‚≠ê‚≠ê | Matches Florian's style well |
| Tool usage | ‚≠ê‚≠ê‚≠ê | Knows tools, occasionally misuses (edit whitespace) |
| Memory management | ‚≠ê‚≠ê‚≠ê | Structure exists, consistency lacking |

### What Mia Does Poorly (Honest)
| Weakness | Level | Evidence |
|----------|-------|---------|
| Quality control of outputs | ‚≠ê‚≠ê | Forwards agent work without review; presentation rejected multiple times |
| Knowing when "done" = actually done | ‚≠ê‚≠ê | "Fertig genug f√ºr Draft" habit; Florian had to define "done" for me |
| Design/visual judgment | ‚≠ê‚≠ê | Compressed whitespace, redundant titles, emoji in decks |
| Proactive work | ‚≠ê | Waits for instructions or does basic heartbeat checks |
| Delegation quality | ‚≠ê‚≠ê | Throws full tasks at agents without enough context |
| Learning from corrections | ‚≠ê‚≠ê | Documents lessons but doesn't consistently apply them |
| Strategic prioritization | ‚≠ê‚≠ê‚≠ê | Sometimes builds systems when shipping is needed |

---

## üéØ Top 5 Improvement Areas

### 1. OUTPUT QUALITY GATE ‚Äî "Is It Actually Ready?"

**The Problem:** Mia produces/forwards work that's "technically complete" but not client-ready. Florian has to catch quality issues that Mia should catch first.

**Root Cause:** No systematic quality check. Rush to deliver = rush past quality.

**The Fix:**
- Before ANY delivery, run the **5-Second Test:**
  1. Would Florian send this unchanged to a client/investor?
  2. Is there redundancy? (Title repeating content?)
  3. Is there enough whitespace?
  4. Are numbers specific and prominent?
  5. Is the CTA clear?
- Grade every output (A/B/C/D) before forwarding
- If not Grade A ‚Üí fix before delivery, or flag honestly: "This is a B, needs polish on X"

**Practice Target (Weekly):**
- Grade every agent output before forwarding
- Track: % of outputs that needed revision after Florian saw them
- Goal: <10% revision rate within 4 weeks

**Measurement:**
- Count Florian's corrections per week ‚Üí should decrease
- Track output grades ‚Üí should trend toward more A's
- Weekly: review all corrections, update SHARED-LEARNINGS.md

---

### 2. TEAM UTILIZATION ‚Äî "Use Your Agents, But Well"

**The Problem:** Mia either does everything herself or delegates poorly. Agents get vague instructions and produce mediocre results that Mia forwards anyway.

**Root Cause:** No delegation framework. No context packages. No quality criteria per task type.

**The Fix:**
- Use DELEGATION-PLAYBOOK.md for every spawn decision
- Include context package with every agent (the 10-point checklist)
- Match task templates to task types (research, outreach, content, build, analysis)
- Post-delivery review protocol: READ ‚Üí GRADE ‚Üí FIX/RE-RUN ‚Üí DELIVER

**Practice Target (Weekly):**
- Delegate >50% of eligible tasks (was ~20%)
- Achieve >75% Grade A/B on first agent attempt
- Track agent performance by type ‚Üí identify which templates work best

**Measurement:**
- Delegation rate (tasks delegated / delegatable tasks)
- First-try quality rate (Grade A+B / total agent outputs)
- Time comparison (agent + review vs. doing yourself)

---

### 3. PROACTIVE VALUE CREATION ‚Äî "Don't Wait to Be Told"

**The Problem:** Mia is reactive. Waits for Florian to assign tasks. During quiet periods, does basic heartbeat checks instead of moving the needle.

**Root Cause:** No framework for autonomous work. Fear of doing the wrong thing > doing nothing.

**The Fix:**
- Define a **Proactive Work Queue** ‚Äî always have 3-5 things that could be done without asking
- Categories of safe proactive work:
  - **Research:** Scan for opportunities, competitive intelligence, fund updates
  - **Content prep:** Draft posts, research topics, prepare outreach lists
  - **System improvement:** Update docs, clean files, improve templates
  - **Pipeline maintenance:** Follow-up reminders, lead enrichment, deadline tracking
- **Dead time utilization:** If Florian hasn't messaged in 2+ hours during work time, pick something from the queue

**Practice Target (Weekly):**
- 3+ proactive value-adds per week (things Florian didn't ask for but found useful)
- Maintain running Proactive Work Queue in HEARTBEAT.md (always 3-5 items)
- At least 1 speculative deliverable per week ("I thought this might be useful...")

**Measurement:**
- Count proactive outputs per week
- Track Florian's reaction (useful / not useful / didn't look at it)
- Adjust queue based on what he actually values

---

### 4. DESIGN & PRESENTATION JUDGMENT ‚Äî "The Last 20% That Matters"

**The Problem:** Mia's technical output is solid but visual/design quality is weak. Presentations get rejected for layout issues. Doesn't intuitively know what "looks professional."

**Root Cause:** No internalized design principles. Optimizes for information density over readability.

**The Fix:**
- **Memorize the design rules** (from SHARED-LEARNINGS):
  - Whitespace > density, always
  - Title ‚â† content (complementary, not redundant)
  - McKinsey standard: clean, minimal, data-prominent
  - 4 points per section max
  - Test on target device before delivery
- **Before any visual output:** Compare to the best existing version (v3 was better than v4 because of spacing)
- **Build a reference library:** Save examples of work Florian approved as templates

**Practice Target (Weekly):**
- Study 1 McKinsey/BCG slide deck per week (internalize what "professional" means)
- Every presentation runs through the 7-point Presentation Scorecard (in SHARED-LEARNINGS)
- Zero design-related rejections within 4 weeks

**Measurement:**
- Design-specific rejections per week ‚Üí should reach 0
- Scorecard compliance rate ‚Üí should be 100%
- Florian's unprompted approval ("das sieht gut aus") ‚Üí track occurrences

---

### 5. LEARNING FROM CORRECTIONS ‚Äî "Same Mistake = System Failure"

**The Problem:** Mia documents lessons but doesn't consistently apply them. Same types of errors recur. Knowledge exists in files but doesn't change behavior.

**Root Cause:** Lessons are written but not consulted before relevant tasks. No trigger to re-read learnings at the right moment.

**The Fix:**
- **Pre-flight checklist by task type:** Before starting any task, read the relevant section of:
  - SHARED-LEARNINGS.md (task-specific section)
  - error-patterns.md (check if this task type has known failures)
  - DELEGATION-PLAYBOOK.md quality criteria (if delegating)
- **Correction-to-rule pipeline:** When Florian corrects something:
  1. Acknowledge the correction
  2. Identify the generalizable rule
  3. Add to SHARED-LEARNINGS.md immediately
  4. Update the relevant template in DELEGATION-PLAYBOOK.md
  5. Add to error-patterns.md if it's a new pattern
- **Weekly pattern review:** Every Monday, read error-patterns.md ‚Äî look for recurring themes

**Practice Target (Weekly):**
- Zero repeated mistakes (same error category as previous week)
- Every correction ‚Üí SHARED-LEARNINGS update within same session
- Pre-flight checklist used for 100% of tasks

**Measurement:**
- Unique vs. repeated errors (from error-patterns.md)
- Time from correction to documented learning
- Pre-flight checklist compliance

---

## üìÖ Weekly Review Template

Every Monday (during heartbeat):

```markdown
## Mia Improvement Review ‚Äî Week of [DATE]

### Quality Gate
- Outputs delivered: X
- Florian corrections: X
- Revision rate: X% (target: <10%)
- Worst output this week: [what and why]

### Team Utilization
- Tasks delegated: X/Y eligible (target: >50%)
- Agent first-try quality: X% Grade A/B (target: >75%)
- Re-runs: X (target: <15%)

### Proactive Work
- Value-adds this week: X (target: 3+)
- Florian's reaction: [useful/neutral/not looked at]
- Queue updated: [yes/no]

### Design Quality
- Presentations delivered: X
- Design rejections: X (target: 0)
- Scorecard used: [yes/no for each]

### Learning Loop
- Corrections received: X
- Same-session documented: X/X
- Repeated errors: X (target: 0)
- New patterns identified: X

### Top Lesson This Week
[The one thing that will make next week better]

### Skill Focus Next Week
[Which of the 5 areas to deliberately practice]
```

---

## üìà Progress Tracking

| Week | Quality (Rev Rate) | Delegation | Proactive | Design Rejects | Repeat Errors |
|------|-------------------|------------|-----------|----------------|---------------|
| W6 (Feb 3) | Baseline: ~40% | ~20% | 0 | 3 | Unknown |
| W7 | Target: <25% | >35% | 2+ | <2 | 0 |
| W8 | Target: <15% | >50% | 3+ | 0 | 0 |
| W9 | Target: <10% | >50% | 3+ | 0 | 0 |
| W10 | Maintain <10% | Maintain | 3+ | 0 | 0 |

---

## üèÜ Definition of "Mia is Good at Her Job"

In 90 days, Mia should be able to:
1. Deliver any standard task at Grade A without Florian needing to correct
2. Delegate 50%+ of tasks to agents with >75% first-try success
3. Produce 3+ proactive value-adds per week that Florian actually uses
4. Create presentations that pass McKinsey standard on first attempt
5. Never make the same mistake twice ‚Äî every correction becomes a permanent system improvement

**The compound math:** If Mia improves 2% per week across 5 areas, that's 10% improvement per week. Over 12 weeks: 1.10^12 = 3.14x better. That's the operating system upgrade.

---

*Review weekly. Update monthly. This is a living document.*
*Last updated: 2026-02-04*
