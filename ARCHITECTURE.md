# Architecture

**Ainary Platform â€” System Design & Technical Reference**

---

## System Overview

The Ainary Platform is a multi-project ecosystem built around a core thesis: **AI agents need accountability infrastructure.** The architecture separates library code (AgentTrust, Calibration), applications (Execution Platform), and knowledge generation (Research Pipeline).

```mermaid
graph TB
    subgraph "User Layer"
        USER[Florian<br/>Browser]
        OBSIDIAN[Obsidian Vault<br/>Knowledge Graph]
    end
    
    subgraph "Application Layer"
        EP[Execution Platform<br/>localhost:8080]
        RP[Research Pipeline<br/>CLI]
    end
    
    subgraph "Library Layer"
        AT[AgentTrust<br/>pip package]
        CAL[Calibration Library<br/>pip package]
    end
    
    subgraph "AI Layer"
        CLAUDE[Anthropic Claude<br/>Haiku/Sonnet/Opus]
        GPT[OpenAI GPT<br/>gpt-4o-mini]
    end
    
    subgraph "Data Layer"
        DB[(SQLite<br/>workbench.db)]
        VAULT[(Obsidian Vault<br/>memory/ symlink)]
        RESEARCH[(Research Reports<br/>markdown)]
    end
    
    USER --> EP
    USER --> OBSIDIAN
    EP --> AT
    EP --> DB
    EP --> CLAUDE
    EP --> GPT
    RP --> CAL
    RP --> CLAUDE
    RP --> RESEARCH
    AT --> CAL
    OBSIDIAN -.symlink.-> VAULT
    EP -.reads.-> VAULT
```

---

## Data Flow

### Example: Email Draft with Trust Scoring

```
1. Florian: "Draft email to Glasswing Ventures" (UI)
   â†“
2. Frontend â†’ POST /api/topics/{id}/messages (REST)
   â†“
3. Backend checks trust score for skill="email_drafts" â†’ 30/100
   â†“
4. Trust < 50 â†’ Guardrail: ðŸŸ¡ REVIEW (confirmation required)
   â†“
5. Frontend shows confirmation dialog
   â†“
6. Florian confirms â†’ Backend â†’ OpenClaw sessions_send()
   â†“
7. Mia generates draft using Claude Sonnet
   â†“
8. Quality Layer â†’ Pre-Flight Check:
   - âœ… No LLM phrases ("I'd be happy to...")
   - âœ… Solo founder voice (I, not We)
   - âš ï¸ Unverified claim: "$120K AUM" (no source)
   - Pre-Flight Score: 85/100 â†’ WARN
   â†“
9. Backend â†’ Frontend with metadata
   â†“
10. UI shows draft with:
    - Trust badge: ðŸŸ¡ 30
    - Pre-flight badge: 85
    - Warning: "Unverified claim in line 3"
   â†“
11. Florian edits: "$120K AUM" â†’ "$120K potential"
    â†“
12. Correction saved â†’ corrections table
    - type: "fact_verification"
    - pattern: "Do not state unverified financial figures"
    - severity: "high"
    â†“
13. Trust score updated: email_drafts 30 â†’ 31 (+1 for honest uncertainty)
    â†“
14. Florian clicks "Send" â†’ gog gmail send
    â†“
15. Event logged:
    - type: "email_sent"
    - topic_id: "glasswing"
    - timestamp: 2026-02-19T10:15:00Z
    â†“
16. WebSocket push â†’ UI updates activity feed
```

**Key properties:**
- Trust-based guardrails prevent unsafe autonomy
- Pre-flight checks catch errors before human review
- Corrections become permanent rules (never repeat the same mistake)
- Event log provides full audit trail
- Real-time updates via WebSocket

---

## Technology Stack

### Backend (Execution Platform)

| Component | Technology | Version | Why |
|-----------|-----------|---------|-----|
| Web Framework | FastAPI | 0.115+ | Async-native, auto-validation, fast dev |
| Database | SQLite | 3.45+ (WAL mode) | Zero-config, single-file, sufficient for single-user |
| ORM | None (raw SQL) | â€” | Direct control, no abstraction overhead |
| Real-time | WebSocket (FastAPI) | â€” | Native browser support, low latency |
| AI Backend | OpenClaw | â€” | Session management, tool orchestration |
| CV Generation | Headless Chrome | â€” | Pixel-perfect PDF rendering |
| HTTP Client | httpx | â€” | Async HTTP for AI API calls |

**Database Schema:** 27 tables. See [workbench/DB-SCHEMA.md](projects/workbench/DB-SCHEMA.md)

### Frontend (Execution Platform)

| Component | Technology | Why |
|-----------|-----------|-----|
| HTML/CSS/JS | Vanilla (single file) | No build step, sub-100ms interactions |
| State Management | Plain JS objects | No framework overhead |
| Real-time | WebSocket client | Native browser API |
| Rendering | Direct DOM manipulation | Fastest possible, no virtual DOM |
| Styling | Custom CSS + dot-grid | Consistent brand, no CSS framework |

**File:** `projects/workbench/index.html` (~40KB, self-contained)

### Libraries (AgentTrust + Calibration)

| Component | Technology | Why |
|-----------|-----------|-----|
| Language | Python 3.10+ | Type hints, dataclasses, ecosystem |
| Dependencies | NumPy (optional) | Minimal dependencies for portability |
| Packaging | setuptools | Standard Python packaging |
| Testing | pytest | Industry standard |
| Linting | ruff | Fast, comprehensive |

### AI Layer

| Provider | Models | Use Case | Cost |
|----------|--------|----------|------|
| **Anthropic** | Claude Haiku | Speed tasks (summarization, extraction) | $0.25/MTok in, $1.25/MTok out |
| **Anthropic** | Claude Sonnet | Synthesis, analysis, quality checks | $3/MTok in, $15/MTok out |
| **Anthropic** | Claude Opus | Final outputs, opinionated analysis | $15/MTok in, $75/MTok out |
| **OpenAI** | GPT-4o-mini | Fallback, cost-sensitive tasks | $0.15/MTok in, $0.60/MTok out |

**Multi-model routing:**
- Haiku: 60% of tasks (intake, extraction)
- Sonnet: 30% of tasks (synthesis, quality)
- Opus: 10% of tasks (final writing)
- **Cost reduction: 60-80% vs. pure GPT-4** (AR-017)

### Knowledge Layer

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Obsidian Vault | Markdown + YAML | Personal knowledge graph with bi-directional links |
| Memory Symlink | `memory/ â†’ Vault/70_Mia/` | Shared memory between Platform and Obsidian |
| Embeddings | (Planned) | Semantic search across vault |
| Search | (Current: keyword) | Fast grep-based search |

### Research Layer

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Semantic Scholar API | REST | Academic paper search + citation tracking |
| arXiv API | REST | Preprint access |
| Web Search | Brave Search API | General web research |
| Report Format | Markdown + HTML | Human-readable + web-publishable |

---

## Key Design Decisions

### 1. Localhost-First Architecture

**Decision:** Run everything on Florian's MacBook. No cloud until Phase 5.

**Rationale:**
- Privacy: All data (vault, conversations, documents) stays local
- Speed: No network latency
- Cost: Zero cloud infrastructure cost
- Simplicity: No auth, no multi-tenancy, no deployment complexity

**Trade-offs:**
- âŒ No mobile access (yet)
- âŒ No collaboration (single-user)
- âœ… Complete data ownership
- âœ… Sub-100ms interactions
- âœ… Works offline (navigation + context)

### 2. Single-File Frontend

**Decision:** `index.html` contains HTML + CSS + JavaScript in one file.

**Rationale:**
- No build step (open in browser = works)
- No dependency hell (React, Vue, etc.)
- Fast loading (40KB total, no bundler)
- Easy to audit (one file to read)
- Sub-100ms interactions (direct DOM, no virtual DOM)

**Trade-offs:**
- âŒ No component reusability (acceptable for single app)
- âŒ No hot module replacement (refresh = instant anyway)
- âœ… Zero dependencies
- âœ… Debuggable (view source = see everything)

### 3. Trust per Skill, Not per Agent

**Decision:** Track trust scores for skills (e.g., "email_drafts", "research") rather than agents (e.g., "Mia", "WRITER").

**Rationale:**
- Skills are stable; agent implementations change
- Trust transfers across rewrites (don't lose history when refactoring)
- More granular control (can trust for research but not for finance)
- Bayesian updates per skill with correlation tracking

**Implementation:** `trust_skills` table with alpha/beta parameters for Bayesian scoring.

### 4. Mandatory Metadata (Beipackzettel)

**Decision:** Every AI output must ship with confidence, sources, uncertainties, and risks.

**Rationale:**
- Transparency: User knows what the AI knows (and doesn't know)
- Accountability: AI can't hide uncertainty
- Calibration: Forces AI to quantify confidence
- Auditability: Full provenance trail

**Inspired by:** Drug package inserts (German: Beipackzettel). Same principle: mandatory disclosure.

### 5. Black-Box Calibration Only

**Decision:** Only implement calibration methods that work without logits.

**Rationale:**
- GPT-4: Top-5 logprobs only
- Claude: No logits exposed
- Gemini: Partial logprobs
- Gold standard technique (temperature scaling) is inaccessible

**Methods used:**
- Self-consistency (sample N times, measure agreement)
- Verbalized confidence (ask "how confident?")
- Conformal prediction (distribution-free guarantees)
- Selective prediction (abstain when uncertain)

### 6. Pre-Flight Quality Gates

**Decision:** Check every AI output against corrections + standards before showing to user.

**Rationale:**
- Catch errors early (before human review)
- Enforce consistency (same rules every time)
- Learn from mistakes (corrections become permanent)
- Reduce cognitive load (user sees only good outputs)

**Layers:**
- L1: Regex (fast, deterministic) â€” e.g., "contains LLM phrases?"
- L2: Structural (schema, logic) â€” e.g., "has sources?"
- L3: LLM (semantic, quality) â€” e.g., "is this persuasive?"

### 7. Multi-Model Routing

**Decision:** Route tasks to cheapest capable model (Haiku â†’ Sonnet â†’ Opus).

**Rationale:**
- Cost reduction: 60-80% savings (AR-017)
- Speed: Haiku is 5x faster than Opus
- Quality: Opus only for final outputs that matter

**Routing logic:**
- Extraction, summarization â†’ Haiku
- Synthesis, analysis â†’ Sonnet
- Final writing, opinionated analysis â†’ Opus

---

## Security & Privacy

### Data Sensitivity

| Data Type | Location | Sensitivity | Backup |
|-----------|----------|-------------|--------|
| Conversations | workbench.db | HIGH | Time Machine + manual |
| Vault | Obsidian Vault (outside repo) | VERY HIGH | iCloud + Time Machine |
| Corrections | workbench.db | MEDIUM | Same as conversations |
| Trust scores | workbench.db | LOW | Same as conversations |
| Research reports | Git (private repo) | LOW | GitHub (private) |

### Access Control

- **Local only:** No remote access until Phase 5
- **No auth:** Single-user, trusted environment
- **API keys:** Stored in `.env` (not in Git)
- **Git:** Private repo, no public access

### Compliance

- **GDPR:** N/A (single-user, no external data)
- **AI Act:** Research-phase monitoring (AR-019)
- **Export Control:** N/A (no sensitive AI models)

---

## Performance Characteristics

### Response Times (p95)

| Operation | Target | Actual (v0.13.0) | Bottleneck |
|-----------|--------|------------------|------------|
| Page load | <500ms | ~200ms | Static file |
| Topic switch | <100ms | ~50ms | SQLite query |
| AI response (Haiku) | <2s | ~1.5s | API latency |
| AI response (Sonnet) | <5s | ~3s | API latency |
| AI response (Opus) | <10s | ~7s | API latency |
| WebSocket update | <50ms | ~20ms | Network |
| Pre-flight check (L1) | <10ms | ~5ms | Regex |
| Pre-flight check (L2) | <50ms | ~30ms | JSON validation |
| Pre-flight check (L3) | <3s | ~2s | LLM call |

### Database Performance

- **Indexes:** 7 key indexes (Phase A, v0.13.0)
- **Query time (p95):** <10ms for indexed lookups
- **Write throughput:** ~1000 inserts/sec (SQLite WAL mode)
- **Database size:** ~600KB (25 topics, 200 messages, 100 findings)

### Cost Per Month (Target: $200)

| Service | Usage | Cost |
|---------|-------|------|
| Anthropic API | ~150 reports + 500 conversations | ~$150 |
| OpenAI API | Fallback (~10% of tasks) | ~$20 |
| GitHub (private repo) | 1 user | $0 (free tier) |
| **Total** | | **~$170** |

**Headroom:** $30/month for experiments.

---

## Deployment

### Current (Phase 1-3)

```bash
# Auto-start on boot via launchd
launchctl start com.ainary.workbench

# Manual start
cd projects/workbench/backend
python3 app.py
```

**Process management:** macOS launchd (auto-restart on crash)

### Future (Phase 5)

- **Option 1:** Tailscale + Cloudflare Tunnel (private access)
- **Option 2:** Fly.io (single-instance, EU region)
- **Option 3:** AWS Lightsail (if need GPU for embeddings)

**Decision deferred** until remote access is needed.

---

## Directory Structure

```
ainary-platform/
â”‚
â”œâ”€â”€ projects/                      # 16 projects
â”‚   â”œâ”€â”€ agenttrust/               # Trust infrastructure library
â”‚   â”‚   â”œâ”€â”€ agenttrust/           # Python package
â”‚   â”‚   â”œâ”€â”€ tests/                # pytest suite
â”‚   â”‚   â”œâ”€â”€ README.md             # Documentation
â”‚   â”‚   â””â”€â”€ setup.py              # Package config
â”‚   â”‚
â”‚   â”œâ”€â”€ workbench/                # Execution Platform
â”‚   â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py            # FastAPI server
â”‚   â”‚   â”‚   â”œâ”€â”€ cv_generator.py   # CV engine
â”‚   â”‚   â”‚   â””â”€â”€ workbench.db      # SQLite database
â”‚   â”‚   â”œâ”€â”€ index.html            # Frontend (single file)
â”‚   â”‚   â”œâ”€â”€ uploads/              # User-uploaded files
â”‚   â”‚   â”œâ”€â”€ CHANGELOG.md          # Version history
â”‚   â”‚   â”œâ”€â”€ DB-SCHEMA.md          # Database docs
â”‚   â”‚   â””â”€â”€ DOCUMENTATION.md      # API reference (84 endpoints)
â”‚   â”‚
â”‚   â”œâ”€â”€ ainary-calibration/       # Calibration library
â”‚   â”‚   â”œâ”€â”€ ainary_calibration/   # Python package
â”‚   â”‚   â”‚   â”œâ”€â”€ consistency.py    # Self-consistency methods
â”‚   â”‚   â”‚   â”œâ”€â”€ propagation.py    # Multi-agent propagation
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.py        # ECE, Brier, etc.
â”‚   â”‚   â”‚   â””â”€â”€ experiments/      # 4 experiments
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ research-pipeline/        # Automated research
â”‚       â”œâ”€â”€ pipeline.py           # Main pipeline
â”‚       â”œâ”€â”€ prepare.py            # Paper fetching (coming soon)
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ research/                     # Published reports
â”‚   â”œâ”€â”€ AR-016-agent-memory-architecture-2026.md
â”‚   â”œâ”€â”€ AR-020-trust-calibration-methods.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ standards/                    # Production standards
â”‚   â”œâ”€â”€ RESEARCH-PROTOCOL.md
â”‚   â”œâ”€â”€ CONTENT-VOICE.md
â”‚   â”œâ”€â”€ Q1-BUILD-VERIFY.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ memory/                       # Symlink â†’ Obsidian Vault/70_Mia/
â”‚   â””â”€â”€ (symlinked, not in Git)
â”‚
â”œâ”€â”€ AGENTS.md                     # Agent system rules
â”œâ”€â”€ SOUL.md                       # System identity
â”œâ”€â”€ ARCHITECTURE.md               # This file
â”œâ”€â”€ README.md                     # Project overview
â””â”€â”€ .gitignore
```

---

## Testing Strategy

### Unit Tests
- **AgentTrust:** pytest suite, 85% coverage
- **Calibration Library:** 4 experiments = smoke tests

### Integration Tests
- **Execution Platform:** Manual QA (Q1-BUILD-VERIFY standard)
- **Research Pipeline:** Generated reports reviewed by human

### Quality Gates
- Pre-flight checks (L1/L2/L3)
- Self-audit at end of every sub-agent task
- Standards compliance before any external send

**Philosophy:** Trust but verify. AI generates, human validates, system learns.

---

## Future Architecture (Phase 4+)

### Planned Enhancements

1. **Embeddings + Semantic Search**
   - Index Obsidian vault with sentence-transformers
   - Retrieve relevant context automatically
   - Reduce manual "grep vault" searches

2. **Multi-User (Phase 5)**
   - Auth: OAuth2 (Google/GitHub)
   - Multi-tenancy: Separate databases per user
   - Deployment: Fly.io or AWS Lightsail

3. **Agent Framework Integration**
   - LangChain middleware for AgentTrust
   - CrewAI plugin
   - AutoGen compatibility layer

4. **Active Learning for Calibration**
   - Collect user feedback on confidence
   - Retrain conformal prediction sets
   - Adaptive thresholds per domain

5. **Mobile Access**
   - Progressive Web App (PWA)
   - Read-only mobile view
   - Push notifications for pending actions

---

## Related Documents

- **[Root README](README.md)** â€” Project overview
- **[Workbench README](projects/workbench/README.md)** â€” Execution Platform details
- **[AgentTrust README](projects/agenttrust/README.md)** â€” Trust library API
- **[Calibration README](projects/ainary-calibration/README.md)** â€” Calibration methods
- **[DB Schema](projects/workbench/DB-SCHEMA.md)** â€” Database design
- **[API Docs](projects/workbench/DOCUMENTATION.md)** â€” 84 endpoints

---

**Last Updated:** 2026-02-19  
**Maintainer:** Florian Ziesche
