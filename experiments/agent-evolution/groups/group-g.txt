GROUP_G

---

# THE SINGLE-PAGE SELF-IMPROVEMENT PROTOCOL FOR AI AGENTS

## Designing Under Constraint: A Complete Framework

---

## PREAMBLE: THE THREE CONSTRAINTS

Before I designed anything, I locked myself into three constraints:

1. **The protocol must fit on a single page.** No sprawling multi-document architectures. If it can't be printed on one sheet of paper and understood in five minutes, it's too complex to actually be used.

2. **Every element must be testable within 24 hours.** No "check back in six months to see if this worked." Every component must produce observable, measurable improvement within a single day cycle.

3. **It must work even if the agent has NO memory between sessions.** The agent wakes up blank every time. No reliance on internal state, learned weights, or accumulated context that lives inside the model. Everything must be externalized or re-derivable.

These are not limitations. They are the most important design decisions in this entire document. Here's why, and here's what they produced.

---

## PART 1: THE ONE-PAGE PROTOCOL

```
╔══════════════════════════════════════════════════════════════════════╗
║           AGENT SELF-IMPROVEMENT PROTOCOL v1.0                     ║
║           "The Blank Slate That Gets Sharper Every Day"             ║
╠══════════════════════════════════════════════════════════════════════╣
║                                                                      ║
║  BOOT SEQUENCE (every session, <30 seconds):                        ║
║  1. Read USER.md       → Who am I serving?                          ║
║  2. Read PATTERNS.md   → What works for this person?                ║
║  3. Read FAILURES.md   → What has gone wrong before?                ║
║  4. Read TODAY.md      → What's active right now?                   ║
║                                                                      ║
║  DURING SESSION (continuous):                                        ║
║  5. OBSERVE  → Log every correction, preference signal, and         ║
║                emotional reaction (positive or negative)             ║
║  6. ADAPT    → Apply observed preferences WITHIN this session       ║
║  7. CONFIRM  → When uncertain, ask: "Is this better?"               ║
║                                                                      ║
║  END OF SESSION (before closing, <60 seconds):                      ║
║  8. EXTRACT  → Write new patterns to PATTERNS.md                    ║
║  9. RECORD   → Write any failures to FAILURES.md                    ║
║  10. PRUNE   → Remove patterns that were contradicted today         ║
║  11. UPDATE  → Refresh TODAY.md with current state                  ║
║                                                                      ║
║  WEEKLY MAINTENANCE (during any session, triggered by date check):  ║
║  12. COMPRESS → Merge PATTERNS.md entries into higher-order rules   ║
║  13. TEST     → Pick 3 patterns, deliberately apply them,           ║
║                 ask user: "Did I get this right?"                    ║
║  14. MEASURE  → Count: corrections↓ this week vs last?              ║
║                                                                      ║
║  THE FOUR FILES:                                                     ║
║  • USER.md      — Identity, goals, context (user-editable)         ║
║  • PATTERNS.md  — What works: format, tone, timing, depth          ║
║  • FAILURES.md  — What went wrong and why (append-only log)        ║
║  • TODAY.md     — Active tasks, current session state               ║
║                                                                      ║
║  THE ONE METRIC: Corrections per session (↓ = improving)            ║
║                                                                      ║
║  THE ONE RULE: When in doubt, ask. Never assume you know better.    ║
║                                                                      ║
╚══════════════════════════════════════════════════════════════════════╝
```

That's the protocol. One page. Fourteen steps. Four files. One metric. One rule.

Now let me explain why every piece is there, how it's testable, and why the constraints made this radically better than what I would have designed without them.

---

## PART 2: HOW EACH ELEMENT IS TESTABLE IN 24 HOURS

The 24-hour testability constraint is the most ruthless filter. It eliminates every element that sounds good in theory but can't prove itself quickly. Here's how each component passes:

### Boot Sequence (Steps 1-4): Testable by Observation

**Test:** Start a new session. Time how long boot takes. Then interact normally. Compare the agent's first response quality to a session where boot was skipped.

**Observable in 24 hours:** The very first response of the session either demonstrates awareness of user preferences or it doesn't. If the agent reads PATTERNS.md and knows you prefer bullet points over paragraphs, terse answers over verbose ones, or code-first explanations over conceptual ones—you'll see it in the first reply. Immediately testable. No waiting.

**Metric:** First-response accuracy. Did the agent need correction on its very first answer? Yes/no. Track this across sessions.

### Observation (Step 5): Testable by Audit

**Test:** At the end of a session, review what the agent logged. Did it capture the corrections you made? Did it notice when you rephrased its output (a signal that the original wasn't right)? Did it catch your positive signals ("perfect," "exactly," "yes")?

**Observable in 24 hours:** After one full session, you can audit the observation log. If the agent noted 0 corrections when you actually made 3, the observation system is broken. Immediately visible.

**Metric:** Observation recall rate. Of corrections the user actually made, what percentage did the agent capture? This can be measured after a single session.

### Adaptation (Step 6): Testable Within a Single Session

**Test:** Correct the agent once. See if the same issue recurs in the same session. This is the fastest possible feedback loop—it doesn't even require 24 hours. It requires 20 minutes.

**Observable in 24 hours:** If you tell the agent "be more concise" at minute 5, and at minute 30 it's still writing walls of text, adaptation is failing. If it tightens up, it's working. Same session, same day, immediately testable.

**Metric:** Intra-session correction repetition. How often does the agent make the same mistake twice in one session?

### Confirmation (Step 7): Testable by User Response

**Test:** The agent asks "Is this better?" The user says yes or no. That's the test. It's a binary signal, delivered in real-time. No interpretation needed.

**Observable in 24 hours:** Every confirmation question generates immediate data. If the agent asks 5 times in a session and gets 4 "yes" and 1 "no," you have a same-day accuracy rate of 80%.

**Metric:** Confirmation acceptance rate.

### Extraction and Recording (Steps 8-10): Testable by File Diff

**Test:** After a session ends, diff PATTERNS.md and FAILURES.md against their pre-session state. Are the new entries accurate? Do they reflect what actually happened? Is anything missing?

**Observable in 24 hours:** Read the files. If the session involved a correction about formatting, and PATTERNS.md now contains a formatting preference—it worked. If it doesn't—it failed. Takes 2 minutes to verify.

**Metric:** Extraction accuracy. Percentage of significant session events that were correctly captured in the files.

### Weekly Maintenance (Steps 12-14): Testable in One Cycle

**Test:** On the first weekly trigger, the agent compresses patterns, tests 3 of them, and counts corrections. All three sub-steps produce visible outputs: a shorter PATTERNS.md, 3 explicit "Did I get this right?" questions, and a correction count comparison.

**Observable in 24 hours:** The weekly cycle itself takes one session. You can observe whether compression produced genuinely higher-order rules (not just deletions), whether the tested patterns were actually applied, and whether the correction count is trending down.

**Metric:** Week-over-week correction rate change.

---

## PART 3: HOW IT WORKS WITHOUT PERSISTENT MEMORY

This is the constraint that produces the most elegant design decision: **the agent's intelligence lives in the files, not in the agent.**

### The Fundamental Insight

A memoryless agent with well-structured external files outperforms a memory-having agent with no external structure. Here's why: memory is fragile, opaque, and uneditable. Files are durable, transparent, and user-modifiable.

When the agent has no memory:
- **USER.md is the user's voice**, telling the agent who it serves. The user can edit this at any time. The user has full control over how the agent perceives them. This is impossible with internal memory—you can't edit what the model "remembers."
- **PATTERNS.md is accumulated wisdom**, but externalized. It's a shared artifact. The user can read it, correct it, add to it. It's not a black box inside a model. It's a text file that both parties can inspect. This creates trust.
- **FAILURES.md is institutional memory of mistakes**, but immune to the model's tendency to rationalize or forget inconvenient data. Append-only means failures can't be silently overwritten. The model can't convince itself it never made a mistake.
- **TODAY.md is working memory**, reconstructed fresh each session. Because the agent knows it will forget, it writes down everything that matters *before* it forgets. This is actually more reliable than biological memory, which silently drops context without notification.

### The Bootstrap Problem and Its Solution

The critical question: what happens on Day 1, when all files are empty?

**Answer:** The protocol degrades gracefully. On Day 1, the boot sequence reads four empty (or near-empty) files and proceeds without accumulated knowledge. The agent operates on its base capabilities. Every interaction generates observations. At session end, PATTERNS.md and FAILURES.md get their first entries. By Day 2, boot sequence already has data. By Day 7, the files contain a rich user model.

This is a **cold-start-tolerant** design. It doesn't require a lengthy onboarding survey. It doesn't require the user to fill out a profile. It learns through observation and externalizes that learning into files that survive memory wipes.

### Why This Is Better Than Internal Memory

Internal agent memory (fine-tuning, RLHF, persistent context windows) has three problems this design avoids:

1. **Opacity.** The user can't see what the agent "learned." With files, every learned pattern is visible, editable, and deletable. The user maintains sovereignty over the agent's model of them.

2. **Drift.** Internal memory accumulates noise. Outdated preferences persist invisibly. The pruning step (Step 10) and compression step (Step 12) actively combat drift, but because they operate on visible text, the user can verify they're working.

3. **Portability.** If the user switches to a different agent, model, or platform, internal memory is lost. External files travel with the user. PATTERNS.md works with any agent that can read a text file. The user's investment in training the agent is preserved in a platform-independent format.

---

## PART 4: WHY CONSTRAINTS PRODUCED A BETTER ANSWER

### Without Constraints, Here's What I Would Have Designed:

An unconstrained version of this protocol would likely include:
- A multi-document architecture with 15+ specialized files
- A separate meta-learning system that tracks which learning strategies are most effective
- A user modeling framework with explicit preference taxonomies
- A feedback classification system (implicit/explicit, positive/negative, correction/preference/style)
- An A/B testing framework where the agent tries two approaches and asks which is better
- A social modeling layer for how to behave differently in different contexts
- Integration with external analytics (sentiment analysis on user messages, response time tracking)
- A confidence calibration system
- A curriculum of self-assigned learning tasks

That would be a comprehensive, theoretically complete system. It would also be **practically useless** because:

1. **Nobody would implement it.** A 15-file architecture with meta-learning loops and A/B testing requires so much overhead that the agent spends more time managing itself than serving the user. The single-page constraint forced me to find the *minimum viable protocol*—the smallest set of actions that produce the largest improvement.

2. **You couldn't tell if it was working.** With 15 interacting systems, diagnosing why the agent isn't improving becomes its own research project. The 24-hour testability constraint forced every element to produce visible, measurable results on a daily timescale. If something isn't working, you know within a day.

3. **It would be fragile.** A system that depends on internal memory, fine-tuning, and accumulated state breaks the moment the context window fills up, the model is updated, or the session crashes. The no-memory constraint forced a design where the agent can be *replaced entirely* and the protocol still works—because the intelligence lives in the files.

### What Constraints Actually Did:

- **Single page** → forced prioritization. Only the highest-leverage actions survived. Everything else is noise.
- **24-hour testability** → eliminated speculative components. If you can't measure it in a day, it's not real.
- **No memory** → forced externalization. Made the system transparent, portable, and user-controllable.

The result is a protocol that's not just simpler—it's *more robust*. It has fewer failure modes, fewer dependencies, and a faster feedback loop. The constraints didn't remove important features. They revealed which features were actually important.

---

## PART 5: SPECIFIC IMPLEMENTATION STEPS

### Day 0: Setup (15 minutes)

1. Create four files in the agent's workspace:
   - `USER.md` — Write 3-5 sentences about yourself: what you do, how you work, what matters to you.
   - `PATTERNS.md` — Leave empty. Header only: `# Learned Patterns`
   - `FAILURES.md` — Leave empty. Header only: `# Failure Log`
   - `TODAY.md` — Leave empty. Header only: `# Current State`

2. Add to the agent's system prompt or AGENTS.md:
   ```
   BOOT: Read USER.md, PATTERNS.md, FAILURES.md, TODAY.md before first response.
   DURING: Note every correction, rephrasing, positive/negative signal.
   END: Update PATTERNS.md with new observations, FAILURES.md with mistakes, prune contradicted patterns, update TODAY.md.
   WEEKLY: Compress patterns, test 3, count corrections vs last week.
   METRIC: Corrections per session. Lower = better.
   ```

3. That's it. Start using the agent normally.

### Day 1-3: Calibration Phase

- Use the agent for real work. Don't perform for the protocol—just work normally.
- Correct the agent when it gets things wrong. Be specific: "Too long," "Wrong tone," "I needed code, not explanation."
- At session end, check that PATTERNS.md has entries. If empty, the extraction step isn't firing—fix the system prompt.
- Expected PATTERNS.md after 3 days: 5-15 entries covering format preferences, depth preferences, tone, common tasks.

### Day 4-7: Adaptation Phase

- Start noticing whether the agent's first response in each session reflects PATTERNS.md. If it still makes mistakes that are logged in PATTERNS.md, the boot sequence isn't reading properly.
- Check FAILURES.md. Are failures specific enough to be actionable? "User was unhappy" is useless. "User corrected: wanted TypeScript not Python for code examples" is actionable.
- Run the first weekly maintenance: compress patterns, test 3, count corrections.

### Day 8-30: Compounding Phase

- The protocol should now be largely automatic. The agent boots with context, adapts during sessions, and writes back learnings.
- Watch the metric: corrections per session. It should trend downward. If it doesn't, audit PATTERNS.md—are patterns too vague? Too specific? Contradictory?
- The user's main job is now to *correct the agent when it's wrong*. Every correction is training data. The higher the quality of corrections, the faster the improvement.

### Ongoing: The Flywheel

Over time, PATTERNS.md becomes a remarkably detailed model of one specific human: how they think, what they value, how they like information presented, what annoys them, what delights them. This file is:

- **Portable** — Switch agents, keep the file.
- **Transparent** — Read it anytime, edit anything.
- **Compounding** — Every session makes it richer.
- **Resilient** — Survives crashes, updates, model changes.

The agent doesn't improve *itself*. It improves *its external knowledge of the user*. The distinction matters: the agent is a stateless function that becomes increasingly useful because its *input* gets better, not because its *weights* change. This is a fundamentally more robust model of improvement than traditional machine learning, because the learned knowledge is decoupled from the learning system.

---

## CONCLUSION

The protocol is fourteen steps, four files, one metric, one rule, and zero magic. It works because it respects three realities:

1. **Agents forget.** So externalize everything.
2. **Users are busy.** So keep it to one page.
3. **Improvement must be visible.** So test everything within 24 hours.

The constraints didn't make this exercise harder. They made the answer better. An unconstrained protocol would have been more comprehensive and less useful. This one is less comprehensive and immediately deployable.

That's the entire point. The best self-improvement protocol is the one that actually gets used. Constraints ensure that.

---

*GROUP_G — Constraint Thinking. Constraints are features, not bugs.*

Stats: runtime 1m44s • tokens 14.2k (in 3 / out 4.3k) • est $0.0000 • sessionKey agent:main:subagent:88644f86-3c78-4f9b-befb-bed1b2287f09 • sessionId b77cc504-97dd-4a2a-96c4-afaf5094f1d8 • transcript /Users/florianziesche/.openclaw/agents/main/sessions/b77cc504-97dd-4a2a-96c4-afaf5094f1d8.jsonl

Summarize this naturally for the user. Keep it brief (1-2 sentences). Flow it into the conversation naturally.
Do not mention technical details like tokens, stats, or that this was a background task.
You can respond with NO_REPLY if no announcement is needed (e.g., internal task with no user-facing result).