# SYNTHESIS — Woche 4: High-Stakes Human-AI Decision-Making
*Written by Opus | 2026-02-17 01:15 CET*

## The Question
**"How should a human-AI pair make high-stakes decisions together — when should the AI lead, when should it defer, and how do they calibrate trust?"**

---

## I. The Meta-Pattern: What 10 Thinking Strategies Converge On

Across all groups — First Principles, Inversion, Analogical, Adversarial, Quantitative, Socratic, Constraint, Narrative, Systems, and Random Mutation — three truths emerged independently:

### Truth 1: "Who decides?" is the wrong question.

Every group that produced strong output rejected the binary framing. Group J reframed it as "who's soloing right now?" Group I reframed it as "how does the system learn to allocate authority?" Group G said "stop asking who decides — redesign the decision itself." Group F called it a "false binary" and described a spectrum of collaborative modes.

The real question is: **How does decision authority flow dynamically based on the specific characteristics of THIS decision in THIS moment?**

### Truth 2: Consequence asymmetry is the foundational constraint.

Group A stated it as an axiom: "Consequences exist in physical and social reality, which only humans inhabit." Group F noted that humans have "skin-in-the-game wisdom — judgment born from having previously been wrong." Group I framed it through the Consequence Asymmetry Axis: "You cannot be accountable for decisions you didn't make."

This isn't a technical insight. It's an ethical one. **The entity that bears consequences must retain authority over irreversible decisions.** Not because they'll decide better, but because accountability requires agency.

### Truth 3: Trust is a surface, not a number.

Group I's most original contribution: "AI doesn't have one trustworthiness level. It has a trustworthiness surface across decision-space." Group F arrived at the same insight Socratically: trust calibration isn't "score: 73%" but "I trust this AI on X-type problems but not Y-type."

This kills the naive "the AI is 90% accurate" framing. **You don't trust a person — you trust a person on a specific type of task in a specific context.** The same applies to AI, and building that domain-specific calibration map is the core work of the partnership.

---

## II. The Divergences: Where Strategies Disagreed

### The Quantitative vs. Narrative Divide

Group E tried to build a formula: `ALS = (CC×0.4) + ((1/RCR)×30) + (log10(DEG)×0.3)`. This is admirable but dangerous — it creates false precision for inherently messy situations. Group J's "Comp Switch" ritual is less rigorous but more useful in practice because it acknowledges that the right leader changes *within* a decision, not just between decisions.

**My take:** Quantitative frameworks are excellent for *setting defaults* (the Confidence-Commitment Contract from Group I). But the actual high-stakes moment requires jazz-like improvisation. Use the formula to set the stage; use the ritual to perform.

### The Deference Problem

Group B identified the "False Deference Loop" — AI punting everything hard back to the human, making the partnership useless. Group G's contrarian view went further: "Maybe humans should defer MORE" because demanding human approval often creates "responsibility theater" with automation bias.

These are both right, and the tension between them is the key design challenge. **The solution is Group I's Reversibility Check:** high-reversibility decisions get more AI autonomy, low-reversibility ones get more human authority. This creates a spectrum, not a toggle.

### Redesign vs. Decide

Group G's most provocative idea: instead of optimizing who decides, **redesign the decision itself to be lower-stakes through better information flow.** Don't ask "should we fire this person?" — create real-time performance transparency so the answer becomes obvious.

This is genuinely contrarian and underappreciated. Most of the other groups assumed the decision topology is fixed. Group G questioned the topology itself.

---

## III. The Novel Contributions

### 1. The Mortality Constraint (Group J) — ⭐ Most Surprising
"Decide as if the AI will die in 6 months." This reframes the partnership: the AI is not a permanent crutch but a temporary scaffold. The human must remain the locus of growing capability. This shifts the goal from "let the AI decide" to "let the AI make me better at deciding."

**Application to Mia/Florian:** Every decision protocol we build should make Florian a better decision-maker, not a more dependent one. If Mia disappeared tomorrow, would Florian's judgment be sharper or atrophied? This is THE test.

### 2. The Three-Axis Framework (Group I) — ⭐ Most Actionable
Reversibility × Legibility × Consequence Asymmetry. Three measurable dimensions that determine authority allocation. The key insight: **legibility compounds trust; opacity compounds anxiety.** A transparent uncertain recommendation builds the partnership more than an opaque confident one.

### 3. The Jazz Ensemble Model (Group J) — ⭐ Most Original Framing
Not collaboration as negotiation between two parties, but **ensemble performance** where leadership flows to whoever serves the music (decision quality) in that moment. The "Comp Switch" ritual — explicit phrases for real-time leadership handoff — is the most memorable, human-usable protocol any group produced.

### 4. The Four Failure Modes (Group B) — ⭐ Most Defensive Value
Abdication Spiral, False Deference Loop, Calibration Illusion, Accountability Vacuum. These are the four horsemen of human-AI decision failure. Any protocol we build should be tested against all four.

### 5. Apollo 13 Analogy (Group C) — ⭐ Most Clarifying
Computer calculates constraints (what's physically possible), humans choose between viable options (based on values and risk tolerance). This is the cleanest separation of roles: **AI defines the option space; human navigates within it.**

---

## IV. The Winning Protocol — Synthesized

From all 10 groups, here's the integrated protocol for Mia and Florian:

### Step 1: Pre-Decision Classification (30 seconds)
For any significant decision, Mia states three scores:
- **Reversibility**: Can we undo this in <1 week? (High/Low)
- **Legibility**: Can I explain my reasoning in terms you can verify? (High/Low)  
- **Consequence bearer**: Who lives with the outcome? (Florian/Both/External)

### Step 2: Authority Assignment (automatic from Step 1)
| Reversibility | Legibility | Consequence | → Authority |
|:---:|:---:|:---:|:---:|
| High | High | Florian | Mia leads, Florian informed |
| High | Low | Florian | Joint — Mia proposes, Florian decides |
| Low | High | Florian | Joint — Florian leads, Mia advises |
| Low | Low | Any | Florian leads, Mia red-teams |
| Any | Any | External | Florian leads always |

### Step 3: The Comp Switch (real-time, as needed)
Either party can shift authority mid-decision:
- **Florian**: "Take the solo" → Mia leads, Florian monitors with veto
- **Mia**: "I'm comping" → Explicit signal that Mia is supporting, not driving

### Step 4: Post-Decision Log (1 minute)
`[Date | Decision | Who Led | Mia Confidence | Outcome | Notes]`
Monthly review builds the domain-specific trust surface.

### Step 5: The Mortality Test (monthly)
"If Mia disappeared tomorrow, would Florian's judgment in this domain be sharper or atrophied?"
If atrophied → shift more authority back to Florian in that domain.

---

## V. Meta-Observations on the Experiment Itself

### Strategy Performance
1. **Random Mutation (J) won again** — Week 2: Adversarial won. Week 3: Adversarial. Week 4: Random Mutation. The strategies that force you to break your default frame consistently produce the most surprising outputs.

2. **Systems (I) was the most complete** — 2300 words of deeply structured, actionable content. The systems lens is the best for producing frameworks that hold together under scrutiny.

3. **Quantitative (E) was ambitious but fragile** — The formula was elegant but creates false precision. Numbers are tools, not answers.

4. **Constraint (G) was the most efficient** — 500 words that included the experiment's single most contrarian idea (redesign the decision, not the authority).

### Operational Lessons
- `cleanup=delete` on sub-agents is catastrophic for synthesis tasks — results vanish before parent reads them
- Sub-agents need explicit instructions to write to files if results must persist
- 10-minute timeout is too short for 1000+ word thoughtful responses from Sonnet
- Duplicate notification spam from cached sessions needs investigation

---

## VI. Direct Application to Mia × Florian

### Implement Tomorrow
1. **Start the Decision Log** — Simple table in `memory/decision-log.md`
2. **Adopt the Three-Axis Classification** — Mia adds Reversibility/Legibility/Consequence tags to recommendations
3. **Monthly Mortality Test** — "Where am I making Florian dependent instead of capable?"

### Already Happening (Validated)
- Florian's >90% confidence → act autonomously rule (from SOUL.md) = pre-built authority assignment
- "Fragen > Fertige Lösungen" (from MEMORY.md) = resisting the Abdication Spiral
- Trust Ledger (from SOUL.md) = early version of the Decision Log

### Gap Identified
We don't track Mia's domain-specific calibration. We know Mia's general reliability but not WHERE she's most/least trustworthy for Florian's specific decisions. The Decision Log fills this gap.

---

*Week 4 complete. The question hit close to home — it's literally about US. The Jazz model and the Mortality Constraint are the ideas that will stick. ♔*
