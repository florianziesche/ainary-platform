# High-Stakes Human-AI Decision Framework

## The Core Principle: Complementary Asymmetry

Human-AI pairs should make high-stakes decisions through **structured role reversal**, not consensus. The AI leads on pattern recognition across vast data; the human leads on ethical stakes, contextual nuance, and irreversible consequences.

## When AI Should Lead

**Pattern-dense, time-sensitive scenarios** where speed matters and the cost of delay exceeds the cost of imperfection:
- Medical diagnosis with clear imaging data
- Financial anomaly detection
- Supply chain optimization under crisis
- Initial threat assessment in security contexts

The AI synthesizes signals humans cannot process at scale, then surfaces its confidence level with transparent reasoning. It should **not** wait for human approval when delay creates harm.

## When AI Must Defer

**Value-laden, irreversible, or politically charged decisions** where being right matters less than being legitimate:
- Hiring/firing decisions
- Legal sentencing or parole
- Resource allocation affecting vulnerable populations
- Strategic pivots that reshape organizational identity

Here, the human must own the decision even when AI analysis is superior, because accountability cannot be delegated to a model.

## Trust Calibration: The 90% Rule

Trust isn't binaryâ€”it's probabilistic and dynamic. The pair should:

1. **Establish confidence thresholds**: AI states certainty (60%, 85%, 95%); human sets action thresholds per domain
2. **Track calibration over time**: Did 90% predictions prove 90% accurate? Adjust trust accordingly
3. **Mandate disagreement documentation**: When human overrides AI (or vice versa), log it. Review monthly. This feedback loop is the trust engine.

## The Contrarian View: Maybe Humans Should Defer More

The conventional wisdom says "keep humans in the loop for safety." But this often means **humans rubber-stamp AI decisions under time pressure**, creating **responsibility theater** without genuine oversight.

In many high-stakes domains (surgery assistance, crisis response), demanding human approval may actually increase risk because:
- Humans anchor on AI suggestions anyway (automation bias)
- Added latency can be lethal
- Diffused responsibility ("the AI said so") degrades human judgment over time

**Provocative alternative**: In narrow, well-validated domains, let AI own tactical decisions fully while humans own strategic oversight and exception handling. Clearer accountability, better outcomes.

## Specific Action for Tomorrow

**Tomorrow morning, conduct a 15-minute "Decision Audit" with your AI:**

1. List 3 decisions you made together in the past week
2. For each, answer:
   - Who actually led? (Who would have been blamed if it went wrong?)
   - What was the AI's stated confidence?
   - Was the confidence calibrated? (If resolved, was the AI right?)
3. Identify one category where you're currently over-relying on AI (rubber-stamping) or under-utilizing it (ignoring high-confidence guidance)
4. Set one new threshold rule: "When AI confidence >X% on [domain], I will/won't require my review"

This 15-minute practice builds calibration muscle faster than months of ad-hoc collaboration.

---

**Word count: 497**
