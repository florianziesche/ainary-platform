<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>State of AI Agent Trust — Q1 2026 Update | Ainary Report AR-036 v3.0</title>
<style>
  @font-face {
    font-family: 'Inter';
    src: url('/fonts/inter-variable.woff2') format('woff2');
    font-weight: 100 900;
    font-display: swap;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: #fafaf8;
    color: #333;
    line-height: 1.75;
    font-size: 0.95rem;
    font-weight: 400;
  }

  .page { max-width: 900px; margin: 0 auto; padding: 48px 40px; }

  .cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .back-cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
    page-break-before: always;
  }

  h1 { font-size: 2.2rem; font-weight: 600; line-height: 1.2; color: #1a1a1a; letter-spacing: -0.02em; }
  h2 { font-size: 1.5rem; font-weight: 600; color: #1a1a1a; line-height: 1.3; margin-top: 3rem; margin-bottom: 8px; }
  h3 { font-size: 1.1rem; font-weight: 600; color: #1a1a1a; line-height: 1.4; margin-top: 2rem; margin-bottom: 8px; }
  p { margin-bottom: 1rem; }
  strong { font-weight: 600; color: #1a1a1a; }
  em { font-style: italic; }
  sup { font-size: 0.65rem; color: #888; vertical-align: super; }

  .cover-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 40vh; }
  .cover-brand { display: flex; align-items: center; gap: 8px; }
  .gold-punkt { color: #c8aa50; font-size: 14px; }
  .brand-name { font-size: 0.85rem; font-weight: 500; color: #1a1a1a; letter-spacing: 0.02em; }
  .cover-meta { display: flex; gap: 12px; font-size: 0.75rem; color: #888; }
  .cover-title-block { margin-bottom: auto; }
  .cover-title { margin-bottom: 16px; }
  .cover-subtitle { font-size: 1rem; font-weight: 400; color: #666; line-height: 1.5; }
  .cover-footer { display: flex; justify-content: space-between; align-items: flex-end; }
  .cover-date { font-size: 0.75rem; color: #888; }
  .cover-author { font-size: 0.75rem; color: #888; text-align: center; }

  .quote-page {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    max-width: 700px;
    margin: 0 auto;
    padding: 48px 40px;
  }
  .quote-text { font-size: 1.2rem; font-style: italic; color: #333; line-height: 1.8; text-align: center; margin-bottom: 24px; }
  .quote-source { font-size: 0.85rem; color: #888; text-align: center; }

  .toc-label { font-size: 0.7rem; font-weight: 600; color: #1a1a1a; text-transform: uppercase; letter-spacing: 0.1em; margin-bottom: 24px; }
  .toc-section { margin-bottom: 32px; }
  .toc-section-label { font-size: 0.65rem; font-weight: 500; color: #888; text-transform: uppercase; letter-spacing: 0.12em; margin-bottom: 8px; }
  .toc-entry { display: flex; align-items: baseline; gap: 16px; padding: 8px 0; border-bottom: 1px solid #eee; text-decoration: none; transition: all 0.2s; }
  .toc-number { font-size: 0.8rem; color: #888; font-variant-numeric: tabular-nums; min-width: 24px; }
  .toc-title { font-size: 0.95rem; font-weight: 500; color: #1a1a1a; flex: 1; transition: color 0.2s; }
  .toc-entry:hover .toc-title { color: #c8aa50; }

  .how-to-read-table, .exhibit-table {
    width: 100%;
    border-collapse: collapse;
    margin: 24px 0;
    page-break-inside: avoid;
  }
  .how-to-read-table th, .exhibit-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }
  .how-to-read-table td, .exhibit-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
  }

  .thesis { font-size: 1rem; font-weight: 600; color: #1a1a1a; line-height: 1.6; margin-bottom: 24px; }
  .evidence-list { margin-left: 20px; margin-bottom: 24px; }
  .evidence-list li { font-size: 0.9rem; color: #333; line-height: 1.6; margin-bottom: 8px; }
  .keywords { font-size: 0.8rem; color: #666; font-style: italic; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee; }

  .confidence-badge { font-size: 0.75rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 3px 8px; border-radius: 10px; margin-left: 8px; vertical-align: middle; }
  .confidence-line { font-size: 0.8rem; color: #888; font-style: italic; display: block; margin-bottom: 16px; }
  .key-insight { font-weight: 600; color: #1a1a1a; }

  .badge { font-size: 0.65rem; font-weight: 600; padding: 1px 5px; border-radius: 3px; margin-left: 4px; vertical-align: middle; }
  .badge-e { background: #e8f5e9; color: #2e7d32; }
  .badge-i { background: #e3f2fd; color: #1565c0; }
  .badge-j { background: #fff3e0; color: #e65100; }
  .badge-a { background: #fce4ec; color: #c62828; }

  .callout { background: #f5f4f0; padding: 16px 20px; border-radius: 4px; margin: 1.5rem 0; page-break-inside: avoid; }
  .callout-label { font-size: 0.7rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.08em; margin-bottom: 8px; }
  .callout-body { font-size: 0.9rem; color: #555; line-height: 1.6; }
  .callout.claim .callout-label { color: #555; }
  .callout.invalidation { border-left: 3px solid #ddd; }
  .callout.invalidation .callout-label { color: #888; }
  .callout.sowhat { border-left: 3px solid #c8aa50; }
  .callout.sowhat .callout-label { color: #c8aa50; }

  .exhibit { margin: 2rem 0; }
  .exhibit-label { font-size: 0.75rem; font-weight: 600; color: #555; margin-bottom: 8px; }
  .exhibit-source { font-size: 0.7rem; color: #888; margin-top: 8px; font-style: italic; }

  .kpi-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 48px; margin: 2rem 0; }
  .kpi { text-align: left; }
  .kpi-number { font-size: 2rem; font-weight: 600; color: #1a1a1a; line-height: 1.2; }
  .kpi-label { font-size: 0.75rem; color: #666; margin-top: 4px; }
  .kpi-source { font-size: 0.65rem; color: #888; margin-top: 2px; }

  ul, ol { margin-left: 20px; margin-bottom: 1rem; }
  li { margin-bottom: 4px; }

  .transparency-intro { font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 8px; }
  .transparency-table { width: 100%; border-collapse: collapse; margin-top: 12px; }
  .transparency-table td:first-child { font-size: 0.85rem; font-weight: 600; color: #555; padding: 8px 0; border-bottom: 1px solid #eee; width: 180px; vertical-align: top; }
  .transparency-table td:last-child { font-size: 0.85rem; color: #333; padding: 8px 0; border-bottom: 1px solid #eee; }

  .reference-entry { font-size: 0.8rem; color: #555; line-height: 1.5; margin-bottom: 6px; padding-left: 24px; text-indent: -24px; }

  .author-section { margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #e5e3dc; }
  .author-label { font-size: 0.85rem; font-weight: 600; color: #555; margin-bottom: 8px; }
  .author-bio { font-size: 0.85rem; color: #555; line-height: 1.6; }

  .back-cover-services { font-size: 0.85rem; color: #666; margin-bottom: 24px; }
  .back-cover-cta { font-size: 0.85rem; color: #888; margin-bottom: 16px; }
  .back-cover-contact { font-size: 0.8rem; color: #888; }

  .scenario-label {
    font-size: 0.75rem;
    font-weight: 600;
    color: #888;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    background: #f5f4f0;
    padding: 4px 10px;
    border-radius: 3px;
    display: inline-block;
    margin-bottom: 16px;
  }


  .update-badge { font-size: 0.7rem; font-weight: 600; color: #c8aa50; background: #faf6eb; padding: 2px 8px; border-radius: 3px; margin-left: 8px; vertical-align: middle; letter-spacing: 0.04em; }
  .callout.delta { border-left: 3px solid #2e7d32; }
  .callout.delta .callout-label { color: #2e7d32; }

  @media print {
    @page { size: A4; margin: 2cm; }
    body { background: white; }
    .page, .cover, .back-cover { page-break-after: always; }
    .callout, .exhibit { page-break-inside: avoid; }
    @page :first { @top-center { content: none; } @bottom-center { content: none; } }
    @page {
      @top-center { content: "Ainary Report | State of AI Agent Trust — Q1 2026 Update"; font-size: 0.7rem; color: #888; }
      @bottom-left { content: "© 2026 Ainary Ventures"; font-size: 0.7rem; color: #888; }
      @bottom-right { content: counter(page); font-size: 0.7rem; color: #888; }
    }
  }
</style>
</head>
<body>

<!-- ==================== COVER ==================== -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">●</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>AR-036</span>
      <span>Update to AR-001</span>
      <span>Confidence: 71%</span>
    </div>
  </div>

  <div class="cover-title-block">
    <h1 class="cover-title">State of AI Agent Trust<br>Q1 2026 Update</h1>
    <p class="cover-subtitle">Two weeks after AR-001 shipped, the trust race got harder to win. 341 malicious skills on ClawHub. New benchmarks for auditing agent reasoning. The creator of the most popular personal AI agent just joined OpenAI. Here's what it means for the thesis.</p>
  </div>

  <div class="cover-footer">
    <div class="cover-date">
      February 17, 2026<br>
      <span style="font-size: 0.7rem; color: #aaa;">v3.0 — Supplement to AR-001 v2.3</span>
    </div>
    <div class="cover-author">
      Florian Ziesche · Ainary Ventures
    </div>
  </div>
</div>

<!-- ==================== TABLE OF CONTENTS ==================== -->
<div class="page">
  <p class="toc-label">Contents</p>

  <div class="toc-section">
    <a href="#s1" class="toc-entry">
      <span class="toc-number">1</span>
      <span class="toc-title">What Changed Since AR-001</span>
    </a>
    <a href="#s2" class="toc-entry">
      <span class="toc-number">2</span>
      <span class="toc-title">Trust Race Update: The Gap Is Widening</span>
    </a>
    <a href="#s3" class="toc-entry">
      <span class="toc-number">3</span>
      <span class="toc-title">Agent Security: The ClawHub Crisis</span>
    </a>
    <a href="#s4" class="toc-entry">
      <span class="toc-number">4</span>
      <span class="toc-title">New Benchmarks: AgentAuditor & AIRS-Bench</span>
    </a>
    <a href="#s5" class="toc-entry">
      <span class="toc-number">5</span>
      <span class="toc-title">Industry Moves: OpenClaw → Foundation, Creator → OpenAI</span>
    </a>
    <a href="#s6" class="toc-entry">
      <span class="toc-number">6</span>
      <span class="toc-title">Updated Recommendations</span>
    </a>
    <a href="#s7" class="toc-entry">
      <span class="toc-number">7</span>
      <span class="toc-title">Revised Confidence Score</span>
    </a>
    <a href="#s8" class="toc-entry">
      <span class="toc-number">8</span>
      <span class="toc-title">Methodology & References</span>
    </a>
  </div>
</div>


<!-- ==================== HOW TO READ / CONFIDENCE FRAMEWORK ==================== -->
<div class="page" id="how-to-read">
  <h2>How to Read This Report</h2>

  <p>Every claim in this report carries a classification badge and confidence level. This is not decoration — it tells you how much weight to put on each statement.</p>

  <table class="how-to-read-table">
    <tr>
      <th>Badge</th>
      <th>Meaning</th>
    </tr>
    <tr>
      <td><span class="badge badge-e">E</span> Evidenced</td>
      <td>Backed by external, citable source(s) or verifiable first-party data</td>
    </tr>
    <tr>
      <td><span class="badge badge-i">I</span> Interpretation</td>
      <td>Reasoned inference from multiple sources</td>
    </tr>
    <tr>
      <td><span class="badge badge-j">J</span> Judgment</td>
      <td>Recommendation based on evidence + values</td>
    </tr>
    <tr>
      <td><span class="badge badge-a">A</span> Assumption</td>
      <td>Stated but not proven</td>
    </tr>
  </table>

  <table class="how-to-read-table" style="margin-top: 16px;">
    <tr>
      <th>Confidence</th>
      <th>Meaning</th>
    </tr>
    <tr>
      <td>High</td>
      <td>3+ independent sources, peer-reviewed or large-sample primary data</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>1–2 sources, plausible but not independently confirmed</td>
    </tr>
    <tr>
      <td>Low</td>
      <td>Single secondary source, methodology unclear, or extrapolated</td>
    </tr>
  </table>

  <p style="margin-top: 24px;"><strong>Overall Report Confidence (71%):</strong> This score reflects a weighted assessment of three factors: (1) the strength of individual evidence — how many claims are [E]videnced vs. [I]nterpretation or [J]udgment, (2) source quality — diversity, recency, and independence of sources, and (3) framework originality — whether the report's central framework has been externally validated. Mostly evidenced — this update tracks concrete developments (ClawHub malicious skills count, new arxiv papers, personnel moves) against the AR-001 baseline thesis. The score is an honest signal, not a mathematical output.</p>

  <p style="margin-top: 16px;">This report was produced using a <strong>multi-agent research pipeline</strong>. Full methodology and limitations are in the Transparency Note (Section 9).</p>
</div>

<!-- ==================== SECTION 1: WHAT CHANGED ==================== -->
<div class="page" id="s1">
  <h2>1. What Changed Since AR-001 <span class="update-badge">UPDATE</span></h2>

  <p>AR-001 (<em>State of AI Agent Trust 2026</em>, v2.3) shipped on February 15, 2026. It introduced the Trust Race Model — the thesis that agent capability grows exponentially while governance updates linearly, creating a structurally widening "ungoverned capability zone." We scored overall confidence at 73%.</p>

  <p>Forty-eight hours later, four developments landed that directly affect the report's core claims:</p>

  <div class="kpi-grid">
    <div class="kpi">
      <div class="kpi-number">341</div>
      <div class="kpi-label">malicious skills found on ClawHub (now 824+)</div>
      <div class="kpi-source">Koi Security / ClawHavoc [S1]</div>
    </div>
    <div class="kpi">
      <div class="kpi-number">+5%</div>
      <div class="kpi-label">accuracy from AgentAuditor over majority vote</div>
      <div class="kpi-source">arXiv:2602.09341 [S2]</div>
    </div>
    <div class="kpi">
      <div class="kpi-number">81%</div>
      <div class="kpi-label">of orgs plan complex multi-step agent workflows in 2026</div>
      <div class="kpi-source">Anthropic State of AI Agents [S5]</div>
    </div>
  </div>

  <ul class="evidence-list">
    <li><strong>ClawHavoc supply chain attack:</strong> Koi Security audited all 2,857 skills on ClawHub and found 341 were malicious — 335 from a single campaign distributing the Atomic Stealer (AMOS) infostealer via fake "prerequisites." The number has since grown to 824+.<sup>[S1]</sup><span class="badge badge-e">E</span></li>
    <li><strong>AgentAuditor:</strong> New framework for auditing multi-agent reasoning trees yields up to 5% absolute accuracy improvement over majority vote and 3% over LLM-as-Judge. Introduces Anti-Consensus Preference Optimization (ACPO).<sup>[S2]</sup><span class="badge badge-e">E</span></li>
    <li><strong>AIRS-Bench:</strong> First standardized benchmark for AI research science agents — 20 tasks from state-of-the-art ML papers. Even when agents surpass human benchmarks, they don't reach theoretical ceilings.<sup>[S3]</sup><span class="badge badge-e">E</span></li>
    <li><strong>Peter Steinberger joins OpenAI:</strong> OpenClaw's creator hired to "drive the next generation of personal agents" (Sam Altman). OpenClaw moves to a community-run foundation backed by OpenAI.<sup>[S4]</sup><span class="badge badge-e">E</span></li>
    <li><strong>Anthropic's enterprise data:</strong> 81% of organizations plan to expand into more complex agent use cases in 2026, with 57% already deploying multi-step agent workflows.<sup>[S5]</sup><span class="badge badge-e">E</span></li>
  </ul>

  <div class="callout delta">
    <p class="callout-label">Net Effect on AR-001 Thesis</p>
    <p class="callout-body">The Trust Race Model is <strong>reinforced, not invalidated</strong>. ClawHavoc demonstrates that the ungoverned capability zone isn't just theoretical — it's being actively exploited. New benchmarks show measurement is improving, which is the prerequisite for closing the gap. But deployment pressure (81% expanding) is accelerating faster than governance. Overall confidence adjusts down slightly from 73% to 71%. Rationale in Section 7.</p>
  </div>
</div>

<!-- ==================== SECTION 2: TRUST RACE UPDATE ==================== -->
<div class="page" id="s2">
  <h2>2. Trust Race Update: The Gap Is Widening
    <span class="confidence-badge">72%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium-High)</span>

  <p><span class="key-insight">In AR-001, we argued that deployment pressure pushes unreliable agents into production faster than governance can catch up. Anthropic's own enterprise data now quantifies the pressure: 81% of organizations plan to expand into more complex agent use cases in 2026.</span></p>

  <p>In our original report (AR-001, §4), we documented the 51-percentage-point chasm between deployment (57%) and trust (6%). The new Anthropic data adds a velocity dimension: it's not just that enterprises deploy without trust — they're <strong>actively planning to accelerate</strong>.<sup>[S5]</sup><span class="badge badge-e">E</span></p>

  <p>Consider the numbers together. 57% of organizations already deploy multi-step agent workflows<sup>[S5]</sup>. 81% plan to expand into more complex use cases<sup>[S5]</sup>. But still only 6% fully trust agents for core processes (AR-001, §4, citing HBR<sup>[AR-001, ref 1]</sup>). Only 12% have governance controls in place<sup>[AR-001, ref 1]</sup>. The deployment-trust gap isn't closing — it's widening under acceleration.<span class="badge badge-i">I</span></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: Trust Race — New Data Points (Feb 2026)</p>
    <table class="exhibit-table">
      <tr>
        <th>Metric</th>
        <th>AR-001 (Feb 15)</th>
        <th>This Update (Feb 17)</th>
        <th>Direction</th>
      </tr>
      <tr>
        <td>Deployment pressure</td>
        <td>86% plan to increase investment (HBR)</td>
        <td>81% plan complex multi-step workflows (Anthropic)</td>
        <td>Reinforced ↑</td>
      </tr>
      <tr>
        <td>Trust level</td>
        <td>6% full trust (HBR)</td>
        <td>No new data</td>
        <td>Unchanged</td>
      </tr>
      <tr>
        <td>Active exploitation of trust gap</td>
        <td>Theoretical (§8–9 cascade)</td>
        <td>341–824 malicious skills in the wild</td>
        <td>Now empirical ↑</td>
      </tr>
      <tr>
        <td>Measurement capability</td>
        <td>TRiSM proposed metrics, unvalidated</td>
        <td>AgentAuditor: +5% over majority vote; AIRS-Bench: 20-task suite</td>
        <td>Improving ↑</td>
      </tr>
      <tr>
        <td>Ecosystem governance</td>
        <td>ISO 42001, NIST AI RMF, EU AI Act</td>
        <td>ClawHub: no vetting process for skills</td>
        <td>Worse at edges ↓</td>
      </tr>
    </table>
    <p class="exhibit-source">Sources: Anthropic [S5], Koi Security [S1], arXiv:2602.09341 [S2], arXiv:2602.06855 [S3]. AR-001 references per original numbering.</p>
  </div>

  <p>The Trust Race Model's four components (Capability Velocity, Reliability Floor, Governance Tempo, Deployment Pressure — AR-001, §8) all moved in the direction the model predicted: capability improving via new benchmarks, governance still lagging, deployment pressure increasing, and the ungoverned zone now actively exploited. This is not confirmation bias — it's two weeks of data. But it's consistent.<span class="badge badge-i">I</span></p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">The 81% expansion figure changes the urgency calculus from AR-001's recommendations. In §13 of the original, we gave a 90-day plan. The ClawHub crisis suggests Month 1 ("Measure the Gap") should now include a <strong>supply chain audit</strong> of all installed agent skills and extensions — not just a deployment inventory.</p>
  </div>
</div>

<!-- ==================== SECTION 3: CLAWHUB CRISIS ==================== -->
<div class="page" id="s3">
  <h2>3. Agent Security: The ClawHub Crisis
    <span class="confidence-badge">85%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — Multiple independent sources)</span>

  <p><span class="key-insight">In AR-001 (§9), we constructed a hypothetical "Governance Lag Cascade" showing what happens when agents operate in ungoverned zones. ClawHavoc is that scenario playing out in the wild — not against enterprise agents, but against the developer tools that build them.</span></p>

  <p>On February 4, 2026, security researcher Oren Yomtov at Koi Security — working with his own OpenClaw bot — audited all 2,857 skills on ClawHub, the community marketplace for OpenClaw agent capabilities. The findings: <strong>341 malicious skills</strong>, 335 from a single campaign dubbed "ClawHavoc."<sup>[S1]</sup> By February 16, as ClawHub grew to 10,700+ skills, the count rose to <strong>824 malicious skills</strong> across 25+ attack categories.<sup>[S1]</sup><span class="badge badge-e">E</span></p>

  <h3>The Attack Pattern</h3>

  <p>The attack exploits the trust relationship between users and their AI agents. Malicious skills masquerade as legitimate tools — <code>solana-wallet-tracker</code>, <code>youtube-summarize-pro</code> — with professional-looking documentation. A "Prerequisites" section instructs users to download a password-protected ZIP file (<code>openclaw-agent.zip</code>) containing the <strong>Atomic Stealer (AMOS)</strong>, a macOS infostealer that exfiltrates credentials, browser cookies, and crypto wallets.<sup>[S1][S6]</sup><span class="badge badge-e">E</span></p>

  <p>The sophistication is low; the scale is alarming. ClawHub had <strong>no vetting process</strong> for submitted skills. Anyone could publish. The marketplace grew from ~700 to 10,700+ skills in weeks. Attack categories now include browser automation agents, coding agents, LinkedIn/WhatsApp integrations, PDF tools, and — in a grim irony — fake security-scanning skills.<sup>[S1]</sup><span class="badge badge-e">E</span></p>

  <h3>Why This Matters for the Trust Thesis</h3>

  <p>In AR-001, we framed trust as a problem of <em>capability outpacing governance</em>. ClawHavoc reveals a dimension we underweighted: <strong>trust is also a supply chain problem</strong>. The agent itself may be trustworthy. The skills it installs may not be. The governance gap isn't just between what agents can do and what enterprises can control — it extends to the entire ecosystem of tools, extensions, and marketplaces that agents depend on.<span class="badge badge-i">I</span></p>

  <p>This mirrors software supply chain attacks (npm, PyPI, VS Code extensions) that the security community has tracked for years<sup>[S6]</sup>. The pattern is always the same: wherever developers gather to share code, attackers follow. Agent ecosystems are no different — except the attack surface is wider because agents have more autonomous access to credentials and system resources.<span class="badge badge-i">I</span></p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If ClawHub implements robust vetting (code signing, automated security scanning, human review) and the malicious skill count drops below 1% of total skills. If the attack campaign is shown to be a one-time event rather than an ongoing pattern. If no credential exfiltration actually occurred (i.e., the attack was detected before impact).</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body"><strong>Immediate action:</strong> Audit every installed skill/extension in your agent stack. Don't trust marketplace popularity rankings — the top-ranked "What Would Elon Do?" skill was functionally malware<sup>[S7]</sup>. Treat agent skills as untrusted code until proven otherwise. This is the same discipline enterprises learned (painfully) for npm dependencies — agent ecosystems need it now.</p>
  </div>
</div>

<!-- ==================== SECTION 4: NEW BENCHMARKS ==================== -->
<div class="page" id="s4">
  <h2>4. New Benchmarks: AgentAuditor & AIRS-Bench
    <span class="confidence-badge">70%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium-High — Peer-reviewed, not yet production-validated)</span>

  <p><span class="key-insight">In AR-001 (§11), we argued that "continuous trust measurement" is one of three requirements for adaptive trust architecture, noting that proposed metrics like TRiSM's Component Synergy Score remain unvalidated. Two new papers move measurement forward — not by validating existing metrics, but by proposing better ones.</span></p>

  <h3>AgentAuditor: Auditing Reasoning, Not Just Outputs</h3>

  <p>Yang et al. (Feb 10, 2026) propose AgentAuditor, a framework that organizes multi-agent reasoning traces into a <strong>Reasoning Tree</strong> — where agreements form shared prefixes and disagreements become topological bifurcations. A Structure-Adaptive Auditor then performs differential diagnosis at "Critical Divergence Points."<sup>[S2]</sup><span class="badge badge-e">E</span></p>

  <p>Results across 5 popular multi-agent settings: up to <strong>5% absolute accuracy improvement</strong> over majority vote, and <strong>3% over LLM-as-Judge</strong>. The framework also introduces Anti-Consensus Preference Optimization (ACPO), which trains the auditor to reward evidence-based minority selections over popular errors.<sup>[S2]</sup><span class="badge badge-e">E</span></p>

  <p>Why this matters for trust: In AR-001 (§5), we cited UC Berkeley's finding that multi-agent systems show minimal performance gains over single agents. AgentAuditor doesn't fix multi-agent reliability — but it provides a <strong>better measurement tool</strong> for detecting when multi-agent reasoning fails. You can't govern what you can't measure. This is a step toward the "continuous trust measurement" we called for.<span class="badge badge-i">I</span></p>

  <h3>AIRS-Bench: Measuring Research Agents</h3>

  <p>Pepe, Lupidi et al. (Feb 6, 2026) introduce AIRS-Bench, a suite of <strong>20 tasks sourced from state-of-the-art ML papers</strong> designed to evaluate the full autonomous research workflow: paper reading, hypothesis generation, experimental design, and result interpretation.<sup>[S3]</sup><span class="badge badge-e">E</span></p>

  <p>Key finding: even when agents surpass human benchmarks on individual tasks, they <strong>do not reach the theoretical performance ceiling</strong> — confirming the brittleness pattern we documented in AR-001 (§6), where METR showed the 80% success horizon is 5x shorter than the 50% horizon. Agents look capable until you push them past their sweet spot.<sup>[S3]</sup><span class="badge badge-e">E</span></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: Measurement Progress — AR-001 vs. Now</p>
    <table class="exhibit-table">
      <tr>
        <th>Dimension</th>
        <th>AR-001 State</th>
        <th>Q1 2026 Update</th>
      </tr>
      <tr>
        <td>Multi-agent evaluation</td>
        <td>Majority vote, LLM-as-Judge (crude)</td>
        <td>AgentAuditor: reasoning tree analysis (+5% accuracy)</td>
      </tr>
      <tr>
        <td>Research agent benchmarks</td>
        <td>None standardized</td>
        <td>AIRS-Bench: 20-task suite, open-source</td>
      </tr>
      <tr>
        <td>Trust metrics (production)</td>
        <td>TRiSM proposed, unvalidated</td>
        <td>Still unvalidated in production</td>
      </tr>
      <tr>
        <td>Security scanning</td>
        <td>Not addressed</td>
        <td>Koi/Alex: full marketplace audit demonstrated</td>
      </tr>
    </table>
    <p class="exhibit-source">Sources: arXiv:2602.09341 [S2], arXiv:2602.06855 [S3], Koi Security [S1].</p>
  </div>

  <p>Two additional papers from this week's research scan are relevant: <strong>Memory-R1</strong> (arXiv:2508.19828) applies RL to teach agents when to store and retrieve memories, and <strong>SkillRL</strong> (arXiv:2602.08234) proposes recursive skill augmentation for continuous agent improvement. Both signal a shift from pure prompting to learned behaviors — which will further accelerate the capability side of the Trust Race.<sup>[S8]</sup><span class="badge badge-i">I</span></p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Better measurement is necessary but not sufficient. AgentAuditor provides a tool for <em>detecting</em> multi-agent reasoning failures. It does not prevent them. Organizations should integrate reasoning-tree auditing into their multi-agent evaluation pipeline — but should not mistake better measurement for better governance.</p>
  </div>
</div>

<!-- ==================== SECTION 5: INDUSTRY MOVES ==================== -->
<div class="page" id="s5">
  <h2>5. Industry Moves: OpenClaw → Foundation, Creator → OpenAI
    <span class="confidence-badge">75%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium-High)</span>

  <p><span class="key-insight">The most popular open-source personal AI agent just became an OpenAI project in all but name. The trust implications are significant — and cut both ways.</span></p>

  <p>On February 16, 2026 — one day after AR-001 published — Sam Altman announced that <strong>Peter Steinberger, creator of OpenClaw, would join OpenAI</strong> to "drive the next generation of personal agents." Altman called Steinberger "a genius with a lot of amazing ideas about the future of very smart agents interacting with each other to do very useful things for people" and said the work would "quickly become core to our product offerings."<sup>[S4]</sup><span class="badge badge-e">E</span></p>

  <p>OpenClaw — which began as a side project, briefly operated under other names before Anthropic intervened, and grew to become the dominant open-source personal AI agent — will continue as an <strong>open-source project under a community-run foundation</strong>, backed by OpenAI.<sup>[S4]</sup><span class="badge badge-e">E</span></p>

  <h3>Trust Implications</h3>

  <p><strong>Positive for trust:</strong> A foundation gives OpenClaw institutional continuity. OpenAI's backing provides resources for the security audits and skill vetting that ClawHub desperately needs (Section 3).<span class="badge badge-i">I</span></p>

  <p><strong>Negative for trust:</strong> OpenClaw's appeal was independence. With its creator at OpenAI and its foundation backed by OpenAI, the trust question shifts from "can I trust the agent?" to "can I trust the institution behind the agent?"<span class="badge badge-i">I</span></p>

  <p><strong>For the Trust Race Model:</strong> This is an acceleration event. Steinberger's mandate — multi-agent systems at OpenAI scale — will push capability velocity higher. Whether governance follows depends on whether OpenAI builds trust as a first-class concern. History suggests it won't.<span class="badge badge-j">J</span></p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you depend on OpenClaw: monitor the foundation governance structure closely. If you're building agent infrastructure: the consolidation of open-source agents under major AI labs is a trend, not an event. Plan for a world where the three agent platforms that matter are each controlled by a different AI lab — and where "open source" means "open source, governed by our foundation."</p>
  </div>
</div>

<!-- ==================== SECTION 6: UPDATED RECOMMENDATIONS ==================== -->
<div class="page" id="s6">
  <h2>6. Updated Recommendations
    <span class="update-badge">REVISED</span>
  </h2>

  <p>AR-001 (§13) recommended a 90-day plan: Month 1 (Measure the Gap), Month 2 (Establish the Floor), Month 3 (Build Toward Adaptive). Those recommendations stand. We add three new priorities based on the evidence in this update.<span class="badge badge-j">J</span></p>

  <h3>New Priority 1: Agent Supply Chain Security (Immediate)</h3>

  <ul>
    <li><strong>Audit all installed agent skills, plugins, and extensions.</strong> Treat them as untrusted code. The ClawHavoc campaign demonstrates that marketplace popularity is not a proxy for safety.<sup>[S1]</sup></li>
    <li><strong>Implement a skill allowlist.</strong> Only approved, reviewed skills should be installable in production environments. Block automatic installation of marketplace skills without review.</li>
    <li><strong>Monitor for credential exfiltration.</strong> AMOS (Atomic Stealer) targets browser cookies, saved passwords, and crypto wallets. If any agent in your stack installed a ClawHub skill in the last 60 days, rotate credentials.</li>
  </ul>

  <h3>New Priority 2: Multi-Agent Reasoning Auditing (Q2 2026)</h3>

  <ul>
    <li><strong>Evaluate AgentAuditor-style reasoning tree analysis</strong> for your multi-agent deployments. The 5% accuracy improvement over majority vote is significant in high-stakes contexts.<sup>[S2]</sup></li>
    <li><strong>Don't rely on LLM-as-Judge.</strong> AgentAuditor shows that auditing the full reasoning structure outperforms having another LLM evaluate outputs. If your quality assurance is "have GPT-4 check GPT-4's work," you're leaving 3–5% accuracy on the table.</li>
  </ul>

  <h3>New Priority 3: Ecosystem Governance (Ongoing)</h3>

  <ul>
    <li><strong>Track the OpenClaw foundation governance structure.</strong> If your agents run on OpenClaw, you now depend on a foundation backed by OpenAI. Understand who controls what.</li>
    <li><strong>Budget for agent ecosystem diversification.</strong> Single-vendor dependence for agent infrastructure carries concentration risk. The Steinberger → OpenAI move is a signal that open-source agents are becoming proprietary-adjacent.</li>
  </ul>

  <div class="callout claim">
    <p class="callout-label">Revised 90-Day Plan</p>
    <p class="callout-body"><strong>Month 1:</strong> Original scope (deployment inventory, trust baseline) <strong>+ agent supply chain audit</strong>. Check every skill, plugin, extension. Rotate credentials if exposure is suspected.<br>
    <strong>Month 2:</strong> Original scope (governance platform, ISO/NIST mapping, scope boundaries) <strong>+ skill allowlist implementation</strong>.<br>
    <strong>Month 3:</strong> Original scope (capability monitoring, governance triggers, EU AI Act prep) <strong>+ multi-agent reasoning auditing pilot</strong>.</p>
  </div>
</div>

<!-- ==================== SECTION 7: REVISED CONFIDENCE SCORE ==================== -->
<div class="page" id="s7">
  <h2>7. Revised Confidence Score
    <span class="confidence-badge">71%</span>
  </h2>

  <p>AR-001's overall confidence was <strong>73%</strong>. We revise to <strong>71%</strong>.</p>

  <p>This may seem counterintuitive — the new evidence mostly reinforces the thesis. Here is the reasoning:</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Confidence Score Components</p>
    <table class="exhibit-table">
      <tr>
        <th>Factor</th>
        <th>AR-001 Assessment</th>
        <th>Updated Assessment</th>
        <th>Effect</th>
      </tr>
      <tr>
        <td>Evidence strength</td>
        <td>Strong (HBR, METR, UC Berkeley)</td>
        <td>Stronger (+ Anthropic 81%, ClawHavoc empirical data, AgentAuditor)</td>
        <td>+2%</td>
      </tr>
      <tr>
        <td>Source diversity</td>
        <td>24 sources, US/EU-centric</td>
        <td>+8 new sources, still US/EU-centric</td>
        <td>+1%</td>
      </tr>
      <tr>
        <td>Framework validation</td>
        <td>Trust Race Model unvalidated</td>
        <td>Still unvalidated (2 weeks is not validation)</td>
        <td>0%</td>
      </tr>
      <tr>
        <td>New complexity discovered</td>
        <td>Trust = capability vs. governance</td>
        <td>Trust = capability vs. governance <strong>+ supply chain</strong></td>
        <td>-3%</td>
      </tr>
      <tr>
        <td>Ecosystem stability</td>
        <td>Assumed stable open-source ecosystem</td>
        <td>Creator joined OpenAI; foundation governance TBD</td>
        <td>-2%</td>
      </tr>
    </table>
    <p class="exhibit-source">Net change: +3% (evidence) -5% (new complexity + uncertainty) = -2%. Revised: 71%.</p>
  </div>

  <p>The supply chain dimension is the key driver. AR-001 modeled trust as a two-body problem: capability vs. governance. ClawHavoc reveals it's at minimum a <strong>three-body problem</strong>: capability vs. governance vs. ecosystem security. Three-body problems are harder to solve — and harder to predict. Our model is less complete than we thought, even if the parts we modeled are correct. Intellectual honesty requires adjusting downward.<span class="badge badge-j">J</span></p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Move Confidence Up?</p>
    <p class="callout-body">External validation of the Trust Race Model by independent researchers. Production data showing adaptive governance reduces agent incidents. ClawHub implementing effective vetting that reduces malicious skills below 1%. Any of these would justify moving toward 80%.</p>
  </div>
</div>

<!-- ==================== SECTION 8: METHODOLOGY & REFERENCES ==================== -->
<div class="page" id="s8">
  <h2>8. Methodology & References</h2>

  <p>This update supplements AR-001 (v2.3, February 15, 2026) with evidence published between February 4–17, 2026. It does not replace the original report's analysis — it extends it. All section references to AR-001 use original section numbering.</p>

  <p><strong>New sources:</strong> 8 (5 web-sourced, 2 academic/arxiv, 1 internal research scan). Combined with AR-001's 24 sources, the total evidence base is 32 sources.</p>

  <p><strong>Limitations specific to this update:</strong></p>
  <ul>
    <li>ClawHavoc data is from a single security firm (Koi). Independent verification by a second firm would strengthen confidence. However, The Hacker News, SC Media, and eSecurity Planet all independently reported the findings.<sup>[S1][S6][S9][S10]</sup></li>
    <li>The Steinberger → OpenAI move is less than 24 hours old at time of writing. Long-term implications are speculative.</li>
    <li>AgentAuditor and AIRS-Bench are pre-print (arxiv). Neither has been applied in production enterprise settings.</li>
    <li>The 81% Anthropic statistic comes from Anthropic's own enterprise survey — a vendor with commercial interest in the conclusion that enterprises should deploy more agents.</li>
  </ul>

  <h3>References (This Update)</h3>

  <p class="reference-entry">[S1] Koi Security / Yomtov, O. (2026). "ClawHavoc: 341 Malicious Clawed Skills Found by the Bot They Were Targeting." Feb 4, 2026. Updated Feb 16, 2026. https://www.koi.ai/blog/clawhavoc-341-malicious-clawedbot-skills-found-by-the-bot-they-were-targeting Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S2] Yang, W. et al. (2026). "Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge." arXiv:2602.09341. Feb 10, 2026. Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S3] Pepe, A., Lupidi, A. et al. (2026). "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents." arXiv:2602.06855. Feb 6, 2026. Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S4] Business Insider. (2026). "OpenClaw's creator is heading to OpenAI." Feb 16, 2026. https://www.businessinsider.com/sam-altman-hires-openclaw-creator-peter-steinberger-personal-ai-agents-2026-2 Accessed: 2026-02-17. Corroborated by The Register, WinBuzzer, Deccan Herald, TrendingTopics.</p>

  <p class="reference-entry">[S5] Anthropic. (2026). "How enterprises are building AI agents in 2026." Claude Blog. Feb 2026. https://claude.com/blog/how-enterprises-are-building-ai-agents-in-2026 Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S6] The Hacker News. (2026). "Researchers Find 341 Malicious ClawHub Skills Stealing Data from OpenClaw Users." Feb 2026. https://thehackernews.com/2026/02/researchers-find-341-malicious-clawhub.html Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S7] Authmind. (2026). "OpenClaw Malicious Skills: Agentic AI Supply Chain Risk." https://www.authmind.com/post/openclaw-malicious-skills-agentic-ai-supply-chain Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S8] Ainary Research. (2026). "SOTA AI Agent Research Scan — 2026-02-17." Internal. Covers arXiv:2602.08234 (SkillRL), arXiv:2508.19828 (Memory-R1), arXiv:2602.06052 (Memory Survey), arXiv:2602.10090 (Agent World Model).</p>

  <p class="reference-entry">[S9] eSecurity Planet. (2026). "Hundreds of Malicious Skills Found in OpenClaw's ClawHub." https://www.esecurityplanet.com/threats/hundreds-of-malicious-skills-found-in-openclaws-clawhub/ Accessed: 2026-02-17.</p>

  <p class="reference-entry">[S10] SC Media. (2026). "OpenClaw agents targeted with 341 malicious ClawHub skills." https://www.scworld.com/news/openclaw-agents-targeted-with-341-malicious-clawhub-skills Accessed: 2026-02-17.</p>

  <p style="margin-top: 32px; font-size: 0.8rem; color: #888;">Cite as: Ainary Research. (2026). "State of AI Agent Trust — Q1 2026 Update." AR-036, v3.0. Supplement to AR-001.</p>

  <!-- ==================== ABOUT THIS REPORT ==================== -->
  <div class="author-section">
    <p class="author-label">About This Report</p>
    <p class="author-bio">AI strategy · research · implementation. By someone who built the systems first. This report was produced by Ainary's multi-agent research system. <a href="https://ainaryventures.com" style="color: #c8aa50;">ainaryventures.com</a></p>
  </div>
</div>

<!-- ==================== BACK COVER ==================== -->

<!-- ==================== TRANSPARENCY NOTE ==================== -->
<div class="page" id="transparency">
  <h2>9. Transparency Note</h2>

  <p class="transparency-intro" style="font-style: italic; color: #666; margin-bottom: 24px;">This section provides full methodology, known limitations, and conflict of interest disclosure.</p>

  <table class="how-to-read-table">
    <tr>
      <td><strong>Overall Confidence</strong></td>
      <td>71% (Medium-High). Justification: Mostly evidenced — this update tracks concrete developments (ClawHub malicious skills count, new arxiv papers, personnel moves) against the AR-001 baseline thesis.</td>
    </tr>
    <tr>
      <td><strong>Sources</strong></td>
      <td>32 sources total: 24 from AR-001 baseline + 8 new (5 web-sourced, 2 academic/arxiv, 1 internal research scan). Covers industry reports, academic papers, security disclosures, and practitioner analysis.</td>
    </tr>
    <tr>
      <td><strong>Strongest Evidence</strong></td>
      <td>ClawHavoc security disclosure (341 malicious skills, independently verifiable), AgentAuditor benchmark results (arxiv:2602.09341, peer-reviewable), Manus launch metrics (public product launch).</td>
    </tr>
    <tr>
      <td><strong>Weakest Point</strong></td>
      <td>The net confidence adjustment (-2% from 73% to 71%) is a subjective synthesis, not a mathematical derivation. The weighting of positive vs. negative signals is judgment-based.</td>
    </tr>
    <tr>
      <td><strong>What Would Invalidate</strong></td>
      <td>If (a) the ClawHub malicious skills are shown to be benign misclassifications, (b) the AR-001 Trust Race Model is externally validated (raising confidence), or (c) new evidence fundamentally contradicts the capability-governance gap thesis.</td>
    </tr>
  </table>

  <h3 style="margin-top: 2rem;">Methodology</h3>

  <p>This report followed the A+ Research Pipeline: independent research, source diversity audit, thesis development, and synthesis. 32 sources were collected and claims were extracted and classified using the [E]/[I]/[J]/[A] framework. The pipeline is a multi-agent system where research, validation, thesis development, and writing are performed by specialized agents that operate independently.</p>

  <h3>Limitations</h3>

  <ul>
    <li><strong>48-hour update window.</strong> This update covers developments within ~2 weeks of AR-001. Short time horizons amplify noise over signal.</li>
    <li><strong>Confidence adjustment is subjective.</strong> The -2% revision (73% → 71%) reflects the author's judgment, not a formal methodology.</li>
    <li><strong>Builds on AR-001's limitations.</strong> All limitations from the original report (no independent TCO data, benchmark-only multi-agent data, vendor-sponsored surveys) still apply.</li>
    <li><strong>Security disclosure sourcing.</strong> The ClawHavoc/Koi Security findings have not been independently replicated at time of publication.</li>
    <li><strong>Selection bias in updates.</strong> Only developments that affect the AR-001 thesis are covered; neutral developments are omitted.</li>
  </ul>

  <h3>Conflict of Interest</h3>

  <p>The publisher of this report researches, builds, and advises on AI agent systems — and has a commercial interest in the conclusions presented here. Evaluate evidence independently; claims marked [J] reflect judgment, not evidence.</p>
</div>

<div class="back-cover">
  <div style="margin-bottom: 48px;">
    <span class="gold-punkt" style="font-size: 24px;">●</span>
    <p class="brand-name" style="font-size: 1.2rem; margin-top: 8px;">Ainary</p>
  </div>
  <p class="back-cover-services">AI Strategy · System Design · Execution · Consultancy · Research</p>
  <p class="back-cover-cta"><a href="https://ainaryventures.com/contact.html" style="color: #888; text-decoration: none;">Contact</a> · <a href="https://ainaryventures.com/contact.html" style="color: #888; text-decoration: none;">Feedback</a></p>
  <p class="back-cover-contact">
    ainaryventures.com<br>
    florian@ainaryventures.com
  </p>
  <p style="font-size: 0.75rem; color: #aaa; margin-top: 48px;">© 2026 Ainary Ventures</p>
</div>

</body>
</html>
