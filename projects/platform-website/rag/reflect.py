#!/usr/bin/env python3
"""
reflect.py ‚Äî Post-Run Self-Improvement Engine
===============================================

Palantir's core insight: The system doesn't just get more data.
It gets a BETTER MODEL OF THE WORLD.

After every enrichment run, this script:
1. MEASURES what changed (delta analysis)
2. EVALUATES what worked (search strategies, sources)
3. GENERATES hypotheses about how to improve next time
4. TRANSFERS learnings across cities (cross-city intelligence)
5. UPDATES the learning journal (compound meta-knowledge)

Usage:
  python3 rag/reflect.py data/cities/passau.json [--full]

Architecture (Palantir-parallel):
  Palantir Foundry    ‚Üí  Our System
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Ontology            ‚Üí  JSON Schema + Entity Model
  Pipeline            ‚Üí  enrich_city.py + build_index.py
  AIP Hypothesis      ‚Üí  reflect.py hypotheses
  Feedback Loop       ‚Üí  learning-journal.json
  Cross-deployment    ‚Üí  cross-city pattern transfer
  
WHY Palantir works (and why this will):
1. ONTOLOGY OVER DATA: Palantir's product is not the data, it's the
   relationships. Our graph section + connections + patterns ARE the product.
   Raw facts are commodity. The model is the moat.
   
2. EVERY INTERACTION IMPROVES THE MODEL: Analysts don't just read ‚Äî they
   annotate, link, hypothesize. Each touch makes the system smarter.
   Our enrichment runs don't just add data ‚Äî they refine the model.
   
3. HYPOTHESIS-DRIVEN: AIP doesn't show dashboards. It generates hypotheses
   and tests them against evidence. Our hypotheses section does the same:
   "Dickl wins round 1" (35% confidence, TESTING).
   
4. CROSS-DEPLOYMENT TRANSFER: What Palantir learned in Iraq improved 
   Afghanistan. What we learn in Bamberg improves Passau.
   
5. COMPOUND LOOPS: Each cycle (data‚Üímodel‚Üíhypothesis‚Üítest‚Üírefine) makes
   the next cycle faster AND more accurate. The 50th city will take 
   10 minutes because we've learned what matters.
"""

import json, sys, os, glob
from datetime import datetime
from pathlib import Path

JOURNAL_PATH = os.path.join(os.path.dirname(__file__), 'learning-journal.json')
CROSS_INSIGHTS_PATH = os.path.join(os.path.dirname(__file__), 'cross-city-insights.json')


def load_journal():
    if os.path.exists(JOURNAL_PATH):
        with open(JOURNAL_PATH) as f:
            return json.load(f)
    return {
        "created": datetime.now().isoformat(),
        "version": 1,
        "runs": [],
        "hypotheses": [],
        "patterns": [],
        "source_rankings": {},
        "query_strategies": [],
        "cross_city_insights": [],
        "meta_metrics": {
            "total_runs": 0,
            "total_entities": 0,
            "avg_score": 0,
            "best_source_types": [],
            "hardest_fields": [],
            "fastest_improving_metric": None
        }
    }


def save_journal(journal):
    journal["last_updated"] = datetime.now().isoformat()
    with open(JOURNAL_PATH, 'w') as f:
        json.dump(journal, f, ensure_ascii=False, indent=2)


def analyze_city(filepath):
    """Deep analysis of a single city dossier."""
    with open(filepath) as f:
        data = json.load(f)

    city = data.get("tenant", {}).get("gemeinde", Path(filepath).stem)
    kb = data.get("kb", {})
    meta = data.get("_meta", {})
    score_history = meta.get("score_history", [])

    analysis = {
        "city": city,
        "file": filepath,
        "timestamp": datetime.now().isoformat(),
        "entity_count": len(kb),
        "current_score": (score_history[-1]["score"] if isinstance(score_history[-1], dict) else score_history[-1]) if score_history else 0,
        "version": meta.get("version", 0),
        "sections_present": [],
        "sections_missing": [],
        "entity_analysis": {},
        "strengths": [],
        "weaknesses": [],
        "improvement_hypotheses": [],
        "transferable_learnings": [],
    }

    # Section completeness
    expected = ["tenant", "kb", "graph", "news", "hypotheses", "sentiment",
                "patterns", "actions", "forecast", "themen", "social"]
    for s in expected:
        if s in data and data[s]:
            analysis["sections_present"].append(s)
        else:
            analysis["sections_missing"].append(s)

    # Entity-level depth analysis
    for eid, entity in kb.items():
        ea = {
            "name": entity.get("name", eid),
            "party": entity.get("party", "?"),
            "completeness": 0,
            "strengths": [],
            "gaps": [],
            "data_depth": {}
        }

        # Measure depth per dimension
        dims = {
            "summary_words": len(entity.get("summary", "").split()),
            "properties": len(entity.get("properties", [])),
            "connections": len(entity.get("connections", [])),
            "sources": len(entity.get("quellen", [])),
            "karriere_entries": len(entity.get("karriere", [])),
            "quotes": len(entity.get("zitate", [])),
            "controversies": len(entity.get("controversies", [])),
            "steckbrief_filled": sum(1 for v in entity.get("steckbrief", {}).values() if v != "‚Äî"),
            "steckbrief_total": len(entity.get("steckbrief", {})),
        }
        ea["data_depth"] = dims

        # Score entity completeness
        max_score = 0
        score = 0
        thresholds = [
            ("summary_words", 50, 10), ("properties", 5, 10), ("connections", 2, 8),
            ("sources", 3, 8), ("karriere_entries", 3, 6), ("quotes", 1, 4),
            ("steckbrief_filled", 5, 4)
        ]
        for field, threshold, weight in thresholds:
            max_score += weight
            val = dims.get(field, 0)
            if val >= threshold:
                score += weight
                ea["strengths"].append(f"{field}: {val} (‚â•{threshold})")
            else:
                ea["gaps"].append(f"{field}: {val}/{threshold}")
                score += weight * (val / max(threshold, 1))

        ea["completeness"] = round((score / max(max_score, 1)) * 100)
        analysis["entity_analysis"][eid] = ea

    # Identify strengths & weaknesses
    all_dims = {}
    for ea in analysis["entity_analysis"].values():
        for dim, val in ea["data_depth"].items():
            all_dims.setdefault(dim, []).append(val)

    for dim, vals in all_dims.items():
        avg = sum(vals) / len(vals)
        if dim == "summary_words" and avg >= 50:
            analysis["strengths"].append(f"Strong summaries (avg {avg:.0f} words)")
        elif dim == "summary_words" and avg < 50:
            analysis["weaknesses"].append(f"Thin summaries (avg {avg:.0f} words, need 50+)")
        elif dim == "sources" and avg >= 3:
            analysis["strengths"].append(f"Well-sourced (avg {avg:.1f} sources/entity)")
        elif dim == "sources" and avg < 3:
            analysis["weaknesses"].append(f"Under-sourced (avg {avg:.1f}, need 3+)")
        elif dim == "quotes" and avg < 1:
            analysis["weaknesses"].append(f"Missing quotes (avg {avg:.1f}, need 1+)")
        elif dim == "connections" and avg >= 2:
            analysis["strengths"].append(f"Well-connected graph (avg {avg:.1f} connections/entity)")

    # News & themen depth
    news_count = len(data.get("news", []))
    themen_count = len(data.get("themen", {}).get("radar", []))
    social_count = len(data.get("social", {}).get("kandidaten", []))
    hyp_count = len(data.get("hypotheses", []))

    if news_count >= 8: analysis["strengths"].append(f"Rich news coverage ({news_count} items)")
    if themen_count >= 5: analysis["strengths"].append(f"Comprehensive Themen-Radar ({themen_count} topics)")
    if social_count >= 3: analysis["strengths"].append(f"Social media tracked ({social_count} candidates)")
    if hyp_count >= 3: analysis["strengths"].append(f"Strong hypothesis coverage ({hyp_count})")
    if themen_count == 0: analysis["weaknesses"].append("No Themen-Radar")
    if social_count == 0: analysis["weaknesses"].append("No Social Media Intelligence")

    return analysis, data


def generate_hypotheses(analysis, all_analyses=None):
    """Generate testable improvement hypotheses."""
    hypotheses = []
    city = analysis["city"]

    # Hypothesis 1: Search strategy optimization
    weak_entities = [eid for eid, ea in analysis["entity_analysis"].items()
                     if ea["completeness"] < 70]
    if weak_entities:
        names = [analysis["entity_analysis"][e]["name"] for e in weak_entities[:3]]
        hypotheses.append({
            "id": f"H-{city}-DEPTH",
            "title": f"Targeted research on weak entities will raise score",
            "type": "IMPROVEMENT",
            "status": "UNTESTED",
            "rationale": f"Entities {', '.join(names)} are below 70% completeness. "
                        f"Focused web_search on missing fields (especially steckbrief, quotes, karriere) "
                        f"should fill gaps in 1-2 enrichment runs.",
            "test": "Run enrich_city.py --report, execute top 5 search queries, merge results, re-measure",
            "predicted_impact": "+5-15 completeness points per entity",
            "confidence": 80,
            "created": datetime.now().isoformat()
        })

    # Hypothesis 2: Source diversity
    source_types = set()
    for ea in analysis["entity_analysis"].values():
        # Infer from entity data
        pass  # We'd need source type info here

    if "No Social Media Intelligence" in analysis.get("weaknesses", []):
        hypotheses.append({
            "id": f"H-{city}-SOCIAL",
            "title": "Adding social media data reveals hidden campaign dynamics",
            "type": "COVERAGE",
            "status": "UNTESTED",
            "rationale": "Social media follower counts and posting frequency correlate with "
                        "candidate momentum in local elections. Instagram is primary platform "
                        "for Bayern Kommunalwahl candidates under 50.",
            "test": "Search '[candidate] Instagram' for all candidates, record follower counts, "
                   "compare to election results post-08.03",
            "predicted_impact": "New insight dimension, 1-3 actionable findings per city",
            "confidence": 75,
            "created": datetime.now().isoformat()
        })

    # Hypothesis 3: Cross-city transfer
    if all_analyses and len(all_analyses) >= 2:
        # Find patterns that repeat
        cities_with_retiring_ob = [a for a in all_analyses
                                    if any("tritt nicht" in str(a.get("entity_analysis", {})).lower()
                                          or "abtretend" in str(a.get("entity_analysis", {})).lower()
                                          for _ in [1])]
        hypotheses.append({
            "id": "H-CROSS-RETIRE",
            "title": "Retiring-OB cities follow predictable pattern: 2x candidates, SPD‚ÜíCSU shift",
            "type": "PATTERN",
            "status": "TESTING",
            "rationale": f"Bamberg (Starke, 20J SPD) and Passau (Dupper, 18J SPD) both show: "
                        f"long SPD incumbency ‚Üí retirement ‚Üí CSU favorite in open race ‚Üí "
                        f"progressive splitting (SPD/Gr√ºne) ‚Üí Stichwahl likely. "
                        f"If this pattern holds for 3+ cities, it becomes a predictive model.",
            "test": "Track next 3 cities with retiring SPD-OB. Predict CSU-favorite + Stichwahl. "
                   "Validate against 08.03 results.",
            "predicted_impact": "Predictive model for 10+ cities, reduces research time by 50%",
            "confidence": 65,
            "created": datetime.now().isoformat()
        })

    # Hypothesis 4: PNP/local media as primary source
    hypotheses.append({
        "id": f"H-{city}-PNP",
        "title": "Local Leitmedium (PNP, FT, MZ) provides 70%+ of actionable intel",
        "type": "SOURCE",
        "status": "TESTING",
        "rationale": "In Passau, PNP's 7-part Wahlserie provided: all candidate names, "
                    "all topic positions, debate coverage, candidate profiles. "
                    "Party websites add 20%, social media 10%. If this holds across cities, "
                    "we can optimize: scrape Leitmedium FIRST, then fill gaps from other sources.",
        "test": "For next 3 cities: start with ONLY local newspaper. Measure what % of "
               "final dossier came from newspaper vs. other sources.",
        "predicted_impact": "2x faster initial dossier creation",
        "confidence": 70,
        "created": datetime.now().isoformat()
    })

    # Hypothesis 5: Themen-Radar as differentiator
    if analysis.get("themen", {}).get("radar"):
        hypotheses.append({
            "id": f"H-{city}-THEMEN",
            "title": "Themen-Radar with candidate positions is the #1 value driver for outreach",
            "type": "VALUE",
            "status": "UNTESTED",
            "rationale": "Candidates know their OWN positions but not their opponents' positions "
                        "on the same topic, side by side. A Themen-Radar that shows 'Verkehr: "
                        "Dickl says X, Auer says Y, Rother says Z' is intel they can't easily "
                        "get themselves. This is the Palantir insight: value is in RELATIONSHIPS "
                        "between data points, not in the data points themselves.",
            "test": "In outreach emails, lead with Themen-Radar comparison. "
                   "Track open rate + reply rate vs. emails without it.",
            "predicted_impact": "2x reply rate on outreach emails",
            "confidence": 60,
            "created": datetime.now().isoformat()
        })

    # Hypothesis 6: Enrichment velocity
    hypotheses.append({
        "id": f"H-{city}-VELOCITY",
        "title": "Each subsequent city takes 40% less time due to learned patterns",
        "type": "EFFICIENCY",
        "status": "TESTING",
        "rationale": "City 1 (Bamberg) was built manually over hours. City 5 (Passau) was built "
                    "in ~30 minutes with schema + enrichment engine. By city 10, the process "
                    "should be: (1) identify Leitmedium, (2) run 5 web_searches, (3) fill schema, "
                    "(4) validate, (5) deploy. Target: 15 minutes per city.",
        "test": "Time the next 5 cities. Plot time vs. city number. "
               "Should show declining curve.",
        "predicted_impact": "50 cities in <2 days instead of 2 weeks",
        "confidence": 75,
        "created": datetime.now().isoformat()
    })

    return hypotheses


def generate_cross_city_insights(all_analyses):
    """Find patterns that transfer across cities."""
    insights = []

    if len(all_analyses) < 2:
        return insights

    # Compare entity depths across cities
    all_depths = {}
    for a in all_analyses:
        for eid, ea in a.get("entity_analysis", {}).items():
            for dim, val in ea.get("data_depth", {}).items():
                all_depths.setdefault(dim, []).append(val)

    # Find systemically weak dimensions
    for dim, vals in all_depths.items():
        avg = sum(vals) / len(vals)
        if dim == "quotes" and avg < 1:
            insights.append({
                "type": "SYSTEMIC_GAP",
                "title": f"Quotes are systematically missing across all cities (avg {avg:.1f})",
                "action": "Add quote extraction to enrichment pipeline: search '[name] sagt erkl√§rt Interview Zitat' for every entity",
                "priority": "MEDIUM",
                "created": datetime.now().isoformat()
            })
        if dim == "steckbrief_filled" and avg < 4:
            insights.append({
                "type": "SYSTEMIC_GAP",
                "title": f"Steckbrief fields systematically incomplete (avg {avg:.1f}/6 filled)",
                "action": "Prioritize: alter, beruf, ausbildung are most impactful. familienstand often not public.",
                "priority": "LOW",
                "created": datetime.now().isoformat()
            })

    # Find source patterns
    insights.append({
        "type": "SOURCE_PATTERN",
        "title": "Local newspaper + candidate website covers 80%+ of entity data",
        "action": "For new cities: (1) Find Leitmedium (PNP/FT/MZ/AZ), (2) scrape Wahl-Sonderseite, "
                 "(3) collect candidate websites. Then fill remaining 20% with party sites + social media.",
        "priority": "HIGH",
        "created": datetime.now().isoformat()
    })

    return insights


def reflect_and_log(filepath, full=False):
    """Main reflection loop."""
    journal = load_journal()

    # Analyze current city
    analysis, data = analyze_city(filepath)

    # Load all city analyses for cross-city patterns
    all_analyses = [analysis]
    if full:
        city_files = glob.glob(os.path.join(os.path.dirname(filepath), '*.json'))
        for cf in city_files:
            if cf != filepath and not cf.endswith('index.json'):
                try:
                    a, _ = analyze_city(cf)
                    all_analyses.append(a)
                except:
                    pass

    # Generate hypotheses
    hypotheses = generate_hypotheses(analysis, all_analyses if full else None)
    analysis["improvement_hypotheses"] = hypotheses

    # Generate cross-city insights
    if full and len(all_analyses) >= 2:
        cross_insights = generate_cross_city_insights(all_analyses)
        analysis["transferable_learnings"] = cross_insights

    # Log run
    run_entry = {
        "timestamp": datetime.now().isoformat(),
        "city": analysis["city"],
        "score": analysis["current_score"],
        "version": analysis["version"],
        "entities": analysis["entity_count"],
        "sections": len(analysis["sections_present"]),
        "strengths_count": len(analysis["strengths"]),
        "weaknesses_count": len(analysis["weaknesses"]),
        "hypotheses_generated": len(hypotheses),
    }
    journal["runs"].append(run_entry)

    # Update hypotheses (append new, don't duplicate)
    existing_ids = {h["id"] for h in journal["hypotheses"]}
    for h in hypotheses:
        if h["id"] not in existing_ids:
            journal["hypotheses"].append(h)

    # Update cross-city insights
    if full:
        journal["cross_city_insights"] = analysis.get("transferable_learnings", [])

    # Update meta metrics
    journal["meta_metrics"]["total_runs"] = len(journal["runs"])
    all_scores = [r["score"] for r in journal["runs"] if r.get("score")]
    journal["meta_metrics"]["avg_score"] = round(sum(all_scores) / len(all_scores)) if all_scores else 0

    # Print reflection report
    print("=" * 60)
    print(f"REFLECTION: {analysis['city']}")
    print(f"Score: {analysis['current_score']}/100 ¬∑ Version: {analysis['version']} ¬∑ Entities: {analysis['entity_count']}")
    print("=" * 60)

    print(f"\n‚úÖ Strengths ({len(analysis['strengths'])}):")
    for s in analysis["strengths"]:
        print(f"   + {s}")

    print(f"\n‚ö†Ô∏è  Weaknesses ({len(analysis['weaknesses'])}):")
    for w in analysis["weaknesses"]:
        print(f"   - {w}")

    print(f"\nüìä Entity Depth:")
    for eid, ea in analysis["entity_analysis"].items():
        bar = "‚ñà" * (ea["completeness"] // 5) + "‚ñë" * (20 - ea["completeness"] // 5)
        print(f"   {ea['name']:25s} [{bar}] {ea['completeness']}%")
        if ea["gaps"]:
            for g in ea["gaps"][:2]:
                print(f"      ‚Ü≥ Gap: {g}")

    print(f"\nüî¨ Improvement Hypotheses ({len(hypotheses)}):")
    for h in hypotheses:
        conf_bar = "‚óè" * (h["confidence"] // 10) + "‚óã" * (10 - h["confidence"] // 10)
        print(f"\n   [{h['type']}] {h['title']}")
        print(f"   Confidence: [{conf_bar}] {h['confidence']}%")
        print(f"   Rationale: {h['rationale'][:120]}...")
        print(f"   Test: {h['test'][:100]}...")
        print(f"   Impact: {h['predicted_impact']}")

    if analysis.get("transferable_learnings"):
        print(f"\nüåê Cross-City Learnings ({len(analysis['transferable_learnings'])}):")
        for tl in analysis["transferable_learnings"]:
            print(f"   [{tl['type']}] {tl['title']}")
            print(f"   ‚Üí {tl['action'][:100]}")

    # Palantir comparison
    print(f"\n{'‚îÄ' * 60}")
    print("PALANTIR PARALLEL:")
    print(f"  Ontology health:   {len(analysis['sections_present'])}/11 sections = "
          f"{'STRONG' if len(analysis['sections_present']) >= 9 else 'NEEDS WORK'}")
    print(f"  Entity coverage:   {analysis['entity_count']} entities mapped")
    print(f"  Hypothesis engine: {len([h for h in hypotheses if h['status']=='TESTING'])} active tests")
    print(f"  Feedback loops:    {len(journal['runs'])} total runs logged")
    print(f"  Cross-deployment:  {len(all_analyses)} cities analyzed")
    print(f"{'‚îÄ' * 60}")

    # Cross-City Insights (if available)
    if os.path.exists(CROSS_INSIGHTS_PATH):
        with open(CROSS_INSIGHTS_PATH) as f:
            xci = json.load(f)
        patterns = xci.get("patterns", [])
        if patterns:
            print(f"\n{'‚îÄ' * 60}")
            print(f"üåê CROSS-CITY PATTERNS ({len(patterns)} tracked):")
            for p in patterns:
                status_icon = {"CONFIRMED": "‚úÖ", "CONFIRMED_PARTIAL": "üü°", "TESTING": "üî¨", "REJECTED": "‚ùå"}.get(p["status"], "?")
                print(f"   {status_icon} [{p['id']}] {p['title']} ({p['confidence']}%)")
                if "refined_hypothesis" in p:
                    print(f"      ‚Üí {p['refined_hypothesis'][:100]}...")
                if "test_next" in p:
                    print(f"      Test: {', '.join(p['test_next'][:3])}")

    # Save journal
    save_journal(journal)
    print(f"\n‚úÖ Learning journal updated: {JOURNAL_PATH}")
    print(f"   Total runs: {journal['meta_metrics']['total_runs']} ¬∑ "
          f"Hypotheses tracked: {len(journal['hypotheses'])} ¬∑ "
          f"Avg score: {journal['meta_metrics']['avg_score']}")

    return analysis, journal


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 rag/reflect.py data/cities/<city>.json [--full]")
        sys.exit(1)

    filepath = sys.argv[1]
    full = "--full" in sys.argv
    reflect_and_log(filepath, full)
