# Quality, Process, Leadership & AI Management
## Research Compendium — Actionable Frameworks for Small Teams

*Compiled: 2026-02-05*

---

# 1. Achieving Quality — Deming, Six Sigma, TQM

## 1.1 W. Edwards Deming's 14 Points for Management

Deming's 14 Points are the foundation of modern quality thinking. Originally written for post-war Japanese manufacturing, they remain startlingly relevant for any team that produces work.

**The Full 14 Points:**

1. **Create constancy of purpose** toward improvement of product and service
2. **Adopt the new philosophy** — take on leadership for change
3. **Cease dependence on inspection** — build quality into the product in the first place
4. **End awarding business on price tag alone** — minimize total cost, build long-term supplier relationships
5. **Improve constantly and forever** the system of production and service
6. **Institute training on the job**
7. **Institute leadership** — supervision should help people do a better job
8. **Drive out fear** — so everyone may work effectively
9. **Break down barriers between departments** — work as a team
10. **Eliminate slogans, exhortations, and targets** for the workforce — they create adversarial relationships
11. **Eliminate quotas and management by numbers** — substitute leadership
12. **Remove barriers to pride of workmanship** — abolish annual merit ratings
13. **Institute a vigorous program of education and self-improvement**
14. **Put everybody to work on the transformation** — it's everybody's job

**Source:** [Deming Institute — 14 Points](https://deming.org/explore/fourteen-points/) | Deming, W.E. *Out of the Crisis* (MIT Press)

### What's Relevant for a 2-Person Team?

| Deming Point | Small Team Translation |
|---|---|
| #3 Cease dependence on inspection | Don't review output at the end. Build quality into the *process* — templates, checklists, clear specs upfront |
| #5 Improve constantly | Weekly retros: "What worked? What didn't? What do we change?" Even 15 minutes |
| #7 Institute leadership | Lead by enabling, not controlling. Remove blockers for your partner |
| #8 Drive out fear | Create psychological safety so both people speak up about problems immediately |
| #10 Eliminate slogans | Don't set targets without giving people the system/tools to achieve them |
| #13 Education & self-improvement | Budget time for learning. It's not overhead — it's investment |
| #14 Transformation is everybody's job | Both people own quality. It's not one person checking the other's work |

### Deming's Core Insight for Knowledge Work
> "Quality is made in the boardroom. A worker can deliver lower quality, but a worker alone cannot deliver quality. It's the system that determines quality."

**The system** = your processes, tools, communication patterns, feedback loops. Fix the system, not the person.

---

## 1.2 Six Sigma DMAIC — Applied to Knowledge Work

Six Sigma's DMAIC cycle is the gold standard for process improvement. Originally from Motorola/GE for manufacturing defect reduction, it translates powerfully to knowledge work.

**The 5 Phases:**

| Phase | Definition | Knowledge Work Application |
|---|---|---|
| **D — Define** | What's the problem? Who's the customer? What does "good" look like? | Write a 1-pager: What are we trying to achieve? What does the client/user actually need? Define "done" explicitly |
| **M — Measure** | How does the current process perform? What data do we have? | Track: time per task, error rate, rework rate, client satisfaction, turnaround time |
| **A — Analyze** | What are the root causes of problems? | 5 Whys. Pareto analysis (which 20% of errors cause 80% of rework?). Where does work get stuck? |
| **I — Improve** | Design and implement solutions | Change the process, add checklists, automate a step, improve templates, eliminate handoffs |
| **C — Control** | Sustain the improvement | Document the new process. Create dashboards. Schedule periodic reviews to prevent backsliding |

### DMAIC for a Small Team — Lightweight Version:
1. **Define:** Before starting any project, write a 3-sentence brief: *Who needs this? What does success look like? What's out of scope?*
2. **Measure:** After every project, note: How long did it take? Was it right the first time? Did the client request changes?
3. **Analyze:** Monthly, review your notes. What patterns emerge? Where do you keep getting it wrong?
4. **Improve:** Pick ONE thing to fix each month. Don't boil the ocean.
5. **Control:** Update your SOP/template. Make the improvement the default.

**Source:** [ASQ — DMAIC Process](https://asq.org/quality-resources/dmaic) | [GoLeanSixSigma — DMAIC Phases](https://goleansixsigma.com/dmaic-five-basic-phases-of-lean-six-sigma/)

---

## 1.3 Toyota Production System (TPS) — Lessons for a 2-Person Team

Toyota's system isn't about cars. It's about *respect for people + continuous improvement*. The core pillars:

### Two Pillars of TPS

**1. Just-in-Time (JIT):** Produce what is needed, when it's needed, in the amount needed.
- **For a small team:** Don't batch work. Don't hoard inventory (unfinished drafts, half-done tasks). Complete things. Flow > volume.
- **Practical:** Work on fewer things simultaneously. Finish one deliverable before starting the next.

**2. Jidoka (Automation with a Human Touch):** Stop production when problems occur.
- **For a small team:** When you spot a quality issue, STOP. Don't push forward hoping it'll be fine. Fix it now while it's cheap.
- **Practical:** If you realize a deliverable is going in the wrong direction — pause, discuss, realign. 10 minutes now saves 10 hours later.

### Key TPS Concepts for Small Teams

| Concept | What It Means | How to Apply It |
|---|---|---|
| **Kaizen** | Continuous small improvements | End every week with "What's one small thing we can improve?" |
| **Genchi Genbutsu** | "Go and see" — understand problems at the source | Don't guess what's wrong. Look at the actual work, talk to the actual user |
| **Andon** | Pull the cord to stop the line when there's a problem | Make it safe (and expected) to flag problems immediately. No punishment for stopping. |
| **Muda** (Waste) | Eliminate waste in 7 categories | Identify: unnecessary meetings, context switching, waiting, rework, over-processing |
| **Poka-Yoke** | Mistake-proofing | Build templates, checklists, and automations that make errors impossible |
| **5S** | Sort, Set in order, Shine, Standardize, Sustain | Keep your digital workspace organized. Standardize file naming. Clean your tools weekly. |

### The 7 Wastes of Knowledge Work

Toyota identified 7 wastes (*muda*). Adapted for knowledge work:

1. **Overproduction** — Creating reports/features nobody asked for
2. **Waiting** — Blocked by approvals, unclear priorities, missing info
3. **Transportation** — Unnecessary handoffs between people/tools
4. **Over-processing** — Polishing a draft to 100% when 80% was sufficient
5. **Inventory** — Too many projects in progress simultaneously (WIP)
6. **Motion** — Switching between apps, searching for files, context switching
7. **Defects** — Rework due to misunderstanding requirements or sloppy execution

**Source:** [Wikipedia — Toyota Production System](https://en.wikipedia.org/wiki/Toyota_Production_System) | [Marshall University — Lean Manufacturing & Toyota](https://www.mfg.marshall.edu/lean-manufacturing-made-toyota-the-success-story-it-is-today/)

---

## 1.4 Quality as a System, Not an Inspection

### The Key Frameworks

**TQM (Total Quality Management)** — The overarching philosophy:
- Quality is embedded in every process, from design through delivery
- Not inspected at the end; built in from the start
- Everyone owns quality — not a separate QA team
- Continuous improvement is the engine
- Customer focus drives everything

**Source:** [SimplerQMS — Total Quality Management](https://simplerqms.com/total-quality-management/) | [PMI — TQM Practical Guide](https://www.pmi.org/learning/library/total-quality-management-practical-guide-4782)

### Quality System Cheat Sheet for Small Teams

```
INSTEAD OF:                          DO THIS:
─────────────────────────────────────────────────────
Review output at the end      →     Define quality criteria upfront
One person checks another     →     Build quality into the template
Hope the deliverable is good  →     Test against user needs early
Fix problems when found       →     Design processes that prevent them
Blame the person              →     Fix the system
```

### The Quality Hierarchy (Bottom → Top)

1. **Inspection** — Catch defects after production (worst: expensive, reactive)
2. **Quality Control** — Monitor processes and test outputs during production
3. **Quality Assurance** — Design processes that prevent defects
4. **Quality Culture** — Everyone owns quality, continuous improvement is reflexive (best: proactive, systemic)

**Goal:** Operate at Level 3-4. Small teams can skip the bureaucracy of formal QA and go straight to building quality into culture and process.

---

# 2. Process Definition

## 2.1 How to Define a Repeatable Process

### The SOP / Checklist / Playbook Hierarchy

| Tool | What It Is | When to Use |
|---|---|---|
| **SOP (Standard Operating Procedure)** | Step-by-step instructions for a specific task | Recurring tasks that must be done the same way every time (onboarding a client, deploying code) |
| **Checklist** | A list of critical items to verify | Before shipping any deliverable. "Did I do X? Did I check Y?" |
| **Playbook** | A collection of strategies, tactics, and SOPs for a domain | "How we do sales," "How we run client projects" |
| **Template** | A pre-filled starting point | Proposals, reports, emails, briefs — anything you do more than twice |
| **Workflow** | A visual map of who does what, in what order | Multi-step processes with handoffs |

### How to Build an SOP in 30 Minutes

1. **Do the task once** while recording every step (screen record or bullet list)
2. **Strip out the noise** — keep only the steps that matter
3. **Add decision points** — "If X, do Y. If not, do Z."
4. **Test it** — can someone else follow this without asking questions?
5. **Store it** — one central location (Notion, Obsidian, Google Docs)
6. **Date it** — review every quarter. Processes rot.

### The "If it happens 3x, SOP it" Rule:
> If you've done the same task three times, the fourth time should start with writing an SOP. Your future self (and your team) will thank you.

---

## 2.2 The Checklist Manifesto — Key Insights (Atul Gawande)

Gawande's research across medicine, aviation, and construction reveals:

### The Core Problem
Modern work has become too complex for memory alone. We fail not because we don't know enough (*ignorance*), but because we don't consistently apply what we know (*ineptitude*). The solution: checklists.

### The Boeing Story
In 1935, Boeing's next-generation bomber crashed due to pilot error — the plane was too complex for any single pilot's memory. Instead of more training, pilots created a simple checklist. Result: 1.8 million miles flown without a single accident.

### Two Types of Checklists

1. **DO-CONFIRM checklist:** Do the work from memory, then pause and verify against the checklist that nothing was missed
2. **READ-DO checklist:** Read each step and do it sequentially (like a recipe)

### Gawande's Rules for Good Checklists

- **Keep it short** — 5-9 items maximum. If it's too long, people won't use it.
- **Focus on the "killer items"** — the steps that, if missed, cause the most damage
- **Don't try to spell out everything** — a checklist is not an SOP. It's a safety net for critical steps.
- **Include a communication step** — the most powerful checklists force people to talk to each other (like the surgical team timeout)
- **Test and iterate** — no checklist is right the first time
- **Use it** — discipline, not inspiration, makes it work

### Why People Resist Checklists
- They feel insulting to experts ("I don't need a list, I know what I'm doing")
- They seem too simple to work
- They require admitting our fallibility

**But:** The WHO surgical safety checklist reduced surgical complications by **36%** and deaths by **47%** in hospitals worldwide.

**Source:** [Gawande — The Checklist Manifesto](https://atulgawande.com/book/the-checklist-manifesto/) | [Runn — 8 Lessons Summary](https://www.runn.io/blog/the-checklist-manifesto-summary)

---

## 2.3 When to Standardize vs. When to Stay Flexible

### The Standardization Spectrum

```
FULLY STANDARDIZED          STRUCTURED FLEXIBLE           FULLY CREATIVE
(Assembly Line)             (Jazz with Sheet Music)        (Improv Comedy)
│                           │                              │
SOP for every step          Core steps locked +            Guidelines only,
No deviation               flex on details                 adapt in real-time
│                           │                              │
Accounting, compliance,     Client projects,               Strategy, creative,
data entry, onboarding      research, content              brainstorming
```

### Decision Framework: What to Standardize

| Standardize When... | Stay Flexible When... |
|---|---|
| The task is repeated frequently | The task requires creative judgment |
| Errors have high cost | Context changes every time |
| Multiple people need to do it consistently | The process is still evolving (too early to lock down) |
| There's a "right way" that's been validated | Rigid rules would produce worse outcomes |
| You want to free up mental energy for creative work | You need to adapt to individual client/user needs |

### The 80/20 Standardization Rule:
> Standardize the 80% that's routine (setup, formatting, delivery, communication templates). Keep the 20% that's judgment flexible.

### How Top Consulting Firms Define Process

**McKinsey's MECE Principle:**
- **Mutually Exclusive, Collectively Exhaustive** — break any problem into buckets that don't overlap and together cover everything
- Invented by Barbara Minto at McKinsey in the 1960s
- Used to structure problem-solving: create "issue trees" that decompose complex problems into solvable sub-problems
- Each bucket can be delegated as a workstream

**Source:** [Wikipedia — MECE Principle](https://en.wikipedia.org/wiki/MECE_principle) | [StrategyU — WTF is MECE](https://strategyu.co/wtf-is-mece-mutually-exclusive-collectively-exhaustive/)

**McKinsey's Structured Problem Solving Process:**
1. Define the problem (as a question)
2. Structure it (MECE issue tree)
3. Prioritize (80/20 — where's the most impact?)
4. Develop hypotheses
5. Gather evidence
6. Synthesize findings
7. Deliver the answer (pyramid principle: answer first, then evidence)

**The Pyramid Principle (Barbara Minto):**
- Start with the answer/recommendation
- Support with 3 key arguments
- Back each argument with data
- Top-down communication = executive-friendly

**Bain's Process Approach:**
- Results-oriented: every workstream has a clear deliverable
- "Answer first" — form a hypothesis and test it, rather than boiling the ocean
- True North: define the ideal state, then work backward to today
- Two-week sprint cycles for consulting engagements

### For a Small Team:
1. **Write your processes down** — even a bullet list in a shared doc counts
2. **Use MECE to delegate** — split work into clean, non-overlapping buckets
3. **Answer first** — before doing any research, write your hypothesis. Then test it.
4. **Review processes quarterly** — are they still serving you?

---

# 3. Lean Startup — Eric Ries + Steve Blank

## 3.1 Build-Measure-Learn Loop

Eric Ries' central framework from *The Lean Startup*:

```
         ┌──────────┐
         │   IDEAS   │
         └────┬─────┘
              │ BUILD
              ▼
         ┌──────────┐
         │  PRODUCT  │ (MVP)
         └────┬─────┘
              │ MEASURE
              ▼
         ┌──────────┐
         │   DATA    │
         └────┬─────┘
              │ LEARN
              ▼
         ┌──────────┐
         │ INSIGHTS  │──→ Pivot or Persevere?
         └──────────┘          │
              ▲                │
              └────────────────┘
```

### Practical Application

> "The fundamental activity of a startup is to turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere. All successful startup processes should be geared to accelerate that feedback loop." — Eric Ries

**The loop is meant to be run FAST.** Speed through the loop = competitive advantage.

| Step | What to Do | Common Mistake |
|---|---|---|
| **Build** | Create the minimum thing needed to test your hypothesis | Building too much. Over-engineering. Adding features before validating the core. |
| **Measure** | Track whether your hypothesis was correct. Use actionable metrics (not vanity metrics). | Measuring the wrong things (page views vs. activation rate) |
| **Learn** | Extract validated learning. Update your mental model. Decide: pivot or persevere. | Not being honest about what the data says. Confirmation bias. |

### The Key Insight:
> **Start with LEARN.** What do you need to learn? Then figure out what to measure. Then build only what you need to get that measurement.

Most people go Build → Measure → Learn (forward). The best practitioners think **Learn → Measure → Build** (backward). What do I need to learn? → What data would prove/disprove it? → What's the minimum product/test to get that data?

**Source:** [The Lean Startup — Principles](https://theleanstartup.com/principles) | Ries, E. *The Lean Startup* (2011)

---

## 3.2 MVP Thinking — What's the Minimum to Validate?

### What an MVP Is (and Isn't)

| MVP IS... | MVP IS NOT... |
|---|---|
| The smallest experiment to test your riskiest assumption | A crappy version 1.0 of your product |
| A learning vehicle | A marketing beta |
| Designed to answer: "Should we build this?" | Designed to answer: "Can we build this?" |
| Often embarrassingly simple | A polished prototype |

### Types of MVPs

1. **Landing page MVP** — Describe the product, see if people sign up
2. **Concierge MVP** — Deliver the service manually to a few customers (pretend you have a product)
3. **Wizard of Oz MVP** — Looks automated from the front, humans doing the work in the back
4. **Explainer video MVP** — Dropbox famously validated demand with a video before building the product
5. **Piecemeal MVP** — Stitch together existing tools (Zapier + Notion + email) to simulate the product
6. **Single-feature MVP** — Build only the one feature that matters most and ship it

### MVP Decision Tree:
```
What's your riskiest assumption?
│
├── "People have this problem" → Talk to 10 people. That's your MVP.
├── "People will pay for this" → Landing page + price + waitlist
├── "We can build this" → Technical spike / prototype
└── "This solution works" → Concierge MVP with 5 customers
```

---

## 3.3 Pivot vs. Persevere Decision Framework

### When to Pivot

A pivot is a **structured course correction** — changing one element of your strategy while keeping others. It's not failure; it's learning.

**Signs you should pivot:**
- Your metrics are flat despite multiple iterations
- Customers use your product but not for the reason you expected
- Customer acquisition cost is unsustainable
- You've talked to 20+ prospects and can't close
- The market has shifted

**Signs you should persevere:**
- Metrics are improving, even slowly
- Small but passionate user base (10 users who love you > 1000 who are indifferent)
- You haven't given the current approach enough time/effort
- Feedback is "I love this, but..." (fixable problems)

### Types of Pivots (Eric Ries)

1. **Zoom-in pivot** — One feature becomes the whole product
2. **Zoom-out pivot** — The whole product becomes one feature of a larger product
3. **Customer segment pivot** — Right product, wrong customer
4. **Customer need pivot** — Right customer, wrong problem
5. **Platform pivot** — Switch from app to platform (or vice versa)
6. **Business model pivot** — Change how you make money
7. **Channel pivot** — Change how you reach customers
8. **Technology pivot** — Same solution, different tech

---

## 3.4 Customer Development (Steve Blank's 4 Steps)

> "A startup is a temporary organization designed to search for a repeatable and scalable business model."

### The Four Steps to the Epiphany

| Step | Phase | What You Do |
|---|---|---|
| **1. Customer Discovery** | SEARCH | Turn founder's vision into hypotheses. Test customer reactions. Find problem-solution fit. |
| **2. Customer Validation** | SEARCH | Test if the business model is repeatable and scalable. Can you sell it? If not, go back to Step 1. |
| **3. Customer Creation** | EXECUTE | Build end-user demand. Drive demand into your sales channel. Scale. |
| **4. Company Building** | EXECUTE | Transition from startup to company. Build departments, hire, formalize. |

### The Critical Insight:
> Steps 1 and 2 are about **search**. Steps 3 and 4 are about **execution**. Most startups fail because they skip to execution before completing the search.

### Customer Discovery in Practice:
1. Write your hypotheses (Business Model Canvas)
2. Get out of the building — talk to real potential customers
3. Test your hypotheses: Do they have this problem? How do they solve it today? Would they pay for your solution?
4. Iterate until you have evidence, not just opinions
5. Key question: "If this product existed today, would you buy it? Can I have your credit card?"

**Source:** [Steve Blank — Customer Development](https://steveblank.com/) | [Four Stages of Customer Development](https://www.driverlesscrocodile.com/organisational-development/steve-blank-on-the-four-stages-of-customer-development/) | Blank, S. *The Four Steps to the Epiphany*

---

# 4. Leading People — HBR Best Practices

## 4.1 Situational Leadership (Hersey & Blanchard)

The core insight: **there is no single best leadership style.** The best style depends on the follower's maturity/readiness level.

### The Four Leadership Styles

| Style | Behavior | Use When... |
|---|---|---|
| **S1 — Telling** | High direction, low support. Give specific instructions. | Follower is new, unskilled, or unwilling. They need clear direction. (M1 — Low competence, low commitment) |
| **S2 — Selling** | High direction, high support. Explain decisions, provide coaching. | Follower is developing. Some competence but still needs guidance and motivation. (M2 — Some competence, variable commitment) |
| **S3 — Participating** | Low direction, high support. Facilitate and share decision-making. | Follower is capable but may lack confidence or motivation. (M3 — High competence, variable commitment) |
| **S4 — Delegating** | Low direction, low support. Hand off responsibility. | Follower is experienced and self-motivated. They've got this. (M4 — High competence, high commitment) |

### Application: Managing AI Agents (and Sub-Agents)

This maps directly to AI agent management:

| AI Agent Maturity | Leadership Style | What It Looks Like |
|---|---|---|
| **New/untested agent** | S1 — Telling | Detailed prompts, explicit instructions, tight constraints, full output review |
| **Agent with some track record** | S2 — Selling | Clear goals + context, explain the "why," review output but allow some autonomy |
| **Proven agent on familiar tasks** | S3 — Participating | Define the outcome, let the agent choose the approach, spot-check results |
| **Highly reliable agent** | S4 — Delegating | State the goal, trust the agent, review exceptions only |

**Source:** [Investopedia — Situational Leadership](https://www.investopedia.com/terms/h/hersey-and-blanchard-model.asp) | [Business-to-you — Hersey & Blanchard Model](https://www.business-to-you.com/hersey-blanchard-situational-leadership-model/)

---

## 4.2 Radical Candor (Kim Scott)

### The 2x2 Framework

```
                    CHALLENGE DIRECTLY
                          ↑
                          │
    Obnoxious         RADICAL         
    Aggression ◄───── CANDOR ─────► Goal
                          │
    Manipulative      Ruinous
    Insincerity ◄──── Empathy
                          │
                          ↓
                 DON'T CHALLENGE
         ←─── CARE PERSONALLY ───→
         Don't                   Do
```

### The Four Quadrants

| Quadrant | You Care? | You Challenge? | What It Looks Like |
|---|---|---|---|
| **Radical Candor** ✅ | Yes | Yes | "I care about your success, and here's what I honestly think you can improve" |
| **Obnoxious Aggression** | No | Yes | Brutal honesty without empathy. "This is garbage." |
| **Ruinous Empathy** | Yes | No | You're too nice to give honest feedback. Problems fester. |
| **Manipulative Insincerity** | No | No | Backstabbing. Fake praise. Passive-aggressive behavior. |

### Practical Application

**For Praise (Radical Candor style):**
- Be **specific** and **sincere**: "The way you structured that analysis made the recommendation much clearer — especially the comparison table on page 3"
- NOT: "Great job!" (too vague) or "You're so smart" (not actionable)

**For Criticism (Radical Candor style):**
- Be **kind** and **clear**: "I noticed the client brief missed the budget section. That's caused rework on the last two projects. Can we add it to our template?"
- NOT: "You always miss things" (unkind) or "The brief was... fine, I guess" (unclear)

**Kim Scott's Order of Operations:**
1. Ask for feedback before giving it
2. Give praise (specific, sincere) first
3. Give criticism (kind, clear, in private) second
4. Make it safe to disagree

**Source:** [Radical Candor — Our Approach](https://www.radicalcandor.com/our-approach) | [The Manager's Handbook — Radical Candor](https://themanagershandbook.com/coaching-and-feedback/radical-candor)

---

## 4.3 Psychological Safety (Amy Edmondson)

### Definition
> "A shared belief held by members of a team that the team is safe for interpersonal risk taking." — Amy Edmondson, Harvard Business School

When people feel psychologically safe, they:
- Speak up about problems
- Ask for help without stigma
- Admit mistakes and learn from them
- Take risks and innovate
- Challenge ideas constructively

### Why It Matters (The Data)

- **Google's Project Aristotle** (2012): Identified psychological safety as the **#1 predictor** of team success — above dependability, structure, meaning, or impact
- **WHO Surgical Checklist:** Hospitals with psychological safety saw dramatically better patient outcomes because staff flagged problems
- **Edmondson's research:** Teams with psychological safety have higher learning rates, better performance, and more innovation

### Amy Edmondson's 4 Steps to Build Psychological Safety

1. **Encourage teams to bond through day-to-day tasks** — shared work creates trust. Do the work together, not in silos.
2. **Normalize learning from mistakes** — create regular "what went wrong and what did we learn?" reviews. Make it a ritual, not a post-mortem.
3. **Ensure all people feel "seen"** — acknowledge contributions, listen actively, make space for different perspectives.
4. **Model vulnerability as a leader** — admit your own mistakes first. Say "I don't know" and "I need help." This gives permission to everyone else.

### Psychological Safety ≠ Being Nice
It's NOT about:
- Avoiding conflict
- Lowering standards
- Being comfortable all the time

It IS about:
- Making it safe to be honest
- Setting high standards AND supporting people in meeting them
- Productive disagreement

### The Leader's Toolkit for Psychological Safety

| Behavior | Why It Works |
|---|---|
| "What am I missing?" | Invites others to challenge you |
| "Thank you for telling me" (when hearing bad news) | Reinforces that speaking up is valued |
| "I made a mistake here..." | Models vulnerability |
| "How can I help?" | Shifts from judgment to support |
| Celebrate "intelligent failures" | Makes experimentation safe |

**Source:** [HBS Working Knowledge — 4 Steps to Psychological Safety](https://www.library.hbs.edu/working-knowledge/four-steps-to-build-the-psychological-safety-that-high-performing-teams-need-today) | [Amy Edmondson — Psychological Safety](https://amycedmondson.com/psychological-safety/) | Edmondson, A. *The Fearless Organization* (2018)

---

## 4.4 HBR Best Practices: Managing Remote Teams, Feedback, Delegation

### Managing Remote Teams

**Key HBR principles:**
- **Overcommunicate intentionally** — in remote work, information doesn't travel by osmosis. Default to written, async updates.
- **Create structure without surveillance** — daily standups (15 min), weekly team meetings, monthly retrospectives
- **Make work visible** — shared task boards, progress updates, "working out loud"
- **Invest in relationship-building** — 1-on-1s should include personal check-ins, not just task reviews
- **Results over hours** — measure output, not time-in-chair

### Giving Feedback (HBR Best Practices)
- **SBI Model:** Situation → Behavior → Impact
  - "During yesterday's client call (situation), you interrupted the client three times (behavior), which made them seem frustrated and less willing to share (impact)."
- Give feedback **within 48 hours** while it's still fresh
- **Ask permission:** "Can I share some feedback on the presentation?"
- **Be specific:** Observable behaviors, not character judgments
- **Future-focused:** "Next time, try X" rather than "You always do Y"

### Delegation (HBR Framework)
- **Delegate outcomes, not tasks** — "I need a competitive analysis that helps us decide our pricing" vs. "Make a spreadsheet of competitor prices"
- **Match to skill level** (see Situational Leadership above)
- **Define the guardrails:** Budget, timeline, quality standards, decision authority
- **Check in at milestones, not constantly** — agree on check-in points upfront
- **Accept 80% their way** — if they accomplish the outcome differently than you would, that's fine

### Leading When You're Not in the Room

This is the **ultimate test of leadership** — and directly relevant to managing AI agents:

1. **Clarity of purpose** — Does everyone know WHY they're doing what they're doing?
2. **Clear principles** — When facing an ambiguous decision, do people know what you'd do? (= documented values, decision frameworks)
3. **Permission to act** — Do people have authority to make decisions without you?
4. **Trust + verify** — Trust by default, but build in reporting/review mechanisms
5. **Hire/build for judgment** — The best people make good decisions when you're not watching

**Source:** [HBR.org — Remote Teams](https://hbr.org/topic/subject/remote-work) | [HBR — How to Give Feedback](https://hbr.org/2019/03/the-feedback-fallacy) | [HBR — Delegation](https://hbr.org/2017/07/how-to-decide-which-tasks-to-delegate)

---

# 5. Leading Through AI — Managing AI Agents as Employees

## 5.1 Management Frameworks That Apply to AI Agents

### Framework Mapping: Traditional Management → AI Management

| Traditional Framework | AI Agent Application |
|---|---|
| **Situational Leadership** | Adjust prompting style based on agent capability: new agent = detailed instructions; proven agent = high-level goals |
| **SMART Goals** | Every AI task should be: Specific, Measurable, Achievable, Relevant, Time-bound |
| **Management by Objectives (MBO)** | Define expected output clearly. Let the agent choose the method. Judge on results. |
| **RACI Matrix** | Define which agents are Responsible, Accountable, Consulted, Informed for each task |
| **Radical Candor** | Give AI agents clear, specific feedback. Vague prompts = vague output. Be kind to yourself (don't blame the AI for bad prompting). |
| **Toyota's Jidoka** | Build in quality gates: if AI output doesn't meet criteria at checkpoint, stop and fix before continuing |
| **DMAIC** | Define what you need → Measure AI performance → Analyze where it fails → Improve prompts/process → Control via templates |

---

## 5.2 Delegation to AI: Task Clarity, Expected Output, Feedback Loops

### The AI Delegation Checklist

Before delegating any task to an AI agent, provide:

1. **OBJECTIVE** — What are you trying to achieve? (Not "write a report" but "create a competitive analysis that helps us decide pricing for Product X")
2. **CONTEXT** — What does the agent need to know? Background, constraints, who the audience is
3. **EXPECTED OUTPUT** — Format, length, level of detail, deliverable type
4. **QUALITY CRITERIA** — What does "done" look like? What makes this excellent vs. mediocre?
5. **CONSTRAINTS** — What's out of scope? What should the agent NOT do?
6. **EXAMPLES** — Show, don't just tell. Provide a sample of good output if you have one.
7. **FEEDBACK LOOP** — How will you evaluate the output? When should the agent check in?

### The DECO Framework for AI Delegation

| Element | Description | Example |
|---|---|---|
| **D — Define** | What's the task and why does it matter? | "Research top 5 competitors and their pricing to inform our pricing strategy" |
| **E — Expected Output** | What should the deliverable look like? | "A comparison table with columns: Competitor, Product, Price, Key Differentiators, Source URL" |
| **C — Constraints** | Boundaries, limitations, what not to do | "Focus on US market only. Exclude enterprise plans. Max 2 pages." |
| **O — Output Review** | How you'll evaluate quality | "I'll check: Are all sources cited? Is the data current (within 6 months)? Does the analysis lead to a clear recommendation?" |

**Source:** [The Canton Group — Delegation Framework for AI Agents](https://cantongroup.com/insights/delegation-framework-how-delegate-ai-agents) | [Forbes — AI Agents Expose a Leadership Delegation Crisis](https://www.forbes.com/sites/cindyrodriguezconstable/2026/01/28/personal-ai-agents-expose-a-leadership-delegation-crisis/)

---

## 5.3 How to QA AI Output Without Micromanaging

### The Trust Curve for AI Agents

```
Trust Level:
HIGH  │                              ┌──── Spot-check only
      │                         ┌────┘
      │                    ┌────┘         Review exceptions
      │               ┌────┘
      │          ┌────┘                   Review key outputs
      │     ┌────┘
      │┌────┘                             Review all output
LOW   └────────────────────────────────────────────
      First use    10 tasks    50 tasks    100+ tasks
                   TIME / EXPERIENCE
```

### QA Tiers (Match to Trust Level)

| Tier | Review Level | When to Use | What It Looks Like |
|---|---|---|---|
| **Tier 1 — Full Review** | Review 100% of output | New agent, new task type, high-stakes output | Read every word. Check every fact. Verify every link. |
| **Tier 2 — Structured Sample** | Review key sections + spot-check details | Agent has 5-10 successful completions on this task type | Check the conclusion/recommendation + 2-3 random data points |
| **Tier 3 — Exception-Based** | Review only flagged items or anomalies | Agent has a strong track record (20+ tasks) | Look at the summary. Investigate only if something seems off. |
| **Tier 4 — Audit Cycle** | Periodic deep review (e.g., every 10th output) | Agent is fully trusted on this task type | Sample 10% of outputs monthly for quality. Trust the rest. |

### Quality Gates for AI Output

Build automated/semi-automated quality checks:

1. **Format check** — Does the output match the expected structure?
2. **Completeness check** — Are all required sections present?
3. **Source check** — Are claims backed by URLs/references? (Especially for research tasks)
4. **Freshness check** — Is the information current? (Important for market research)
5. **Sanity check** — Does the conclusion make sense? Does it pass the "smell test"?
6. **Contradiction check** — Does the output contradict known facts or previous work?

### The "Agent Performance Log"

Track AI agent performance over time to inform your QA level:

```markdown
| Date | Agent | Task Type | Quality (1-5) | Issues Found | Notes |
|------|-------|-----------|---------------|--------------|-------|
| 2026-02-01 | King | Research brief | 4 | One dead link | Moving to Tier 2 |
| 2026-02-03 | King | Client email draft | 5 | None | Tier 3 for emails |
```

### The Anti-Micromanagement Principle

> "Manage AI agents like you'd manage a competent junior hire: clear brief → let them work → review output → give specific feedback → adjust the brief for next time."

**What micromanaging AI looks like:**
- Rewriting every output from scratch (→ fix the prompt instead)
- Adding 17 constraints to prevent one edge case (→ use a quality gate)
- Not trusting AI for any tasks you could verify (→ start with low-stakes)

**What effective AI management looks like:**
- Clear brief upfront (DECO framework)
- Trust the process, review the output
- When output is bad: improve the *system* (prompt, context, template), not just correct the output
- Track performance over time to calibrate trust

**Source:** [Bain — Building the Foundation for Agentic AI](https://www.bain.com/insights/building-the-foundation-for-agentic-ai-technology-report-2025/) | [BCG — Making AI Agents Safe](https://www.bcg.com/publications/2025/making-ai-agents-safe-for-world)

---

# Quick Reference: Framework Cheat Sheet

| Framework | Creator | Core Idea | Action in 1 Sentence |
|---|---|---|---|
| **14 Points** | Deming | Quality is a system, not an inspection | Build quality into your process; fix the system, not the person |
| **DMAIC** | Six Sigma / Motorola | Define-Measure-Analyze-Improve-Control | For any recurring problem: define it, measure it, find root cause, fix it, prevent backsliding |
| **TPS / Lean** | Toyota / Ohno | Eliminate waste, continuous improvement | Stop the line when there's a problem; improve one small thing every week |
| **TQM** | Various | Everyone owns quality | Quality isn't a phase — it's a culture and a daily habit |
| **Checklist Manifesto** | Gawande | Simple checklists prevent complex failures | Create 5-9 item checklists for your critical recurring tasks |
| **MECE** | Minto / McKinsey | Structure problems into clean, complete buckets | When delegating: make sure workstreams don't overlap and together cover everything |
| **Build-Measure-Learn** | Eric Ries | Validate ideas through fast experiments | Start with what you need to learn, then build the minimum to learn it |
| **Customer Development** | Steve Blank | Search for business model before executing it | Talk to 10 customers before writing a line of code |
| **Situational Leadership** | Hersey & Blanchard | Adapt style to follower readiness | New person = more direction; experienced person = more delegation |
| **Radical Candor** | Kim Scott | Care personally + challenge directly | Honest feedback delivered with genuine care — specific, sincere, timely |
| **Psychological Safety** | Amy Edmondson | Safe teams take risks and learn faster | Model vulnerability; thank people for speaking up; normalize mistakes as learning |
| **DECO** | Synthesis | Define, Expected Output, Constraints, Output Review | Clear AI delegation: say what you want, show what good looks like, set boundaries |

---

# Recommended Reading

| Book | Author | Key Takeaway |
|---|---|---|
| *Out of the Crisis* | W. Edwards Deming | The system determines quality, not the workers |
| *The Lean Startup* | Eric Ries | Validate before you build |
| *The Four Steps to the Epiphany* | Steve Blank | Search for your business model systematically |
| *The Checklist Manifesto* | Atul Gawande | Simple checklists prevent complex failures |
| *Radical Candor* | Kim Scott | Caring + challenging = growth |
| *The Fearless Organization* | Amy Edmondson | Psychological safety enables learning & innovation |
| *The Toyota Way* | Jeffrey Liker | 14 principles for operational excellence |
| *The McKinsey Way* | Ethan Rasiel | Structured problem-solving for consultants |
| *The Pyramid Principle* | Barbara Minto | Communicate answer-first |

---

*Research compiled from primary sources, HBS Working Knowledge, ASQ, official framework documentation, and peer-reviewed publications. All URLs verified at time of compilation.*
