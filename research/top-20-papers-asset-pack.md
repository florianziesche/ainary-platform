# Top 20 AI Agent Papers â€” ASSET PACK

**Created:** 2026-02-11  
**For:** OpenClaw/Mia â€” Compound Machine  
**Purpose:** Reusable knowledge assets from research audit

---

## ğŸ“¦ CONTENTS

1. [Atomic Notes](#atomic-notes) â€” Top 10 Papers distilled
2. [Playbooks](#playbooks) â€” 3 reusable processes
3. [Templates](#templates) â€” 2 reusable structures

---

# ATOMIC NOTES

## Top 10 Papers â€” Kernkonzepte & Nutzung

---

### AB-papers-NOTE-0001: ReAct Pattern

**ID:** AB-papers-NOTE-0001  
**Created:** 2026-02-11  
**Classification:** Evidenced (Paper existiert, weit reproduziert)  
**Confidence:** 95%  
**Sources:** ArXiv 2210.03629, AutoGPT/LangChain Implementations

**This answers:** "Was ist ReAct und wie nutzen wir es in OpenClaw/Mia?"

**Core Concept:**
ReAct = **Reasoning + Acting** in interleaved Pattern:
1. **Thought:** Agent denkt laut ("I need to search for...")
2. **Action:** Agent fÃ¼hrt Tool-Call aus (z.B. `search[query]`)
3. **Observation:** Agent sieht Ergebnis
4. **Repeat:** Bis Task erledigt ist

**Why it matters:**
- Befreit Agents aus reinem Text-Modus â†’ kÃ¶nnen in Umgebungen handeln
- Fundament fÃ¼r moderne Agent-Frameworks (AutoGPT, LangChain, alle nutzen ReAct)
- Einfach zu implementieren (nur Prompt-Engineering)

**How we use it:**
```python
# Pseudo-Code fÃ¼r ReAct-Loop in Mia
while not task_complete:
    thought = llm.generate_thought(context)  # "I should check the calendar"
    action = llm.select_action(thought)      # "calendar.get_events(today)"
    observation = execute_action(action)     # [Meeting at 3pm...]
    context.add(thought, action, observation)
    if is_goal_reached(context):
        break
```

**Known Limitations:**
- Kann in Loops stecken bleiben â†’ Max-Iteration-Limit setzen (z.B. 10)
- Bei schlechten Tools: Garbage-In/Garbage-Out
- Keine Memory-Persistenz â†’ kombinieren mit MemGPT

**Related Notes:**
- AB-papers-NOTE-0002 (MemGPT â€” fÃ¼r Memory)
- AB-papers-NOTE-0003 (Reflexion â€” fÃ¼r Self-Improvement)

**Tags:** #architecture #core #foundational #mvp

---

### AB-papers-NOTE-0002: MemGPT Memory System

**ID:** AB-papers-NOTE-0002  
**Created:** 2026-02-11  
**Classification:** Evidenced (Paper + Open-Source Implementation)  
**Confidence:** 90%  
**Sources:** ArXiv 2310.08560, MemGPT GitHub

**This answers:** "Wie managed Mia Memory Ã¼ber Context-Window hinaus?"

**Core Concept:**
MemGPT behandelt Context-Window wie **Virtual Memory in einem OS**:
- **Main Context** = RAM (aktiver Kontext, z.B. 8k tokens)
- **External Storage** = Disk (persistentes Memory, unbegrenzt)
- **Paging** = Agent entscheidet, wann Daten rein/raus

**Memory Hierarchy:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Context (RAM) â”‚  â† Aktuelle Konversation
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Short-Term Storage  â”‚  â† Recent Sessions
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Long-Term Storage   â”‚  â† Fakten, PrÃ¤ferenzen, Lessons
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it matters:**
- LÃ¶st Context-Window-Problem â†’ unbegrenzte Konversationen
- Agent kann sich an alte Projekte/PrÃ¤ferenzen erinnern
- Selbst-managed (Agent entscheidet, was wichtig ist)

**How we use it:**
```python
# FÃ¼r Mia: Memory-Schichten
class MiaMemory:
    main_context: list[Message]      # Letzte N Messages (im Context)
    short_term: VectorDB              # Embeddings der letzten Sessions
    long_term: KnowledgeGraph         # Fakten, PrÃ¤ferenzen, Lessons
    
    def page_in(self, query):
        # Wenn Main Context voll â†’ relevante Memories aus Storage holen
        relevant = self.short_term.search(query)
        self.main_context.add(relevant)
    
    def page_out(self, threshold=0.5):
        # Wenn Main Context voll â†’ unwichtige Memories raus
        for msg in self.main_context:
            if importance(msg) < threshold:
                self.short_term.store(msg)
                self.main_context.remove(msg)
```

**Known Limitations:**
- Paging-Decisions kÃ¶nnen suboptimal sein (Agent schÃ¤tzt Wichtigkeit falsch)
- Braucht gute Prompts fÃ¼r Memory-Management
- Storage-Retrieval muss schnell sein (sonst Latenz)

**Related Notes:**
- AB-papers-NOTE-0004 (RAG â€” fÃ¼r Dokument-Retrieval)
- AB-papers-NOTE-0001 (ReAct â€” fÃ¼r Action-Loop)

**Tags:** #memory #architecture #mvp #critical

---

### AB-papers-NOTE-0003: Reflexion Self-Improvement

**ID:** AB-papers-NOTE-0003  
**Created:** 2026-02-11  
**Classification:** Evidenced (Paper + Reproduced Results)  
**Confidence:** 90%  
**Sources:** ArXiv 2303.11366

**This answers:** "Wie lernt Mia aus Fehlern?"

**Core Concept:**
Reflexion = **Verbales Reinforcement Learning**:
1. **Execute:** Agent fÃ¼hrt Task aus
2. **Get Feedback:** Test failed / Error message / User feedback
3. **Reflect:** Agent analysiert, was schief ging ("I should try X instead of Y")
4. **Retry:** Agent versucht es erneut mit neuem Ansatz
5. **Store Reflection:** Episodisches Memory speichert Learnings

**Why it matters:**
- Agents lernen aus Fehlern **ohne menschliches Feedback**
- Self-Critique â†’ kontinuierliche Verbesserung
- Funktioniert bei Code, Writing, Reasoning

**How we use it:**
```python
# Reflexion-Loop fÃ¼r Mia
def solve_task_with_reflexion(task, max_attempts=3):
    reflections = []
    for attempt in range(max_attempts):
        solution = agent.generate_solution(task, reflections)
        feedback = evaluate(solution, task)
        
        if feedback.success:
            return solution
        
        # Reflect on failure
        reflection = agent.reflect(
            task=task,
            attempt=solution,
            feedback=feedback,
            previous_reflections=reflections
        )
        reflections.append(reflection)
    
    return None  # Failed after max attempts
```

**Example:**
```
Task: Fix bug in Python function
Attempt 1: Changed variable name â†’ Test failed
Reflection: "Changing the variable name doesn't address the logic error. 
             I should look at the conditional statement instead."
Attempt 2: Fixed conditional â†’ Test passed âœ“
```

**Known Limitations:**
- Kann in Reflection-Loops geraten (endloses "Ich sollte..." ohne Fortschritt)
- Braucht Max-Iteration-Limit (z.B. 3 Attempts)
- Nur so gut wie Feedback (schlechtes Feedback = schlechte Reflections)

**Related Notes:**
- AB-papers-NOTE-0005 (Self-Refine â€” fÃ¼r Output-QualitÃ¤t)
- AB-papers-NOTE-0001 (ReAct â€” fÃ¼r Action-Loop)

**Tags:** #self-improvement #learning #mvp #iteration

---

### AB-papers-NOTE-0004: RAG fÃ¼r Private Knowledge

**ID:** AB-papers-NOTE-0004  
**Created:** 2026-02-11  
**Classification:** Evidenced (Industry Standard)  
**Confidence:** 95%  
**Sources:** ArXiv 2005.11401, Production Deployments

**This answers:** "Wie greift Mia auf Florian's private Dokumente zu?"

**Core Concept:**
RAG = **Retrieval-Augmented Generation**:
1. **Index:** Dokumente â†’ Embeddings â†’ Vector DB
2. **Retrieve:** User Query â†’ Search Vector DB â†’ Top-K relevante Chunks
3. **Augment:** Top-K Chunks + Query â†’ LLM Context
4. **Generate:** LLM generiert Antwort basierend auf echten Dokumenten

**Why it matters:**
- LLM kann auf **aktuelle, private Daten** zugreifen (nicht nur Training-Data)
- Reduziert Halluzination (LLM zitiert echte Dokumente)
- Skaliert besser als "alles in Context packen"

**How we use it:**
```python
# RAG fÃ¼r Mia auf Obsidian/Notion
class MiaRAG:
    def __init__(self):
        self.vector_db = Chroma()  # oder Pinecone, Weaviate, etc.
        self.embed_model = OpenAIEmbeddings()
    
    def index_documents(self, docs):
        # Obsidian Notes, Notion Pages, Code-Repos
        chunks = self.chunk_documents(docs)  # Split into ~500 token chunks
        embeddings = self.embed_model.embed(chunks)
        self.vector_db.add(chunks, embeddings)
    
    def retrieve(self, query, top_k=5):
        query_embedding = self.embed_model.embed(query)
        relevant_chunks = self.vector_db.search(query_embedding, k=top_k)
        return relevant_chunks
    
    def generate(self, query):
        context = self.retrieve(query)
        prompt = f"Based on these documents:\n{context}\n\nAnswer: {query}"
        answer = llm.generate(prompt)
        return answer
```

**Chunking Best Practices:**
- **Size:** 300-500 tokens per chunk (zu klein = kein Kontext, zu groÃŸ = irrelevant)
- **Overlap:** 50-100 tokens (fÃ¼r Kontext-KontinuitÃ¤t)
- **Metadata:** Speichere Source, Date, Tags fÃ¼r Filtering

**Known Limitations:**
- **Retrieval-QualitÃ¤t ist KRITISCH:** Schlechte Embeddings â†’ schlechte Chunks â†’ schlechte Antworten
- **Chunking ist eine Kunst:** Dokumente sinnvoll splitten ist schwierig
- **Cost:** Embedding-DB + Retrieval + LLM ist teuer (aber skaliert besser als Finetuning)

**Related Notes:**
- AB-papers-NOTE-0002 (MemGPT â€” fÃ¼r Session-Memory)
- AB-papers-NOTE-0006 (Chain-of-Thought â€” fÃ¼r Reasoning)

**Tags:** #memory #knowledge #mvp #critical

---

### AB-papers-NOTE-0005: Self-Refine Output Quality

**ID:** AB-papers-NOTE-0005  
**Created:** 2026-02-11  
**Classification:** Evidenced (Paper + Wide Reproduction)  
**Confidence:** 90%  
**Sources:** ArXiv 2303.17651

**This answers:** "Wie verbessert Mia Output-QualitÃ¤t iterativ?"

**Core Concept:**
Self-Refine = **Generate â†’ Feedback â†’ Refine â†’ Repeat**:
1. **Generate:** LLM produziert initialen Output (Code, Email, Text)
2. **Self-Feedback:** LLM gibt sich selbst Feedback ("This could be more concise")
3. **Refine:** LLM verbessert Output basierend auf Feedback
4. **Repeat:** Bis "good enough" (z.B. 2-3 Iterationen)

**Why it matters:**
- **Kein externes Feedback nÃ¶tig** â†’ LLM ist sein eigener Critic
- Funktioniert sofort (keine Finetuning)
- Verbessert QualitÃ¤t signifikant (z.B. Code-Readability +30%)

**How we use it:**
```python
# Self-Refine fÃ¼r Mia's Outputs
def self_refine(initial_output, task, iterations=2):
    current = initial_output
    
    for i in range(iterations):
        # LLM gibt sich selbst Feedback
        feedback = llm.critique(
            output=current,
            task=task,
            criteria=["clarity", "conciseness", "correctness"]
        )
        
        if feedback.is_good_enough:
            break
        
        # LLM verbessert basierend auf Feedback
        current = llm.refine(
            output=current,
            feedback=feedback
        )
    
    return current
```

**Example:**
```
Task: Write professional email
Initial: "Hey, I was wondering if you could maybe help me with..."
Feedback: "Too casual. Use professional tone. Be more direct."
Refined: "Dear [Name], I would appreciate your assistance with..."
```

**Known Limitations:**
- **Overfitting-Risk:** Zu viele Iterationen kÃ¶nnen Output verschlechtern (z.B. zu formal)
- **Stop-Kriterium nÃ¶tig:** "Good enough" definieren (sonst endlose Iterations)
- **Cost:** Jede Iteration = 2 LLM-Calls (Critique + Refine)

**Related Notes:**
- AB-papers-NOTE-0003 (Reflexion â€” fÃ¼r Task-Level Learning)
- AB-papers-NOTE-0006 (CoT â€” fÃ¼r Reasoning)

**Tags:** #quality #iteration #mvp #simple

---

### AB-papers-NOTE-0006: Chain-of-Thought Reasoning

**ID:** AB-papers-NOTE-0006  
**Created:** 2026-02-11  
**Classification:** Evidenced (10,000+ Citations)  
**Confidence:** 95%  
**Sources:** ArXiv 2201.11903, Industry Standard

**This answers:** "Wie denkt Mia Schritt-fÃ¼r-Schritt?"

**Core Concept:**
CoT = **"Let's think step by step"**:
- Statt direkter Antwort â†’ LLM zeigt **intermediate reasoning steps**
- Durch Beispiele (few-shot) lernt LLM, wie man "laut denkt"
- Drastisch bessere Performance bei komplexen Tasks (z.B. Math +50%)

**Why it matters:**
- **Trivial zu implementieren** (nur Prompt-Ã„nderung)
- **Enormer Nutzen** (funktioniert bei fast allen komplexen Tasks)
- **Debuggable** (man sieht, WIE der Agent denkt)

**How we use it:**
```python
# CoT fÃ¼r Mia
def solve_with_cot(task):
    prompt = f"""
Task: {task}

Let's solve this step by step:
1. First, I need to understand what is being asked...
2. Then, I should identify the key information...
3. Next, I can...
4. Finally, I conclude that...

Answer: [final answer]
"""
    return llm.generate(prompt)
```

**Example:**
```
Task: Calculate (23 Ã— 4) + (15 Ã· 3)

CoT Reasoning:
1. First, I'll calculate 23 Ã— 4 = 92
2. Then, I'll calculate 15 Ã· 3 = 5
3. Finally, I'll add 92 + 5 = 97

Answer: 97 âœ“
```

**Few-Shot vs. Zero-Shot:**
- **Few-Shot CoT:** Gib Beispiele von CoT-Reasoning (besser fÃ¼r schwierige Tasks)
- **Zero-Shot CoT:** Einfach "Let's think step by step" anhÃ¤ngen (funktioniert Ã¼berraschend gut!)

**Known Limitations:**
- **Verbose:** CoT produziert viel Text (hÃ¶here Latenz/Cost)
- **Nicht immer nÃ¶tig:** FÃ¼r triviale Tasks ist CoT Overkill
- **Kann halluzinieren:** LLM kann falsche Steps zeigen (aber insgesamt besser als ohne CoT)

**Related Notes:**
- AB-papers-NOTE-0007 (Tree of Thoughts â€” fÃ¼r komplexere Reasoning)
- AB-papers-NOTE-0005 (Self-Refine â€” fÃ¼r Output-QualitÃ¤t)

**Tags:** #reasoning #core #mvp #simple

---

### AB-papers-NOTE-0007: AutoGen Multi-Agent

**ID:** AB-papers-NOTE-0007  
**Created:** 2026-02-11  
**Classification:** Evidenced (Open-Source Framework, Active Use)  
**Confidence:** 85%  
**Sources:** ArXiv 2308.08155, AutoGen GitHub

**This answers:** "Wie bauen wir spezialisierte Sub-Agents fÃ¼r Mia?"

**Core Concept:**
AutoGen = **Multi-Agent-Konversationen**:
- Mehrere Agents mit **unterschiedlichen Rollen** (z.B. Coder, Reviewer, Tester)
- Agents **chatten miteinander** um komplexe Tasks zu lÃ¶sen
- **Orchestrator** koordiniert Conversation-Flow

**Why it matters:**
- **Multi-Agent > Monolith** fÃ¼r komplexe Tasks
- **Spezialisierung:** Jeder Agent ist Experte in seinem Bereich
- **Skalierbarkeit:** Neue Agents einfach hinzufÃ¼gen

**How we use it:**
```python
# AutoGen-Style Multi-Agent fÃ¼r Mia
class MiaMultiAgent:
    def __init__(self):
        self.agents = {
            "researcher": ResearchAgent(),
            "writer": WriterAgent(),
            "coder": CoderAgent(),
            "reviewer": ReviewerAgent()
        }
        self.orchestrator = Orchestrator()
    
    def solve_complex_task(self, task):
        # Orchestrator plant Workflow
        plan = self.orchestrator.plan(task)  
        # â†’ [researcher, writer, reviewer]
        
        context = {}
        for agent_name in plan:
            agent = self.agents[agent_name]
            result = agent.execute(task, context)
            context[agent_name] = result
        
        return context["reviewer"].final_output
```

**Example Workflow:**
```
Task: "Write a blog post about AI Agents"

Orchestrator Plan:
1. Researcher â†’ Finds relevant papers and facts
2. Writer â†’ Drafts blog post based on research
3. Reviewer â†’ Checks for clarity, accuracy, flow
4. Writer â†’ Refines based on feedback

Output: Polished blog post âœ“
```

**Role Design Best Practices:**
- **Clear Responsibilities:** Jeder Agent hat ONE job
- **Well-Defined Inputs/Outputs:** Agent weiÃŸ, was er bekommt und produziert
- **Communication Protocol:** Agents nutzen strukturierte Messages (JSON)

**Known Limitations:**
- **Overhead:** Mehr Agents = mehr API-Calls = hÃ¶here Cost
- **Debugging schwierig:** Wer hat den Fehler gemacht?
- **Design ist KRITISCH:** Schlechte Rollen-Definition = Chaos

**Related Notes:**
- AB-papers-NOTE-0001 (ReAct â€” fÃ¼r Single-Agent Loop)
- AB-papers-NOTE-0008 (MCP â€” fÃ¼r Tool-Integration)

**Tags:** #multi-agent #architecture #v2 #scaling

---

### AB-papers-NOTE-0008: MCP Standard fÃ¼r Tools

**ID:** AB-papers-NOTE-0008  
**Created:** 2026-02-11  
**Classification:** Evidenced (Linux Foundation, Industry Adoption)  
**Confidence:** 90%  
**Sources:** Anthropic MCP Announcement, modelcontextprotocol.io

**This answers:** "Wie integriert Mia Tools standardisiert?"

**Core Concept:**
MCP = **Model Context Protocol** (wie USB fÃ¼r AI Agents):
- **Ein Standard** fÃ¼r alle Tool-Integrations (statt custom Connectors)
- **MCP Server:** Tool-Provider bietet standardisierte API
- **MCP Client:** Agent nutzt standardisiertes Protocol
- **InteroperabilitÃ¤t:** Jeder Agent kann jeden MCP-Server nutzen

**Why it matters:**
- **LÃ¶st Fragmentierung:** Statt N Ã— M Integrations â†’ N Server + M Clients
- **Industry-backed:** Anthropic, OpenAI, Google, Microsoft, AWS alle dabei
- **Future-proof:** Wird DER Standard fÃ¼r Agent-Tool-Integration

**How we use it:**
```python
# Mia nutzt MCP fÃ¼r Tool-Integration
class MiaMCPClient:
    def __init__(self):
        self.servers = {
            "calendar": MCPServer("mcp://calendar.local"),
            "email": MCPServer("mcp://gmail.com/api"),
            "notion": MCPServer("mcp://notion.so/api"),
            "obsidian": MCPServer("mcp://obsidian.local")
        }
    
    def use_tool(self, tool_name, params):
        server = self.servers[tool_name]
        result = server.call(params)
        return result
```

**MCP vs. Custom Integration:**

| Approach | MCP | Custom |
|----------|-----|--------|
| **Development:** | Nutze existierende MCP-Server | Baue jeden Connector selbst |
| **Maintenance:** | Server-Updates automatisch | Jedes API-Change = eigener Fix |
| **Interoperability:** | Andere Agents kÃ¶nnen nutzen | Nur fÃ¼r dich |
| **Future-proof:** | Industry-Standard | Kann obsolet werden |

**Known Limitations:**
- **Noch frÃ¼h:** MCP ist neu (2025/2026), Ecosystem wÃ¤chst noch
- **Nicht alles hat MCP-Server:** FÃ¼r Nischen-Tools musst du selbst bauen
- **Performance:** Extra Layer kann Latenz hinzufÃ¼gen (aber minimal)

**Related Notes:**
- AB-papers-NOTE-0001 (ReAct â€” fÃ¼r Tool-Use Pattern)
- AB-papers-NOTE-0007 (AutoGen â€” fÃ¼r Multi-Agent)

**Tags:** #infrastructure #tools #mvp #standard

---

### AB-papers-NOTE-0009: LLM Agent Survey (Reference)

**ID:** AB-papers-NOTE-0009  
**Created:** 2026-02-11  
**Classification:** Evidenced (1000+ Citations, Continuously Updated)  
**Confidence:** 95%  
**Sources:** ArXiv 2308.11432, GitHub Repo

**This answers:** "Was ist der State-of-the-Art fÃ¼r LLM Agents?"

**Core Concept:**
Comprehensive Survey Ã¼ber **LLM-based Autonomous Agents**:
- **Architecture:** Perception, Planning, Action Modules
- **Applications:** Code, Robotics, Web Navigation, Games
- **Evaluation:** Benchmarks, Metrics, Challenges
- **Future Directions:** Open Problems

**Why it matters:**
- **Beste Ãœbersicht** Ã¼ber das gesamte Feld (300+ Papers referenziert)
- **Kontinuierlich updated** (GitHub Repo mit neuesten Papers)
- **Praktische Taxonomie** (hilft bei Design-Entscheidungen)

**Key Takeaways fÃ¼r OpenClaw/Mia:**

**Agent Architecture (3 Modules):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PERCEPTION                  â”‚
â”‚  (Multimodal Input, Memory, RAG)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PLANNING                     â”‚
â”‚  (CoT, ToT, ReAct, Reflexion)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ACTION                       â”‚
â”‚  (Tool Use, Code Exec, API Calls)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Best Practices (from Survey):**
1. **Modular Design:** Perception/Planning/Action getrennt (einfacher zu debuggen)
2. **Memory Hierarchy:** Short-term + Long-term (wie MemGPT)
3. **Human-in-the-Loop:** FÃ¼r kritische Entscheidungen
4. **Evaluation:** Track Success Rate, Cost, Latency

**Related Notes:**
- AB-papers-NOTE-0001 (ReAct)
- AB-papers-NOTE-0002 (MemGPT)
- AB-papers-NOTE-0006 (CoT)

**Tags:** #reference #survey #architecture #comprehensive

---

### AB-papers-NOTE-0010: Self-Consistency for Robustness

**ID:** AB-papers-NOTE-0010  
**Created:** 2026-02-11  
**Classification:** Evidenced (2000+ Citations, Widely Used)  
**Confidence:** 90%  
**Sources:** ArXiv 2203.11171

**This answers:** "Wie macht Mia kritische Entscheidungen robuster?"

**Core Concept:**
Self-Consistency = **Sample Multiple + Majority Vote**:
1. **Sample N Reasoning Paths** (mit Temperature > 0, z.B. N=5)
2. **Extract Answers** aus jedem Path
3. **Majority Vote** â†’ finale Antwort

**Why it matters:**
- **Drastisch hÃ¶here Accuracy** (z.B. von 60% â†’ 80%)
- **Robust gegen einzelne Fehler** (ein falscher Path wird Ã¼berstimmt)
- **Trivial zu implementieren** (nur N LLM-Calls + Voting)

**How we use it:**
```python
# Self-Consistency fÃ¼r Mia's kritische Entscheidungen
def decide_with_self_consistency(task, n_samples=5):
    answers = []
    
    for i in range(n_samples):
        # Sample mit Temperature > 0 (fÃ¼r Diversity)
        reasoning = llm.generate(task, temperature=0.7)
        answer = extract_answer(reasoning)
        answers.append(answer)
    
    # Majority Vote
    final_answer = most_common(answers)
    confidence = count(final_answer) / n_samples
    
    return final_answer, confidence
```

**Example:**
```
Task: "Should I send this email now or wait?"

Sample 1: "Wait (recipient is in different timezone)"
Sample 2: "Send now (urgent matter)"
Sample 3: "Wait (better to send during business hours)"
Sample 4: "Wait (recipient might be offline)"
Sample 5: "Wait (not time-sensitive)"

Majority Vote: WAIT (4/5) â†’ Confidence 80%
```

**When to use:**
- **Kritische Entscheidungen:** Email senden, Code deployen, Datei lÃ¶schen
- **Unsichere Reasoning:** Wenn Task mehrdeutig ist
- **High-Stakes Actions:** Wenn Fehler teuer sind

**Known Limitations:**
- **Cost:** N Ã— LLM-Calls (z.B. 5Ã— teurer als Single-Call)
- **Latenz:** N Ã— Zeit (kÃ¶nnen parallel machen, aber trotzdem langsamer)
- **Nicht immer nÃ¶tig:** FÃ¼r triviale Tasks ist Overkill

**Related Notes:**
- AB-papers-NOTE-0006 (CoT â€” fÃ¼r Reasoning)
- AB-papers-NOTE-0005 (Self-Refine â€” fÃ¼r QualitÃ¤t)

**Tags:** #robustness #critical #decision-making #quality

---

# PLAYBOOKS

## 3 Reusable Processes

---

## PLAYBOOK 1: Paper Evaluation in 15 Min

**Purpose:** Schnell neue Papers bewerten ohne vollstÃ¤ndiges Lesen  
**When to use:** Wenn neues Paper erscheint und du entscheiden musst ob relevant  
**Time:** 15 Minuten

### Schritte:

**1. META-CHECK (2 Min)**
- [ ] ArXiv-Link Ã¶ffnen
- [ ] Autoren checken (bekannte Namen? Affiliation?)
- [ ] Zitationen checken (Google Scholar, Semantic Scholar)
- [ ] Datum checken (wie aktuell?)
- [ ] Code verfÃ¼gbar? (GitHub Link?)

**Red Flags:**
- âŒ Keine bekannten Autoren + keine Zitationen + >6 Monate alt â†’ wahrscheinlich nicht wichtig
- âŒ Keine Code-VerfÃ¼gbarkeit + "State-of-the-art claims" â†’ skeptisch sein

**2. ABSTRACT + INTRO (5 Min)**
- [ ] Lies Abstract: Was ist die **Kernidee**?
- [ ] Lies Intro: Was ist das **Problem** und warum ist es wichtig?
- [ ] Ãœberfliege Related Work: Wie ordnet sich das Paper ein?

**Ask yourself:**
- Was ist NEU an diesem Paper? (vs. existierende LÃ¶sungen)
- Ist das Problem RELEVANT fÃ¼r uns?

**3. RESULTS + FIGURES (5 Min)**
- [ ] Ãœberspringe Methodology (fÃ¼r jetzt)
- [ ] Gehe direkt zu Results Section
- [ ] Schaue Tabellen/Graphs an: Was sind die **Kernresultate**?
- [ ] Lies Discussion/Conclusion: Was sind **Limitations**?

**Ask yourself:**
- Sind die Ergebnisse beeindruckend? (vs. Baselines)
- Sind die Experimente realistisch? (oder nur Toy-Examples)
- Was sind bekannte Failures/Limitations?

**4. PRACTICAL BUILDABILITY (3 Min)**
Score auf 3 Dimensionen (1-10):

**Theoretical Impact:**
- 1-3: Incremental improvement
- 4-7: Solid contribution
- 8-10: Paradigm shift

**Practical Buildability:**
- 1-3: Braucht Wochen + Expertise
- 4-7: Code existiert, moderat komplex
- 8-10: Trivial zu implementieren (z.B. Prompt-Change)

**Relevance for Us:**
- 1-3: Interessant, aber nicht direkt relevant
- 4-7: KÃ¶nnte nÃ¼tzlich sein fÃ¼r spezifische Tasks
- 8-10: MÃœSSEN wir nutzen

**Final Score = Buildability Ã— Relevance**

**Decision:**
- Score >50 â†’ **Deep-Dive** (lies Methodology, implementiere)
- Score 30-50 â†’ **Monitor** (merke dir, revisit spÃ¤ter)
- Score <30 â†’ **Skip** (nicht relevant fÃ¼r uns)

---

## PLAYBOOK 2: Paper-Konzept in OpenClaw/Mia implementieren

**Purpose:** Systematisch Paper-Ideen in Code Ã¼bersetzen  
**When to use:** Wenn du entschieden hast, ein Paper zu nutzen  
**Time:** Variable (Stunden bis Tage)

### Phase 1: VERSTEHEN (30-60 Min)

**1. Kernkonzept extrahieren:**
- [ ] Lies Methodology Section vollstÃ¤ndig
- [ ] Erstelle Flussdiagramm des Algorithmus
- [ ] Identifiziere **kritische Komponenten** (was MUSS funktionieren?)

**2. Code-Review (wenn verfÃ¼gbar):**
- [ ] Clone GitHub Repo
- [ ] Lies README + Docs
- [ ] Finde **Haupt-Entry-Point** (meist `main.py` oder `run.py`)
- [ ] Identifiziere **Dependencies** (welche Libraries?)

**3. Atomic Note erstellen:**
- [ ] Nutze Template AB-papers-NOTE-XXXX
- [ ] Dokumentiere Kernkonzept, Use-Case, Limitations

### Phase 2: PROTOTYPE (1-3 Stunden)

**1. Minimal Viable Implementation:**
- [ ] Erstelle EINFACHSTE Version (ignore edge cases)
- [ ] Nutze existierenden Code wenn mÃ¶glich (don't reinvent)
- [ ] Teste mit EINEM Beispiel (funktioniert Kernidee?)

**2. Integration-Check:**
- [ ] Wo gehÃ¶rt das hin in Mia's Architektur?
  - Core Loop (ReAct)? â†’ `agent/core.py`
  - Memory System? â†’ `agent/memory.py`
  - Tool? â†’ `agent/tools/`
  - Utility? â†’ `agent/utils.py`

**3. Quicktest:**
```python
# Standalone Test (bevor Integration)
def test_new_feature():
    input = "test case"
    output = new_feature(input)
    assert output == expected
    print("âœ“ Feature works!")
```

### Phase 3: REFINE (2-5 Stunden)

**1. Robustheit:**
- [ ] Error-Handling hinzufÃ¼gen
- [ ] Edge-Cases behandeln
- [ ] Logging hinzufÃ¼gen (fÃ¼r Debugging)

**2. Integration:**
- [ ] Feature in Mia's Main-Loop integrieren
- [ ] Config-Options hinzufÃ¼gen (feature toggle)
- [ ] Tests schreiben (Unit + Integration)

**3. Dokumentation:**
- [ ] Docstrings fÃ¼r Functions
- [ ] Update README (new feature)
- [ ] Example-Usage dokumentieren

### Phase 4: EVALUATE (1 Stunde)

**1. Performance-Check:**
- [ ] Latenz messen (wie viel langsamer?)
- [ ] Cost messen (wie viel teurer?)
- [ ] Quality messen (wie viel besser?)

**2. Decision:**
- Wenn Performance/Quality-Gain > Cost/Latency â†’ **KEEP**
- Wenn nicht â†’ **REMOVE** oder **MAKE OPTIONAL**

**3. Lessons Learned:**
- [ ] Update `failures/output-tracker.md`
- [ ] Was hat funktioniert? Was nicht?
- [ ] Update FLORIAN.md (falls relevant fÃ¼r PrÃ¤ferenzen)

---

## PLAYBOOK 3: Paper-Insights in Content verwandeln

**Purpose:** Von Paper-Reading zu Blog/Tweet/Artikel  
**When to use:** Wenn du Paper-Insights teilen willst  
**Time:** 1-2 Stunden (Draft), 30 Min (Distribution)

### Phase 1: SYNTHESIS (30 Min)

**1. Key Insights extrahieren:**
- [ ] Was ist die **eine groÃŸe Idee** aus dem Paper?
- [ ] Warum sollte **jemand anders** das interessant finden?
- [ ] Was kann man **sofort nutzen**?

**2. Angle finden:**
Choose ONE:
- **Practical:** "How to implement X in 10 lines of code"
- **Critical:** "Why Paper X is overhyped (and what actually works)"
- **Explanatory:** "Understanding X: The concept that changed Y"
- **Comparative:** "ReAct vs. Reflexion: When to use which?"

**3. Outline erstellen:**
```markdown
# Title: [Catchy + Clear]

## Hook (1 paragraph)
- Problem statement
- Why now?

## Core Concept (2-3 paragraphs)
- Explain the idea (ELI5)
- Why it matters
- How it works (simple example)

## Practical Use (2-3 paragraphs)
- How can YOU use this?
- Code example / Implementation
- Known pitfalls

## Conclusion (1 paragraph)
- Key Takeaway
- Call-to-Action (try it / read more / discuss)
```

### Phase 2: WRITE (30-60 Min)

**1. Draft (No Editing!):**
- [ ] Write FAST (get ideas out)
- [ ] Use simple language (avoid jargon unless necessary)
- [ ] Add code examples (practical > theoretical)
- [ ] Include visuals if possible (diagrams, tables)

**2. Self-Refine (15 Min):**
- [ ] Run through Self-Refine loop (AB-papers-NOTE-0005)
- [ ] Check: Clarity, Conciseness, Correctness
- [ ] Remove fluff (every sentence should add value)

**3. Florian-Check:**
- [ ] WÃ¼rde Florian das lesen? (his test: "Do I learn something NEW in 2 minutes?")
- [ ] Tone: Direct, insightful, founder-operator perspective
- [ ] No bullshit: If you're not sure, say so

### Phase 3: DISTRIBUTE (30 Min)

**Multi-Channel Repurposing:**

**Blog Post (Substack/Medium):**
- [ ] Full article (800-1500 words)
- [ ] Add SEO-friendly title
- [ ] Include links to Paper, Code, Related Posts

**LinkedIn Post:**
- [ ] Summary (300-500 words)
- [ ] Emphasize **practical value**
- [ ] Add "Read full article" CTA

**Twitter Thread:**
- [ ] 5-7 tweets
- [ ] Tweet 1: Hook (problem + solution teaser)
- [ ] Tweet 2-5: Core concept (bite-sized)
- [ ] Tweet 6: Code example or visual
- [ ] Tweet 7: Conclusion + link to full article

**Email Newsletter (if applicable):**
- [ ] Personal intro ("Why I read this paper...")
- [ ] Summary + Insights
- [ ] Link to full article

**Template Beispiel (Twitter Thread):**
```
ğŸ§µ I just read [Paper Name] and it's a game-changer for [use-case].

Here's what you need to know (and how to use it):

1/7

---

The problem: [Current state is X, but we want Y]

Most people try [common approach], but it fails because [limitation].

2/7

---

[Paper Name] solves this with [core concept]:

Instead of X, they do Y.

The key insight: [one sentence explanation]

3/7

---

How it works (simplified):

1. [Step 1]
2. [Step 2]
3. [Step 3]

Example: [concrete example]

4/7

---

You can implement this in ~10 lines of code:

[code snippet or pseudocode]

5/7

---

Known limitations:
- [Limitation 1]
- [Limitation 2]

When to use: [use-case]
When to skip: [anti-use-case]

6/7

---

Key Takeaway: [one sentence]

If you're building [X], you should try [Y].

Full breakdown + code: [link to blog]

7/7
```

---

# TEMPLATES

## 2 Reusable Structures

---

## TEMPLATE 1: Paper Evaluation Template

**Purpose:** Wiederverwendbar fÃ¼r jedes neue Paper  
**Format:** Markdown (fÃ¼r Obsidian/Notion)

```markdown
# Paper Evaluation: [Paper Title]

**Evaluated:** [YYYY-MM-DD]  
**Evaluator:** [Your Name / Mia]  
**Time Spent:** [X minutes]

---

## ğŸ“‹ META

- **Title:** [Full Title]
- **Authors:** [Names + Affiliations]
- **Published:** [Date + Venue (e.g., "October 2022, ICLR 2023")]
- **ArXiv:** [Link]
- **Code:** [GitHub Link or "N/A"]
- **Citations:** [Number + Source (e.g., "5000+ (Semantic Scholar)")]

---

## ğŸ¯ CORE CONCEPT

**In one sentence:**
[What is the key idea?]

**Problem it solves:**
[What existing problem does this address?]

**How it works (simplified):**
1. [Step 1]
2. [Step 2]
3. [Step 3]

**Key Innovation:**
[What is NEW compared to prior work?]

---

## ğŸ“Š RESULTS

**Key Findings:**
- [Finding 1 + metric]
- [Finding 2 + metric]
- [Finding 3 + metric]

**Baselines Compared:**
- [Baseline 1]
- [Baseline 2]

**Performance Gains:**
- [Metric]: Baseline X â†’ This Paper Y (+Z%)

---

## âš ï¸ LIMITATIONS

**Known Issues (from paper):**
- [Limitation 1]
- [Limitation 2]

**Community Criticism (if any):**
- [Critique 1]
- [Critique 2]

**What doesn't work in practice:**
- [Failure mode 1]
- [Failure mode 2]

---

## ğŸ“ˆ SCORES

**Theoretical Impact:** [1-10] / 10  
**Justification:** [Why this score?]

**Practical Buildability:** [1-10] / 10  
**Justification:** [Why this score?]

**Relevance for Us:** [1-10] / 10  
**Justification:** [Why this score?]

**FINAL SCORE:** [Buildability Ã— Relevance] = [Score]

---

## ğŸ¯ DECISION

**Action:** [SOFORT NUTZEN / MITTELFRISTIG / SPÃ„TER / SKIP]

**Reasoning:**
[Why this decision?]

**Next Steps (if relevant):**
- [ ] Read Methodology Section
- [ ] Clone GitHub Repo
- [ ] Prototype Feature X
- [ ] Integrate into [System]

---

## ğŸ”— RELATED

**Related Papers:**
- [Paper 1 + ArXiv Link]
- [Paper 2 + ArXiv Link]

**Related Concepts:**
- [Concept 1]
- [Concept 2]

**Tags:** #[tag1] #[tag2] #[tag3]

---

## ğŸ’¡ ATOMIC NOTE (if score >50)

**Created:** [ ] Yes / [ ] No  
**Note ID:** [AB-papers-NOTE-XXXX]  
**Location:** [Path to note]

---

*Template version: 1.0*  
*Last updated: 2026-02-11*
```

---

## TEMPLATE 2: Paper-to-Article Template (Substack Format)

**Purpose:** Von Paper zu publishable Artikel  
**Format:** Markdown (Substack/Medium-ready)

```markdown
# [Catchy Title: Promise + Intrigue]

**Subtitle:** [What you'll learn in one sentence]

---

## The Problem

[2-3 sentences describing current pain point]

Most people try [common approach], but it fails because [reason].

What if there was a better way?

---

## Enter: [Paper Name]

Researchers at [Institution] just published [Paper Name], and it's changing how we think about [topic].

**The key insight:** [One sentence explanation]

Here's what you need to know.

---

## How It Works (The Simple Version)

[Explain concept like you're talking to a smart friend who's not an expert]

Think of it like this: [Analogy]

The process:
1. [Step 1 â€” plain language]
2. [Step 2 â€” plain language]
3. [Step 3 â€” plain language]

**Example:**
[Concrete example that illustrates the concept]

---

## Why This Matters

Before this paper, [old way].

Now, [new way].

**Real-world impact:**
- [Benefit 1]
- [Benefit 2]
- [Benefit 3]

---

## How You Can Use This (Right Now)

[Practical implementation section]

**Option 1: The Quick Win** (5 minutes)
[Simplest way to apply the concept]

```[language]
[Code snippet or step-by-step]
```

**Option 2: The Deep Dive** (1-2 hours)
[More involved implementation]

**Resources:**
- Paper: [ArXiv Link]
- Code: [GitHub Link]
- [Other relevant resources]

---

## What Doesn't Work (Be Honest)

This isn't a silver bullet. Here's what you should know:

**Limitations:**
- [Limitation 1 + why it matters]
- [Limitation 2 + why it matters]

**When to use this:**
- [Use case 1]
- [Use case 2]

**When to skip this:**
- [Anti-use-case 1]
- [Anti-use-case 2]

---

## The Big Picture

[Connect to broader trends or implications]

[Paper Name] is part of a bigger shift: [trend].

Other papers to watch:
- [Related Paper 1]
- [Related Paper 2]

---

## Key Takeaway

[One sentence summary of main point]

**Your next step:**
[Clear call-to-action â€” try it, read more, share thoughts, etc.]

---

**Further Reading:**
- Full paper: [Link]
- Related articles: [Link]
- My other AI posts: [Link]

**Discussion:**
What do you think? Have you tried this? [Invite comments]

---

*Written by [Your Name] | [Date]*  
*Subscribe for more AI insights: [Link]*
```

---

## ğŸ“Œ USAGE NOTES

### For Atomic Notes:
- **Storage:** `$OBSIDIAN/60_Resources/Knowledge/Papers/`
- **Naming:** `AB-papers-NOTE-[XXXX]-[Short-Title].md`
- **Linking:** Cross-reference other notes with `[[Note-ID]]`
- **Tags:** Use consistent tags (#mvp, #architecture, #memory, etc.)

### For Playbooks:
- **When to use:** Before starting any new task in the domain
- **Customization:** Adapt steps to your context (skip what's not relevant)
- **Iteration:** Update playbook based on lessons learned

### For Templates:
- **Duplication:** Copy template, don't edit original
- **Completion:** Check off [ ] boxes as you go
- **Version Control:** Note template version for future updates

---

**Asset Pack Version:** 1.0  
**Created:** 2026-02-11  
**Maintained by:** Mia (OpenClaw)  
**Last Updated:** 2026-02-11

---

*These assets are LIVING DOCUMENTS â€” update them as you learn!*
