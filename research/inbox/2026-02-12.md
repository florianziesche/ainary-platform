# Research Intake ‚Äî 2026-02-12

**Source:** Latent Space AINews (Feb 10-11, 2026)
**Scan Time:** 2026-02-12 13:00 CET
**Articles Scanned:** 1 new article from Latent Space
**Relevance Filter:** AI agents, memory, self-improvement, architecture, multi-agent

---

## Top 10 Most Relevant Findings

### üî• 1. DeepSeek V4-lite: 1M Context + "Inhabits Context" [Relevance: 9/10]
**Link:** https://www.latent.space/p/ainews-zai-glm-5-new-sota-open-weights (see DeepSeek section)
**Summary:** DeepSeek rolled out 1M context with "frontier-level attention" ‚Äî not just retrieval, but the model "inhabits a context" proactively. Uses mature sparse/NSA-like approach. MLA architecture with new DP Attention (DPA) fix for tensor parallelism (zero KV redundancy, +92% throughput).
**Why It Matters:** Breakthrough in long-context reasoning. "First truly capable 1M context model out of China."

### üèóÔ∏è 2. GLM-5: 744B MoE with DeepSeek Sparse Attention [Relevance: 8/10]
**Link:** https://z.ai/blog/glm-5
**Summary:** Zhipu AI released GLM-5 (744B params, 40B active). Integrates DeepSeek Sparse Attention (DSA) for cheaper long-context serving. MIT license. Positioned for "agentic engineering" and long-horizon tasks. Scores 50 on Intelligence Index (new open weights leader).
**Why It Matters:** Open MoE architecture learning from DeepSeek. Shows convergence on sparse attention as key primitive.

### ü§ñ 3. Karpathy's Agent-as-Refactoring-Engine Workflow [Relevance: 9/10]
**Link:** https://twitter.com/karpathy/status/2021633574089416993
**Summary:** Concrete example of "malleable software" ‚Äî used DeepWiki MCP + GitHub CLI to have agent "rip out" specific implementation from torchao fp8 repo into self-contained file with tests, deleting dependencies. Small speed win. Repo-as-docs, agents as porting engines.
**Why It Matters:** New software composition pattern. Not just code generation ‚Äî code extraction and refactoring.

### üîç 4. Agent Observability as Evaluation Substrate [Relevance: 8/10]
**Link:** https://twitter.com/LangChain/status/2021722975121420496
**Summary:** LangChain argues "the run is the primary artifact" ‚Äî traces as source-of-truth for agent evaluation. Published guidance distinguishing agent observability/evaluation from traditional logging.
**Why It Matters:** Evaluation methodology for multi-step agents. Critical for reliability engineering.

### üõ†Ô∏è 5. MCP Support in llama.cpp (Agentic Loops) [Relevance: 9/10]
**Link:** https://www.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/
**Summary:** MCP integration in llama.cpp enables native agentic loops with local models. Addresses tooling overhead and hallucinated tool calls. Focus on robust client-side foundation before extending to llama-server backend.
**Why It Matters:** Local agentic workflows without cloud APIs. Self-hosting breakthrough.

### üèõÔ∏è 6. Agent Architecture: Sandbox-in-Agent vs Agent-in-Sandbox [Relevance: 8/10]
**Link:** https://twitter.com/bernhardsson/status/2021527682534760709
**Summary:** Key design choice for agent systems: what can LLM-generated code touch vs what the agent can do. Debate on separation of concerns and security boundaries.
**Why It Matters:** Fundamental architecture pattern for safe agentic systems.

### ‚ö° 7. Mini-SWE-Agent 2.0: Minimal Coding Agent (~100 LoC) [Relevance: 7/10]
**Link:** https://twitter.com/KLieret/status/2021606142699356215
**Summary:** Deliberately minimal coding agent (100 LoC each for agent/model/env). Used for benchmarks and RL training. Push toward simpler, auditable harnesses vs giant frameworks.
**Why It Matters:** Minimalism as design principle. Easier to understand, modify, and verify.

### üí∞ 8. $3M Open Benchmarks Grants (Snorkel + Partners) [Relevance: 7/10]
**Link:** https://twitter.com/vincentsunnchen/status/2021663737716125781
**Summary:** $3M commitment to fund open benchmarks to close eval gap. Partners: HF, Together, Prime Intellect, Factory, Harbor, PyTorch. Sentiment: public evals lag internal frontier testing.
**Why It Matters:** Evaluation infrastructure for open models. Critical for independent progress measurement.

### üöÄ 9. OpenAI: Multi-Hour Workflow Primitives [Relevance: 7/10]
**Link:** https://twitter.com/OpenAIDevs/status/2021725246244671606
**Summary:** OpenAI DevRel published guidance for running multi-hour workflows reliably. 1,500 PRs shipped by "steering Codex" with zero manual coding. Harness engineering as key discipline.
**Why It Matters:** Production patterns for long-running agents. Reliability engineering insights.

### ‚öôÔ∏è 10. Unsloth MoE Triton Kernels: 12x Faster Training [Relevance: 7/10]
**Link:** https://www.reddit.com/r/LocalLLaMA/comments/1r26vfw/train_moe_models_12x_faster_with_30_less_memory/
**Summary:** New Unsloth kernels enable MoE training 12x faster with 35% less VRAM. No accuracy loss. Compatible with consumer and datacenter GPUs. Uses PyTorch's torch._grouped_mm.
**Why It Matters:** Democratizes MoE training. Makes local experimentation feasible.

---

## Breakthrough Flag üö®

**None this scan.** Closest to 9/10: DeepSeek V4-lite's "inhabits context" claim and Karpathy's refactoring workflow. Both worth monitoring.

---

## Next Steps

1. Deep dive on DeepSeek Sparse Attention architecture
2. Test MCP in llama.cpp for local agentic workflows
3. Explore agent observability patterns (LangChain guidance)
4. Review OpenAI's multi-hour workflow primitives docs

---

**Processing Status:** ‚úÖ Complete
**Articles Marked Read:** Pending (run `blogwatcher read-all` after review)
