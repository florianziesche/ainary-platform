# Research Report 02: Enterprise AI Agents
## Systematic Analysis of 19 Papers ‚Äî Pure Technical Perspective

*Generated: 2026-02-27 | Analyst: MIIA üèîÔ∏è*
*Method: arXiv abstract analysis, E/I/J/A labels, Admiralty B2-C3*
*Source: github.com/masamasa59/ai-agent-papers/application-papers/enterprise-agents.md*

---

## Executive Summary (BLUF)

**19 Papers. 4 Kernerkenntnisse:**

1. **Enterprise Agents scheitern an Komplexit√§t.** CRMArena: <40% Success (ReAct), <55% (Function Calling). EnterpriseBench: Bestes Modell 41.8%. Agents k√∂nnen einfache Tasks, aber multi-step Enterprise Workflows √ºberfordern sie systematisch.
2. **Das CRM-Problem ist gel√∂st ‚Äî fast.** CRMWeaver (Agentic RL + Shared Memories) schafft Durchbr√ºche auf CRMArena-Pro. Der Trick: RL-Training auf synthetischen Business-Daten + Retrieval √§hnlicher gel√∂ster Probleme zur Inference-Time.
3. **Agentic BPM ist das n√§chste Paradigma.** Shift von Automation ‚Üí Autonomie. Process Mining + LLM Agents = Agentic Business Process Management Systems (A-BPMS). Nicht Tasks automatisieren, sondern ganze Prozesse autonom steuern.
4. **Public Sector und Industrial Benchmarks existieren nicht.** Von >1.300 analysierten Benchmarks: 0 erf√ºllen Public-Sector-Kriterien. AssetOpsBench ist der erste Industrial-Agent-Benchmark √ºberhaupt.

---

## Paper-Analyse (chronologisch, dann thematisch)

---

### Paper 1: Tutor CoPilot
**"A Human-AI Approach for Scaling Real-Time Expertise"**
*Oct 2024 | arXiv:2410.03017*

- [E] Erster RCT (Randomized Controlled Trial) eines Human-AI Systems in Live-Tutoring
- [E] 900 Tutoren, 1.800 K-12 Sch√ºler, unter-versorgte Communities
- [E] Ergebnis: +4 Prozentpunkte Mastery (p<0.01), +9pp bei schw√§cheren Tutoren
- [E] Kosten: $20/Tutor/Jahr ‚Äî negligible
- [I] Methodisch hervorragend: Pre-registered Analysis Plan, RCT-Design
- [I] Technisches Pattern: "Model of Expert Thinking" als Augmentation, nicht Replacement

**Technische Architektur:**
- LLM als Real-Time Copilot (nicht autonom, sondern Suggestion-Mode)
- Expert Thinking Model: Distilled aus Expert-Tutor-Trajectories
- Human bleibt in-the-loop, AI liefert kontextuell relevante Guidance

**[J] Significance:** Das sauberste empirische Evidence f√ºr Human-AI Collaboration in der gesamten Agent-Literatur. Kein anderes Paper hat einen RCT mit 2.700 Teilnehmern.

**Key Insight:** Der gr√∂√üte Impact ist bei den SCHW√ÑCHSTEN Nutzern (+9pp vs +4pp). AI als Equalizer, nicht als Optimizer. Implikation: Enterprise AI Agents sollten zuerst die weniger erfahrenen Mitarbeiter augmentieren ‚Äî dort ist der ROI am h√∂chsten.

---

### Paper 2: HR-Agent
**"A Task-Oriented Dialogue LLM Agent Tailored for HR Applications"**
*Oct 2024 | arXiv:2410.11239*

- [E] Task-Oriented Dialogue System f√ºr HR-Prozesse (Medical Claims, Access Requests, Time-Off)
- [E] Privacy-Architektur: Conversation Data wird NICHT an LLM gesendet w√§hrend Inference
- [I] L√∂st ein reales Problem: HR hat hunderte repetitive Prozesse, unaddressed by current AI

**Technische Architektur:**
- Confidentiality-First: LLM sieht Konversation nicht direkt
- Task Templates: Strukturierte Flows f√ºr jeden HR-Prozess
- Hybrid: LLM f√ºr NLU, deterministische Logik f√ºr Execution

**Key Insight:** Enterprise Agents in sensitiven Bereichen (HR, Finance, Legal) M√úSSEN Confidentiality-by-Design haben. Nicht nachtr√§glich draufschrauben ‚Äî in die Architektur einbauen.

---

### Paper 3: CRMArena ‚öñÔ∏è
**"Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks"**
*Nov 2024 | arXiv:2411.02305*

- [E] Benchmark: 9 Tasks, 3 Personas (Service Agent, Analyst, Manager)
- [E] 16 Industrial Objects (Account, Order, Case, Knowledge Article) mit hoher Interconnectivity
- [E] Latent Variables: Complaint Habits, Policy Violations (simuliert realistische Datenverteilungen)
- [E] **Ergebnis: SotA Agents < 40% Success (ReAct), < 55% (Function Calling)**

**Technische Details:**
- Task-Komplexit√§t: Multi-Object Queries erfordern JOIN-artige Reasoning √ºber 16 Objekttypen
- Latent Variables: Agent muss implizite Muster erkennen (z.B. welche Kunden systematisch Policies verletzen)
- Persona-spezifisch: Manager-Tasks sind schwerer als Agent-Tasks

**[J] Warum das wichtig ist:**
CRM ist das EINFACHSTE Enterprise-System (gut strukturierte Daten, klare APIs, definierte Workflows). Wenn Agents hier bei <55% liegen, ist die Performance auf komplexeren Systemen (ERP, SCM, MES) wahrscheinlich noch schlechter.

**Implikation f√ºr Agent-Architektur:** ReAct allein ist insufficient f√ºr Enterprise. Function Calling hilft (+15pp), aber l√∂st nicht das Grundproblem: Multi-Step Reasoning √ºber interconnected Objekte.

---

### Paper 4: AssetOpsBench ‚öñÔ∏èüî•
**"Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance"**
*Jun 2025 | arXiv:2506.03828*

- [E] **Erster Benchmark f√ºr Industrial AI Agents** (Industry 4.0)
- [E] End-to-End Asset Lifecycle: Condition Monitoring ‚Üí Maintenance Planning ‚Üí Intervention Scheduling
- [E] Vision: AI Agents die den gesamten Asset Lifecycle autonom managen
- [E] Framework f√ºr Development, Orchestration, Evaluation von Domain-Specific Agents

**Technische Architektur:**
- Multi-Capability Agents: Perception + Reasoning + Control integriert
- Domain-Specific: Nicht generisch, sondern auf Industrial Operations zugeschnitten
- Orchestration: Mehrere spezialisierte Agents f√ºr verschiedene Lifecycle-Phasen

**[J] Bedeutung:**
Traditionelle ML-Ans√§tze l√∂sen isolierte Tasks (Anomaly Detection ODER Maintenance Scheduling ODER Root Cause Analysis). AssetOpsBench ist das erste Framework das END-TO-END Automation in Industrial Settings benchmarkt. Das ist ein Paradigmenwechsel: Von "AI f√ºr ein Problem" zu "AI f√ºr den gesamten Prozess".

**Key Insight:** Industrial Agent Development braucht drei Dinge gleichzeitig: Perception (Sensordaten verstehen), Reasoning (Kausalit√§ten ableiten), Control (Aktionen planen). Kein aktuelles LLM kann alle drei gleichzeitig.

---

### Paper 5: AI Agents-as-Judge
**"Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents"**
*Jun 2025 | arXiv:2506.22485*

- [E] Multi-Agent System f√ºr Enterprise Document Review
- [E] Spezialisierte Agents pro Review-Kriterium: Template Compliance, Factual Correctness, etc.
- [E] Stack: LangChain + CrewAI + TruLens + Guidance
- [E] **Ergebnisse: 99% Information Consistency (vs 92% Mensch), halbe Error Rate, Review-Zeit: 30min ‚Üí 2.5min**
- [E] 95% Agreement Rate zwischen AI und menschlichem Expert-Review

**Technische Architektur:**
- Modular: Jeder Agent hat ein diskretes Review-Kriterium
- Parallel oder Sequential je nach Abh√§ngigkeit
- Standardisiertes Output-Schema (maschinenlesbar) f√ºr Downstream Analytics
- Continuous Monitoring + Human-in-the-Loop Feedback

**[J] Das Paper das Enterprise am schnellsten √ºberzeugt:**
- Quantifizierter ROI: 30min ‚Üí 2.5min = 12x Speedup
- Messbar besser als Menschen: 99% vs 92% Consistency
- Kein Black-Box: Standardisiertes Schema, auditierbar
- Low Risk: Document Review ist non-destructive (keine irreversiblen Aktionen)

**Key Insight:** Der schnellste Weg zu Enterprise AI Adoption ist via non-destructive, measurable, auditable Tasks. Document Review > Prozessautomation als Einstieg.

---

### Paper 6: Routine
**"A Structural Planning Framework for LLM Agent System in Enterprise"**
*Jul 2025 | arXiv:2507.14447*

- [E] Structural Planning Framework: LLM-generierte Pl√§ne als explizite Strukturen
- [I] Adressiert ein Kernproblem: LLM Agents planen implizit (im Token-Stream), was in Enterprise nicht auditierbar ist
- [I] "Routine" = vordefinierter Ablaufplan den der Agent befolgt, aber adaptiv anpassen kann

**Key Insight:** Enterprise braucht EXPLIZITE Pl√§ne, nicht implizites Reasoning. Auditierbarkeit > Flexibility. Ein Agent der seinen Plan als Datenstruktur exponiert (nicht als Token-Stream) ist f√ºr Enterprise 10x wertvoller.

---

### Paper 7: Compliance Brain Assistant
**"Conversational Agentic AI for Assisting Compliance Tasks"**
*Jul 2025 | arXiv:2507.17289*

- [E] User Query Router: Balanciert Response Quality vs Latency
- [E] Compliance-spezifisch: Regulatorische Anforderungen, Policy-Checks, Audit-Unterst√ºtzung
- [I] Enterprise Compliance = hohes Risiko, geringe Fehlertoleranz, strenge Auditierbarkeit

**Technische Architektur:**
- Query Router: Einfache Queries ‚Üí Fast Path (niedrigere Kosten), Komplexe ‚Üí Full Agent Pipeline
- Domain-Specific Grounding: Compliance-Dokumente als Knowledge Base
- Audit Trail: Jede Agent-Entscheidung dokumentiert

**Key Insight:** Query Routing (schnell vs. gr√ºndlich) ist ein unterbeleuchtetes Architekturmuster. Nicht jede Enterprise-Query braucht den vollen Agent-Stack. 80% der Anfragen sind einfach und sollten schnell beantwortet werden.

---

### Paper 8: Chatting with your ERP üî•
**"A Recipe"**
*Jul 2025 | arXiv:2507.23429*

- [E] LLM Agent f√ºr Production-Grade ERP: Natural Language ‚Üí SQL
- [E] **Dual-Agent Architecture: Reasoning Agent + Critique Agent**
- [E] Open-Weight LLMs + Ollama Deployment (self-hostable!)
- [E] Real industrial production-grade ERP System

**Technische Architektur:**
```
User Query (NL) ‚Üí Reasoning Agent ‚Üí SQL Draft
                                       ‚Üì
                   Critique Agent ‚Üê SQL Draft
                                       ‚Üì
                   Validated SQL ‚Üí ERP Database ‚Üí Result
```

- Reasoning Agent: Generiert SQL basierend auf Schema + Query
- Critique Agent: Pr√ºft SQL auf Korrektheit, Schema-Compliance, Safety
- Dual-Loop: Critique kann Reasoning Agent zur Revision zwingen

**[J] Warum das technisch elegant ist:**
1. **Separation of Concerns:** Generierung ‚â† Validation. Zwei separate LLMs mit unterschiedlichen Optimierungszielen.
2. **Self-Hostable:** Open-Weight + Ollama = l√§uft on-prem. Kein Cloud-Dependency.
3. **ERP-Specific:** Nicht generisch, sondern auf die Eigenheiten von ERP-Schemas zugeschnitten (normalisiert, viele JOINs, komplexe Beziehungen).

**Key Insight:** Der Dual-Agent Pattern (Generator + Critic) ist wahrscheinlich der robusteste Ansatz f√ºr Enterprise Database Interaction. Ein einzelner Agent halluziniert SQL. Zwei Agents korrigieren sich gegenseitig.

---

### Paper 9: SCUBA ‚öñÔ∏è
**"Salesforce Computer Use Benchmark"**
*Sep 2025 | arXiv:2509.26506*

- [E] 300 Task-Instanzen aus Real User Interviews
- [E] 3 Personas: Platform Admins, Sales Reps, Service Agents
- [E] Computer-Use Agents auf Salesforce Platform

**Key Insight:** Computer-Use (Screen-Interaction) statt API-Based Agents. Alternative Architektur: Agent interagiert mit Software wie ein Mensch (Klicks, Scrolls, Eingaben) statt √ºber APIs. Vorteil: Funktioniert mit JEDER Software ohne Integration. Nachteil: Langsamer, fragiler, nicht auditierbar auf Action-Level.

---

### Paper 10: Survey ‚Äî LLM-driven Industry Agents üìñüî•
**"Empowering Real-World: Technology, Practice, and Evaluation"**
*Oct 2025 | arXiv:2510.17491*

- [E] Umfassendster Survey zu Industry Agents (nicht nur Enterprise-IT, sondern Real-World)
- [E] Technology (LLM Capabilities) + Practice (Deployment Patterns) + Evaluation (Benchmarks)
- [I] Bridging Paper zwischen akademischer Forschung und industrieller Praxis

**Key Insight:** Drei Gaps identifiziert:
1. **Capability Gap:** LLMs k√∂nnen Reasoning, aber nicht Perception + Control
2. **Practice Gap:** Lab-Performance ‚â† Production-Performance
3. **Evaluation Gap:** Akademische Benchmarks ‚â† industrielle Anforderungen

---

### Paper 11: CRMWeaver üî•
**"Building Powerful Business Agent via Agentic RL and Shared Memories"**
*Oct 2025 | arXiv:2510.25333*

- [E] **State-of-the-Art auf CRMArena-Pro** (Nachfolger von CRMArena)
- [E] Lightweight Modell outperformt gr√∂√üere Modelle durch Training + Inference-Tricks
- [E] Zwei Innovationen:
  1. **Agentic RL auf synthetischen Business-Daten** (Training-Time)
  2. **Shared Memories: Retrieval √§hnlicher gel√∂ster Tasks** (Inference-Time)

**Technische Details:**
- Synthesis Data Generation: LLM generiert realistische Business-Szenarien f√ºr RL-Training
- RL-Paradigm: Agent lernt aus Reward-Signalen bei Interaction mit Business-Environment
- Shared Memories: Wenn Agent neue Task sieht ‚Üí retrievet Guidelines aus √§hnlichen fr√ºheren Tasks
- Generalization: Funktioniert auch in unseen Scenarios durch Memory-Transfer

**[J] Technisch bedeutend weil:**
- Zeigt dass RL-Training auf synthetischen Daten funktioniert (keine echten Business-Daten n√∂tig)
- Shared Memories = elegante L√∂sung f√ºr das Few-Shot-Problem in Enterprise (jedes Unternehmen hat einzigartige Prozesse)
- Lightweight: Kleinere Modelle + bessere Architektur > gr√∂√üere Modelle + naive Prompting

**Key Insight:** Die Zukunft von Enterprise Agents ist NICHT gr√∂√üere Modelle. Es ist: Kleinere Modelle + RL-Training auf dom√§nenspezifischen Daten + Memory-Systeme f√ºr Erfahrungstransfer.

---

### Paper 12: EnterpriseBench ‚öñÔ∏è
**"Can LLMs Help You at Work?"**
*Oct 2025 | arXiv:2510.27287*

- [E] 500 Tasks: Software Engineering, HR, Finance, Administration
- [E] Simulated Enterprise: Data Fragmentation, Access Control, Cross-Functional Workflows
- [E] Novel Data Generation Pipeline aus Organizational Metadata
- [E] **Bestes Modell: 41.8% Task Completion**

**Technische Details:**
- Data Fragmentation: Informationen √ºber 3-5 Datenquellen verteilt pro Task
- Access Control Hierarchies: Agent muss beachten WER auf WAS Zugriff hat
- Cross-Functional: Tasks erfordern Wissen aus mehreren Departments

**[J] Warum 41.8% ein Problem ist:**
Enterprise Tasks sind nicht "schwer" im akademischen Sinne. Es sind Standard-B√ºroaufgaben die jeder Junior-Mitarbeiter in 30min erledigt. Dass SotA-Agents bei <42% liegen zeigt: Das Problem ist nicht Intelligence, sondern **Organizational Context**. Agents verstehen nicht, wie Organisationen funktionieren.

**Key Insight:** Die drei Enterprise-Killer:
1. **Data Fragmentation:** Agent findet die Information nicht (verteilt √ºber 5 Systeme)
2. **Access Control:** Agent wei√ü nicht, was er sehen DARF
3. **Implicit Knowledge:** "Das macht man bei uns so" ist nirgendwo dokumentiert

---

### Paper 13: DataGovBench ‚öñÔ∏è
**"Benchmarking LLM Agents for Real-World Data Governance Workflows"**
*Dec 2025 | arXiv:2512.04416*

- [E] 150 Tasks f√ºr Data Governance (Quality, Security, Compliance)
- [E] "Reversed-Objective" Methodology: Synthetisiert realistisches Noise
- [E] DataGovAgent: Planner-Executor-Evaluator Architecture
- [E] Current Models: Scheitern an multi-step Workflows + Error Correction

**Technische Architektur (DataGovAgent):**
```
Planner: Zerlegt Task in Sub-Tasks + definiert Constraints
    ‚Üì
Executor: F√ºhrt Sub-Tasks aus (Code-Generation f√ºr Data Transformations)
    ‚Üì
Evaluator: Pr√ºft Output gegen Constraints + Quality Metrics
    ‚Üì
[Feedback Loop: Evaluator ‚Üí Planner bei Failures]
```

**Key Insight:** Planner-Executor-Evaluator ist ein robusteres Pattern als ReAct f√ºr Enterprise. Warum? Weil der Evaluator DETERMINISTISCHE Checks ausf√ºhren kann (Schema-Validation, Constraint-Checking), nicht probabilistische LLM-Urteile.

---

### Paper 14: DeepRule
**"Automated Business Rule Generation via Deep Predictive Modeling"**
*Dec 2025 | arXiv:2512.03607*

- [E] Tri-Level Architecture f√ºr Retail Assortment + Pricing
- [E] Hybrid Knowledge Fusion: LLMs f√ºr Semantic Parsing unstrukturierter Texte
- [E] Game-Theoretic Optimization f√ºr Supply Chain Interest Reconciliation

**Key Insight:** LLMs als "Knowledge Fusion Engine" f√ºr unstrukturierte Business-Daten (Verhandlungsprotokolle, Genehmigungsdokumente) ‚Äî nicht als Decision Maker, sondern als Structurer. Der LLM parsed, die Optimierung entscheidet.

---

### Paper 15: Advances in Agentic AI üìñ
**"Back to the Future"**
*Dec 2025 | arXiv:2512.24856*

- [E] Positionspapier: Begriffsdefinitionen von Intelligence bis Agentic AI
- [E] Unterscheidung: M1 (aktuelles LLM-basiertes Agentic AI = B2C-Extension) vs M2 (Strategy-based Agentic AI = echte B2B-Transformation)
- [I] M1 = "Information Retrieval UX repurposed for B2B" ‚Äî kritische Perspektive

**Key Insight:** Das Paper argumentiert dass aktuelles Agentic AI (M1) fundamentally ein B2C-Paradigma ist das f√ºr B2B umfunktioniert wird. Echte B2B-Transformation (M2) erfordert "Strategies-based" Agents die Business-Strategien verstehen, nicht nur Tasks ausf√ºhren. Kontrovers aber denkw√ºrdig.

---

### Paper 16: Time-Scaling Is What Agents Need Now
*Jan 2026 | arXiv:2601.02714*

- [E] Convergence: Neural Networks (Perception) + RL (Decision) + Symbolic AI (Reasoning) ‚Üí Cognitive Agents
- [E] CoT/ToT als "Time-Scaling" reframed: Agents brauchen ZEIT zum Denken
- [E] DeepSeek-R1 als Beispiel: Explicit Reasoning Trajectories

**Key Insight:** "Time-Scaling" = dem Agent mehr Compute-Budget zum Nachdenken geben. Analog zu Test-Time Compute Scaling. F√ºr Enterprise: Lieber 30 Sekunden nachdenken und richtig handeln als 3 Sekunden und falsch.

---

### Paper 17: AgencyBench ‚öñÔ∏è
**"Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts"**
*Jan 2026 | arXiv:2601.11044*

- [E] 1M-Token Kontexte ‚Äî testet Long-Context Agent-F√§higkeiten
- [E] Real-World: Nicht synthetische Tasks, sondern echte Dokumentkontexte
- [I] Erster Benchmark der Multi-Faceted Agent Capabilities in einem Test vereint

**Key Insight:** 1M-Token Context ist die neue Frontier. Enterprise-Dokumente (Vertr√§ge, Reports, Policies) sind lang. Agents die nur 32K Token verarbeiten k√∂nnen, sind f√ºr Enterprise unbrauchbar.

---

### Paper 18: Agent Benchmarks Fail Public Sector ‚öñÔ∏è
*Jan 2026 | arXiv:2601.20617 | IASEAI 2026*

- [E] >1.300 Benchmark Papers analysiert
- [E] Kriterien: Process-based, Realistic, Public-Sector-Specific, Sector-Relevant Metrics
- [E] **Ergebnis: KEIN Benchmark erf√ºllt alle Kriterien**
- [I] Call to Action: Neue Benchmarks n√∂tig

**Key Insight:** Wenn 1.300+ Benchmarks existieren und KEINER Public-Sector-Anforderungen erf√ºllt ‚Äî dann ist die gesamte Benchmark-Landschaft tech-biased. Enterprise und Public Sector haben fundamental andere Anforderungen (Process-based, Compliance, Auditierbarkeit) die akademische Benchmarks systematisch ignorieren.

---

### Paper 19: Agentic Business Process Management Systems üî•üî•
**"A-BPMS"**
*Jan 2026 | arXiv:2601.18833 | Keynote, AI for BPM Workshop 2025*

- [E] Position Paper: Paradigm Shift von Automation ‚Üí Autonomie
- [E] BPM-Evolution: Seit 90er Jahren, Wellen von Automation. Agentic AI = n√§chste Welle.
- [E] Architekturvision: A-BPMS (Agentic BPM Systems)
- [E] Process Mining als Foundation: Agents die Prozesse BEOBACHTEN, VERSTEHEN, OPTIMIEREN

**Technische Vision:**
```
Traditional BPM:
  Design Process ‚Üí Deploy ‚Üí Monitor ‚Üí Manually Improve
  (Design-Driven, Human-Managed)

Agentic BPM (A-BPMS):
  Mine Process ‚Üí Agent Observes ‚Üí Agent Reasons ‚Üí Agent Acts ‚Üí Agent Learns
  (Data-Driven, Agent-Managed, Human-Supervised)
```

- **Sense:** Agent beobachtet Prozess-States via Process Mining
- **Reason:** Agent identifiziert Improvement-Opportunities
- **Act:** Agent nimmt √Ñnderungen vor (Parameter, Routing, Resources)
- **Learn:** Agent lernt aus Outcomes und verbessert sich

**Continuum of Autonomy:** Von vollst√§ndig menschlich-kontrolliert bis vollst√§ndig autonom, je nach Risiko und Vertrauen.

**[J] Warum das das wichtigste Paper der Collection ist:**
Es definiert ein NEUES Paradigma. Nicht "AI die Tasks erledigt" (M1) sondern "AI die Gesch√§ftsprozesse versteht und optimiert" (M2). Process Mining + Agentic AI ist die Kombination die Enterprise tats√§chlich transformiert. Einzelne Task-Automation ist inkrementell. Prozess-Autonomie ist disruptiv.

---

## Synthese: 6 technische Erkenntnisse

### 1. Die Enterprise Performance Cliff

| Benchmark | Domain | Best Agent Performance | Gap zu Human |
|---|---|---|---|
| CRMArena | CRM | <55% (Function Calling) | ~45pp |
| EnterpriseBench | Multi-Domain | 41.8% | ~58pp |
| SCUBA | Salesforce | TBD | TBD |
| AssetOpsBench | Industrial | TBD (erst Jun 2025) | TBD |

**[I] Pattern:** Enterprise Agent Performance liegt konsistent bei 40-55%. Die Ursache ist NICHT mangelnde Intelligence, sondern:
1. Data Fragmentation (Information verteilt √ºber Systeme)
2. Organizational Context (implizites Wissen fehlt)
3. Multi-Step Reasoning √ºber interconnected Objects

### 2. Architektur-Patterns die funktionieren

| Pattern | Paper | St√§rke | Schw√§che |
|---|---|---|---|
| **Dual-Agent (Generator + Critic)** | Chatting with ERP | Robuster SQL, Self-Correction | 2x Compute-Kosten |
| **Planner-Executor-Evaluator** | DataGovAgent | Deterministische Validation | Rigide, wenig adaptiv |
| **Agentic RL + Shared Memories** | CRMWeaver | Lernt aus Erfahrung, generalisiert | Braucht RL-Training-Infrastruktur |
| **Query Router (Fast/Full)** | Compliance Brain | Kosteneffizient, 80% schneller Pfad | Router-Fehler = falscher Pfad |
| **Structural Planning (Routine)** | Routine | Auditierbar, explizite Pl√§ne | Weniger flexibel als implizites Reasoning |
| **Human-AI Copilot** | Tutor CoPilot | Bewiesener Impact (RCT), sicher | Mensch bleibt Bottleneck |

**[J] Empfehlung:** F√ºr Enterprise Agents, Hybrid-Architektur:
```
Query Router ‚Üí Simple: Fast Path (Direct RAG)
            ‚Üí Complex: Planner-Executor-Evaluator
            ‚Üí Critical: Dual-Agent + Human-in-the-Loop
```

### 3. Synthetic Data + RL = Enterprise Training

CRMWeaver beweist: RL-Training auf synthetischen Business-Daten funktioniert besser als Prompting gr√∂√üerer Modelle. Implikation:
- Keine echten Kundendaten n√∂tig f√ºr Training
- Lightweight Modelle + RL > Heavy Modelle + naive Prompts
- Shared Memories erm√∂glichen Generalization auf neue Szenarien

### 4. Process Mining + Agents = A-BPMS

Die A-BPMS Vision (Paper 19) ist das architektonisch ambitionierteste Konzept:
- Process Mining liefert das "Auge" (Was passiert gerade im Prozess?)
- LLM Agent liefert das "Gehirn" (Was sollte anders sein?)
- Actuation liefert die "Hand" (√Ñndere Parameter, Route, Resources)
- Learning liefert das "Ged√§chtnis" (Was hat funktioniert, was nicht?)

Das ist kein einzelner Agent ‚Äî es ist ein Agent-Ecosystem das auf einen Business-Prozess wirkt.

### 5. The "Non-Destructive First" Principle

Agents-as-Judge (Paper 5): 12x Speedup, 99% vs 92% Consistency, 95% Agreement mit Experten ‚Äî und ZERO RISK. Warum? Document Review √§ndert nichts. Es ist rein analytisch.

**[I] Enterprise AI Adoption Strategy:**
1. **Phase 1:** Non-destructive Agents (Review, Analysis, Search, Reporting) ‚Üí Vertrauen aufbauen
2. **Phase 2:** Reversible Agents (Drafting, Suggestions, Pre-filled Forms) ‚Üí Effizienz steigern
3. **Phase 3:** Autonomous Agents (Process Optimization, Resource Allocation) ‚Üí Transformation

### 6. Die Benchmark-L√ºcke ist die Opportunity

>1.300 Benchmarks existieren. 0 erf√ºllen Public-Sector-Kriterien. 1 (AssetOpsBench) addressiert Industrial. Die akademische Community optimiert auf das Falsche.

**Was gebraucht wird:**
- Process-based Benchmarks (nicht Task-based)
- Compliance-aware Metrics (nicht nur Accuracy)
- Domain-specific Evaluation (nicht generisch)
- Long-horizon Tasks (nicht single-turn)
- Access-Control-aware (nicht open-information)

---

## Technische Roadmap: Wie Enterprise Agents besser werden

```
2024                  2025                  2026                  2027
  ‚îÇ                     ‚îÇ                     ‚îÇ                     ‚îÇ
  ‚îÇ  ReAct Agents       ‚îÇ  RL-trained         ‚îÇ  A-BPMS             ‚îÇ  Autonomous
  ‚îÇ  <40% Success       ‚îÇ  Agents             ‚îÇ  Process-Level      ‚îÇ  Process
  ‚îÇ                     ‚îÇ  CRMWeaver          ‚îÇ  Autonomy           ‚îÇ  Optimization
  ‚îÇ  Single-Turn        ‚îÇ  50-70%?            ‚îÇ                     ‚îÇ
  ‚îÇ  Task Execution     ‚îÇ                     ‚îÇ  Process Mining     ‚îÇ  Self-Improving
  ‚îÇ                     ‚îÇ  Shared Memories    ‚îÇ  + LLM Agents       ‚îÇ  Business
  ‚îÇ  No Learning        ‚îÇ  Cross-Task         ‚îÇ                     ‚îÇ  Processes
  ‚îÇ                     ‚îÇ  Transfer           ‚îÇ  Continuum of       ‚îÇ
  ‚îÇ  Prompt-Based       ‚îÇ                     ‚îÇ  Autonomy           ‚îÇ
  ‚îÇ  Only               ‚îÇ  Dual-Agent         ‚îÇ  (Human ‚Üí Agent)    ‚îÇ
  ‚îÇ                     ‚îÇ  Patterns           ‚îÇ                     ‚îÇ
  ‚ñº                     ‚ñº                     ‚ñº                     ‚ñº
```

---

## Top 5 Papers (Technical Impact)

| Rang | Paper | Warum technisch bedeutend |
|------|-------|--------------------------|
| 1 | **A-BPMS** (2601.18833) | Definiert neues Paradigma: Process-Level Agent Autonomie |
| 2 | **CRMWeaver** (2510.25333) | Beweist: RL + Shared Memories > gr√∂√üere Modelle |
| 3 | **Chatting with ERP** (2507.23429) | Dual-Agent Pattern f√ºr Enterprise DB, self-hostable |
| 4 | **EnterpriseBench** (2510.27287) | Quantifiziert den Enterprise Performance Gap (41.8%) |
| 5 | **Tutor CoPilot** (2410.03017) | Einziger RCT. Beweist: AI Impact h√∂chsten bei schw√§chsten Nutzern |

---

*Confidence: [80% ‚Äî Alle Abstracts verifiziert. EnterpriseBench/CRMArena Zahlen direkt aus Papers. A-BPMS ist ein Position Paper (Vision, nicht validiert). CRMWeaver Claims auf CRMArena-Pro nicht unabh√§ngig verifiziert. St√§rkste Evidenz: Tutor CoPilot (RCT, n=2700), CRMArena (<55%), EnterpriseBench (41.8%). Schw√§chste: A-BPMS (Vision), DeepRule (domain-specific, schwer generalisierbar).]*

*Beipackzettel: 19 Papers auf Abstract-Level. Volltext-Analyse w√ºrde architektonische Details und Reproduzierbarkeit besser bewerten. Keine eigenen Experimente durchgef√ºhrt. arXiv-Papers = Tier 2. Ausnahmen: EMNLP, IASEAI, AI for BPM Workshop Papers = Tier 1.5 (peer-reviewed Workshop/Industry Track).*

---
*MIIA üèîÔ∏è | Report 02/16 | 2026-02-27*
