# Experiment Log: AR-020 v3 â€” Self-Calibrating Research Report

**Experiment ID:** EXP-2026-02-19-001
**Start:** 2026-02-19 09:30 CET
**Operator:** Mia (Main Agent, Claude Opus 4.6)
**Goal:** Produziere den ersten selbst-kalibrierenden Research Report Ã¼ber Trust Calibration

---

## 1. Experiment-Design

### Forschungsfrage
"Kann ein AI-Agent-System einen Research Report produzieren, der (a) Harvard/McKinsey-QualitÃ¤t erreicht, (b) eigene empirische Daten enthÃ¤lt, und (c) die Methoden die er beschreibt auf sich selbst anwendet?"

### Hypothese
Ein Multi-Agent-System mit spezialisierten Sub-Agents (Research, Code, Synthesis) produziert qualitativ bessere Reports als ein einzelner Agent â€” aber nur wenn die Agents aufeinander aufbauen (Pipeline), nicht parallel arbeiten.

### Versuchsaufbau

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAIN AGENT (Opus)                     â”‚
â”‚              Orchestrierung, QA, Final Edit              â”‚
â”‚                                                         â”‚
â”‚  Parallel Phase:                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Agent 1      â”‚ â”‚ Agent 2      â”‚ â”‚ Agent 3          â”‚â”‚
â”‚  â”‚ RESEARCH     â”‚ â”‚ PYTHON       â”‚ â”‚ SYNTHESIS        â”‚â”‚
â”‚  â”‚ (Opus)       â”‚ â”‚ (Sonnet)     â”‚ â”‚ (Opus)           â”‚â”‚
â”‚  â”‚              â”‚ â”‚              â”‚ â”‚                   â”‚â”‚
â”‚  â”‚ 7Ã— R2 Topics â”‚ â”‚ Calibration  â”‚ â”‚ AR-020 v3 Report â”‚â”‚
â”‚  â”‚ 35 Hypothesenâ”‚ â”‚ Library +    â”‚ â”‚ (wartet auf 1+2) â”‚â”‚
â”‚  â”‚              â”‚ â”‚ 4 Experimentsâ”‚ â”‚                   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â”‚                â”‚                   â”‚          â”‚
â”‚         â–¼                â–¼                   â–¼          â”‚
â”‚    R2-frontier.md   ainary-calibration/   AR-020-v3    â”‚
â”‚    (35 Verdicts)    (Python + Results)    (Mega-Report)â”‚
â”‚                                                         â”‚
â”‚  Sequential Phase:                                      â”‚
â”‚  Main Agent reviewt alle 3 Outputs, QA, Final Assembly  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Warum dieses Design?

**Agent 1 (Research) und Agent 2 (Python) sind unabhÃ¤ngig** â€” sie kÃ¶nnen parallel laufen weil:
- Research braucht keinen Code
- Code braucht keine neuen Research-Ergebnisse (baut auf V2)

**Agent 3 (Synthesis) ist abhÃ¤ngig** â€” wartet auf 1+2 weil:
- Muss Research-Verdicts integrieren
- Muss Experiment-Daten einbauen
- Ist die "Synthesis-Schicht" die alles zusammenfÃ¼hrt

**Erwartete Failure Modes:**
- Agent 3 Timeout bevor Agent 1 fertig (mitigation: 40 Min Timeout, polling)
- Research-Agent findet keine Evidenz fÃ¼r radikale Hypothesen (mitigation: "keine Evidenz" ist ein valides Ergebnis)
- Python-Experiments simuliert statt real (mitigation: transparent dokumentieren)

---

## 2. Vorgeschichte (Iterationen V1 â†’ V2 â†’ V3)

### V1: Speed-Batch (05:00 CET)
- **Methode:** 1 Sub-Agent (Sonnet), Prompt "schreib 10 Reports", 3Ã— web_search pro Report
- **Dauer:** ~8 Min fÃ¼r AR-020
- **QualitÃ¤t:** B-Grade. Wikipedia-Level Zusammenfassung.
- **Kritische Fehler:** 
  - Empfiehlt Temperature Scaling als Default â†’ funktioniert nicht fÃ¼r API-basierte LLMs
  - Keine Hypothese vor Research
  - Keine Disconfirmation
  - Keine Contradictions gefunden
  - 0 Connections zu bestehendem Wissen
- **Quality Gate Score:** Nicht durchgefÃ¼hrt (kein Gate definiert)
- **Quellen:** 10, keine Admiralty Ratings
- **WÃ¶rter:** ~1.900

### V2: Golden Standard (08:18 CET)
- **Methode:** 1 Sub-Agent (Opus), R2 Standard + Research Protocol, 10+ web_search
- **Dauer:** ~10 Min
- **QualitÃ¤t:** A-Grade. Erste echte Deep Dive.
- **Verbesserungen vs V1:**
  - Hypothesis VOR Research â†’ Verdict: NUANCED
  - 6 Familien statt 3 AnsÃ¤tze
  - RLHF-Calibration Discovery (V1 hatte das nicht)
  - Black-Box Constraint erkannt (V1 ignorierte es)
  - 3 Contradictions gefunden und aufgelÃ¶st
  - 4 Connections zu bestehendem Wissen
  - Multi-Agent Calibration Gap identifiziert
- **Quality Gate Score:** 15/15
- **Quellen:** 20, mit Admiralty Ratings (A1-B2)
- **WÃ¶rter:** ~4.000
- **Outputs:** 4 (Obsidian Note, Full Report, Asset Pack, HTML)

### V2 â†’ V3: Was fehlt noch?

| Gap | V2 Status | V3 Ziel |
|-----|-----------|---------|
| Eigene Daten | 0 | Monte Carlo Sims, ECE Tests |
| Python Library | 0 | pip-installierbares Package |
| Case Studies | 0 | 2-3 echte Failure Cases |
| Practitioner Checklist | 0 | 10-Step "Monday Morning" Guide |
| Visual Exhibits | 0 | Decision Tree, Cost Waterfall, Propagation Chart |
| Hypothesen-Vielfalt | 1 Hypothesis | 35 Hypothesen (5 Ã— 7 Topics) |
| Self-Calibration | 0 | Report kalibriert sich selbst |
| Cross-References | Obsidian nur | HTML + Obsidian |
| Back Cover | Falsch | "AI Strategy Â· System Design Â· Execution Â· Consultancy Â· Research" |

---

## 3. Agent-Konfiguration

### Agent 1: calibration-frontier-research
- **Model:** Claude Opus 4.6 (max QualitÃ¤t fÃ¼r Research)
- **Timeout:** 30 Min
- **Input:** R2 Standard, Research Protocol, AR-020 v2
- **Task:** 7 R2 Topics Ã— 5 Hypothesen = 35 Verdicts
- **Expected Output:** ~15.000 WÃ¶rter, 21+ web_search
- **Risiko:** Timeout bei 7 Topics (mitigation: Quality > Completeness)

### Agent 2: calibration-python-library
- **Model:** Claude Sonnet 4.5 (gut genug fÃ¼r Code, schneller)
- **Timeout:** 30 Min
- **Input:** AR-020 v2 (6 Familien, 3-Tier Architektur)
- **Task:** Python Package + 4 Simulationsexperimente
- **Expected Output:** ~20 Python Files, Experiment JSONs
- **Risiko:** Code lÃ¤uft nicht (mitigation: Agent muss selbst testen)
- **Constraint:** Kein API-Key â†’ alle Experiments sind Simulationen, NICHT echte LLM-Calls

### Agent 3: ar020-v3-mega
- **Model:** Claude Opus 4.6 (Synthesis braucht Opus)
- **Timeout:** 40 Min
- **Input:** V2 + Agent 1 Output + Agent 2 Output
- **Task:** Mega-Report mit 5 Verbesserungen + Self-Calibration
- **Expected Output:** 4 Files (Obsidian, Full, HTML, Assets), ~8.000 WÃ¶rter
- **Dependency:** Pollt Agent 1+2 Outputs alle 60s, Fallback nach 20 Min
- **Risiko:** Bekommt Inputs nicht rechtzeitig (mitigation: Fallback auf V2 Daten)

---

## 4. Metriken & Erfolgskriterien

### Quantitativ
| Metrik | V1 | V2 | V3 Target |
|--------|-----|-----|-----------|
| WÃ¶rter | 1.900 | 4.000 | 5.000-8.000 |
| Quellen | 10 | 20 | 30+ |
| Hypothesen getestet | 0 | 1 | 35 |
| Contradictions | 0 | 3 | 5+ |
| Connections | 0 | 4 | 10+ |
| Eigene Datenpunkte | 0 | 0 | 1000+ (Simulationen) |
| Quality Gate | n/a | 15/15 | 15/15 |
| Visual Exhibits | 0 | 0 | 3-5 |
| Case Studies | 0 | 0 | 2-3 |

### Qualitativ
- [ ] Self-Calibration Section vorhanden und glaubwÃ¼rdig
- [ ] Practitioner Checklist actionable (nicht generisch)
- [ ] Python Library lauffÃ¤hig (python3 run_experiments.py)
- [ ] Jede Hypothese hat Verdict + Evidenz
- [ ] Report widerspricht sich nicht intern
- [ ] Back Cover korrekt

---

## 5. Kosten-Tracking

| Agent | Model | Est. Input Tokens | Est. Output Tokens | Est. Cost |
|-------|-------|-------------------|--------------------| --------- |
| Agent 1 (Research) | Opus | ~50K | ~25K | ~$0.75 |
| Agent 2 (Python) | Sonnet | ~30K | ~40K | ~$0.70 |
| Agent 3 (Synthesis) | Opus | ~80K | ~30K | ~$1.05 |
| **Total** | | ~160K | ~95K | **~$2.50** |

Zum Vergleich:
- V1: ~$0.15 (1 Sonnet Call)
- V2: ~$0.50 (1 Opus Call)
- V3: ~$2.50 (3 Agents, 2 Opus + 1 Sonnet)
- McKinsey-Report gleichwertiger Tiefe: ~$50.000-150.000

**Cost Efficiency: ~1:20.000 bis 1:60.000 vs. menschlicher Report.**

---

## 6. Timeline

| Zeit (CET) | Event | Status |
|------------|-------|--------|
| 04:00 | Session Start, Vault Infrastructure | âœ… Done |
| 05:00 | AR-016-025 V1 Batch (Speed Mode) | âœ… Done |
| 07:46 | AR-020 V1 HTML generiert | âœ… Done |
| 08:18 | AR-020 V2 Golden Standard (Opus, R2) | âœ… Done |
| 08:30 | V2 PDF generiert und an Florian gesendet | âœ… Done |
| 09:15 | Hierarchical Lookup Tool gebaut | âœ… Done |
| 09:30 | V3 Experiment gestartet (3 parallele Agents) | ğŸ”„ Running |
| ~09:45 | Agent 2 (Python) erwartet fertig | â³ Pending |
| ~09:55 | Agent 1 (Research) erwartet fertig | â³ Pending |
| ~10:05 | Agent 3 (Synthesis) erwartet fertig | â³ Pending |
| ~10:15 | Main Agent: QA, Final Assembly, PDF | â³ Pending |

---

## 7. Lessons Learned (wird live aktualisiert)

### Von V1 â†’ V2
1. **Speed-Batch produziert gefÃ¤hrliche Outputs** â€” V1 empfahl eine Methode die nicht funktioniert
2. **Hypothesis + Disconfirmation = 10x bessere Insights** â€” V2 fand RLHF-Problem, V1 nicht
3. **R2 Quality Gate ist notwendig, nicht optional** â€” ohne Gate keine QualitÃ¤tskontrolle
4. **Opus > Sonnet fÃ¼r Research** â€” Sonnet reproduziert bekanntes Wissen, Opus findet Neues

### Von V2 â†’ V3 (Hypothesen, noch nicht verifiziert)
5. **Multi-Agent > Single-Agent fÃ¼r komplexe Reports** â€” zu verifizieren
6. **Eigene Daten > Zitate** â€” zu verifizieren
7. **Self-Calibration macht den Report glaubwÃ¼rdiger** â€” zu verifizieren
8. **35 Hypothesen > 1 Hypothese** â€” zu verifizieren (Risiko: Breite > Tiefe)

---

## 8. Reproduzierbarkeit

Dieses Experiment kann reproduziert werden mit:
1. OpenClaw Gateway mit Claude Opus 4.6 + Sonnet 4.5
2. Die 3 Sub-Agent Prompts (in Session Transcripts)
3. AR-020 v2 als Basis-Input
4. R2-DEEP-DIVE-RESEARCH.md + RESEARCH-PROTOCOL.md Standards
5. ~$2.50 API-Budget

**Nicht reproduzierbar:**
- Exakte web_search Ergebnisse (zeitabhÃ¤ngig)
- Memory/Vault Context (spezifisch fÃ¼r unsere Installation)
- Timing/Ordering der Agent-Completion

---

## 9. Ergebnisse (wird nach Completion ausgefÃ¼llt)

### Agent 1 (Research)
- Status: â³ Running
- Completion Time: â€”
- Output Size: â€”
- Hypothesen Verdicts: â€”/35
- Neue Insights: â€”

### Agent 2 (Python)
- Status: â³ Running
- Completion Time: â€”
- Output Size: â€”
- Experiments Run: â€”/4
- Code LauffÃ¤hig: â€”

### Agent 3 (Synthesis)
- Status: â³ Running
- Completion Time: â€”
- Output Size: â€”
- Self-Calibration: â€”
- Quality Gate: â€”/15

### Final Assessment
- V3 besser als V2? â€”
- Self-Calibration glaubwÃ¼rdig? â€”
- Publishable Quality? â€”
- McKinsey-Level? â€”

---

*Log wird live aktualisiert durch Main Agent.*
*NÃ¤chstes Update: bei Completion der Sub-Agents.*
