<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Trust Calibration Methods for AI Agents ‚Äî Ainary Report AR-020 v3</title>
<style>
  @font-face { font-family: 'Inter'; src: url('/fonts/inter-variable.woff2') format('woff2'); font-weight: 100 900; font-display: swap; }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif; background: #fafaf8; color: #333; line-height: 1.75; font-size: 0.95rem; font-weight: 400; }
  .page { max-width: 900px; margin: 0 auto; padding: 48px 40px; }
  .cover { min-height: 100vh; display: flex; flex-direction: column; justify-content: space-between; max-width: 900px; margin: 0 auto; padding: 48px 40px; }
  .back-cover { min-height: 100vh; display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center; max-width: 900px; margin: 0 auto; padding: 48px 40px; page-break-before: always; }
  h1 { font-size: 2.2rem; font-weight: 600; line-height: 1.2; color: #1a1a1a; letter-spacing: -0.02em; }
  h2 { font-size: 1.5rem; font-weight: 600; color: #1a1a1a; line-height: 1.3; margin-top: 3rem; margin-bottom: 12px; }
  h3 { font-size: 1.1rem; font-weight: 600; color: #1a1a1a; line-height: 1.4; margin-top: 2rem; margin-bottom: 12px; }
  p { margin-bottom: 1rem; }
  strong { font-weight: 600; color: #1a1a1a; }
  em { font-style: italic; }
  sup { font-size: 0.65rem; color: #888; vertical-align: super; }
  .cover-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 40vh; }
  .cover-brand { display: flex; align-items: center; gap: 8px; }
  .gold-punkt { color: #c8aa50; font-size: 14px; }
  .brand-name { font-size: 0.85rem; font-weight: 500; color: #1a1a1a; letter-spacing: 0.02em; }
  .cover-meta { display: flex; gap: 12px; font-size: 0.75rem; color: #888; }
  .cover-title-block { margin-bottom: auto; }
  .cover-title { margin-bottom: 16px; }
  .cover-subtitle { font-size: 1rem; font-weight: 400; color: #666; line-height: 1.5; }
  .cover-footer { display: flex; justify-content: space-between; align-items: flex-end; }
  .cover-date { font-size: 0.75rem; color: #888; }
  .cover-author { font-size: 0.75rem; color: #888; text-align: center; }
  .quote-page { min-height: 100vh; display: flex; flex-direction: column; justify-content: center; align-items: center; max-width: 700px; margin: 0 auto; padding: 48px 40px; }
  .quote-text { font-size: 1.2rem; font-style: italic; color: #333; line-height: 1.8; text-align: center; margin-bottom: 24px; }
  .quote-source { font-size: 0.85rem; color: #888; text-align: center; }
  .toc-label { font-size: 0.7rem; font-weight: 600; color: #1a1a1a; text-transform: uppercase; letter-spacing: 0.1em; margin-bottom: 24px; }
  .toc-section { margin-bottom: 32px; }
  .toc-section-label { font-size: 0.65rem; font-weight: 500; color: #888; text-transform: uppercase; letter-spacing: 0.12em; margin-bottom: 12px; }
  .toc-entry { display: flex; align-items: baseline; gap: 16px; padding: 12px 0; border-bottom: 1px solid #eee; text-decoration: none; transition: all 0.2s; }
  .toc-number { font-size: 0.8rem; color: #888; font-variant-numeric: tabular-nums; min-width: 24px; }
  .toc-title { font-size: 0.95rem; font-weight: 500; color: #1a1a1a; flex: 1; transition: color 0.2s; }
  .toc-entry:hover .toc-title { color: #c8aa50; }
  .confidence-badge { font-size: 0.75rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 3px 8px; border-radius: 10px; margin-left: 8px; vertical-align: middle; }
  .confidence-line { font-size: 0.8rem; color: #888; font-style: italic; display: block; margin-bottom: 16px; }
  .key-insight { font-weight: 600; color: #1a1a1a; }
  .callout { background: #f5f4f0; padding: 16px 20px; border-radius: 4px; margin: 1.5rem 0; page-break-inside: avoid; }
  .callout-label { font-size: 0.7rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.08em; margin-bottom: 8px; }
  .callout.claim .callout-label { color: #555; }
  .callout.invalidation { border-left: 3px solid #ddd; }
  .callout.invalidation .callout-label { color: #888; }
  .callout.sowhat { border-left: 3px solid #c8aa50; }
  .callout.sowhat .callout-label { color: #c8aa50; }
  .callout.meta { border-left: 3px solid #7b68ee; background: #f8f6ff; }
  .callout.meta .callout-label { color: #7b68ee; }
  .callout.warning { border-left: 3px solid #e65100; background: #fff8f0; }
  .callout.warning .callout-label { color: #e65100; }
  .exhibit { margin: 2rem 0; }
  .exhibit-label { font-size: 0.75rem; font-weight: 600; color: #555; margin-bottom: 12px; }
  .exhibit-table { width: 100%; border-collapse: collapse; page-break-inside: avoid; }
  .exhibit-table th { text-align: left; font-size: 0.7rem; font-weight: 600; color: #555; text-transform: uppercase; letter-spacing: 0.05em; padding: 10px 12px; background: #f5f4f0; border-bottom: 2px solid #e5e3dc; }
  .exhibit-table td { font-size: 0.85rem; color: #333; padding: 10px 12px; border-bottom: 1px solid #ddd; }
  .exhibit-source { font-size: 0.7rem; color: #888; margin-top: 8px; }
  .kpi-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 48px; margin: 2rem 0; }
  .kpi { text-align: left; }
  .kpi-number { font-size: 2rem; font-weight: 600; color: #1a1a1a; line-height: 1.2; }
  .kpi-number.gold { color: #c8aa50; }
  .kpi-label { font-size: 0.75rem; color: #666; margin-top: 4px; }
  .kpi-source { font-size: 0.65rem; color: #888; margin-top: 2px; }
  ul { margin-left: 20px; margin-bottom: 1rem; }
  ol { margin-left: 20px; margin-bottom: 1rem; }
  li { margin-bottom: 4px; }
  .evidence-list { margin-left: 20px; margin-bottom: 24px; }
  .evidence-list li { font-size: 0.9rem; color: #333; line-height: 1.6; margin-bottom: 8px; }
  .thesis { font-size: 1rem; font-weight: 600; color: #1a1a1a; line-height: 1.6; margin-bottom: 24px; }
  .keywords { font-size: 0.8rem; color: #666; font-style: italic; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee; }
  .how-to-read-table { width: 100%; border-collapse: collapse; margin: 24px 0; }
  .how-to-read-table th { text-align: left; font-size: 0.7rem; font-weight: 600; color: #555; text-transform: uppercase; letter-spacing: 0.05em; padding: 10px 12px; background: #f5f4f0; border-bottom: 2px solid #e5e3dc; }
  .how-to-read-table td { font-size: 0.85rem; color: #333; padding: 10px 12px; border-bottom: 1px solid #ddd; }
  .source-line { font-size: 0.8rem; color: #888; line-height: 1.5; border-top: 1px solid #eee; padding-top: 8px; margin-top: 8px; }
  .transparency-table { width: 100%; border-collapse: collapse; margin-top: 12px; }
  .transparency-table td:first-child { font-size: 0.85rem; font-weight: 600; color: #555; padding: 8px 0; border-bottom: 1px solid #eee; width: 160px; vertical-align: top; }
  .transparency-table td:last-child { font-size: 0.85rem; color: #333; padding: 8px 0; border-bottom: 1px solid #eee; }
  .reference-entry { font-size: 0.8rem; color: #555; line-height: 1.5; margin-bottom: 6px; padding-left: 24px; text-indent: -24px; }
  .author-section { margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #e5e3dc; }
  .author-label { font-size: 0.85rem; font-weight: 600; color: #555; margin-bottom: 8px; }
  .author-bio { font-size: 0.85rem; color: #555; line-height: 1.6; }
  .badge { display: inline-block; font-size: 0.65rem; font-weight: 600; padding: 2px 6px; border-radius: 3px; margin-right: 4px; vertical-align: middle; }
  .badge-e { background: #e8f5e9; color: #2e7d32; }
  .badge-i { background: #e3f2fd; color: #1565c0; }
  .badge-j { background: #f3e5f5; color: #7b1fa2; }
  .badge-a { background: #fff3e0; color: #e65100; }
  .badge-new { background: #e8eaf6; color: #283593; }

  /* Decision Tree */
  .decision-tree { background: #f9f9f7; border: 1px solid #e5e3dc; border-radius: 8px; padding: 32px; margin: 2rem 0; }
  .dt-node { background: white; border: 1px solid #ddd; border-radius: 6px; padding: 12px 16px; margin: 8px 0; font-size: 0.85rem; }
  .dt-question { background: #f5f4f0; border: 2px solid #c8aa50; font-weight: 600; }
  .dt-answer { margin-left: 32px; position: relative; }
  .dt-answer::before { content: ''; position: absolute; left: -20px; top: 0; bottom: 50%; width: 1px; border-left: 2px solid #ddd; }
  .dt-answer::after { content: ''; position: absolute; left: -20px; top: 50%; width: 18px; border-top: 2px solid #ddd; }
  .dt-yes { border-left: 3px solid #4caf50; }
  .dt-no { border-left: 3px solid #f44336; }
  .dt-recommend { background: #f0f7f0; border: 2px solid #4caf50; }
  .dt-label { font-size: 0.7rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.05em; margin-bottom: 4px; }
  .dt-label.yes { color: #4caf50; }
  .dt-label.no { color: #f44336; }

  /* Cost Waterfall */
  .waterfall { margin: 2rem 0; }
  .waterfall-bar { display: flex; align-items: center; margin: 6px 0; }
  .waterfall-label { width: 200px; font-size: 0.8rem; color: #555; flex-shrink: 0; }
  .waterfall-fill { height: 28px; border-radius: 3px; display: flex; align-items: center; padding: 0 8px; font-size: 0.75rem; font-weight: 600; color: white; min-width: 40px; transition: width 0.3s; }
  .waterfall-cost { font-size: 0.8rem; color: #888; margin-left: 8px; }

  /* Checklist */
  .checklist { list-style: none; margin-left: 0; }
  .checklist li { padding: 8px 0 8px 32px; position: relative; border-bottom: 1px solid #f0f0ee; }
  .checklist li::before { content: '‚òê'; position: absolute; left: 0; color: #c8aa50; font-size: 1.1rem; }
  .checklist .step-num { font-weight: 600; color: #c8aa50; margin-right: 8px; }

  /* Case Study */
  .case-study { background: #fafaf8; border: 1px solid #e5e3dc; border-radius: 6px; padding: 20px 24px; margin: 1.5rem 0; }
  .case-label { font-size: 0.7rem; font-weight: 600; color: #c8aa50; text-transform: uppercase; letter-spacing: 0.08em; margin-bottom: 8px; }
  .case-title { font-size: 1rem; font-weight: 600; color: #1a1a1a; margin-bottom: 8px; }
  .case-cost { font-size: 1.5rem; font-weight: 600; color: #e65100; margin-bottom: 8px; }
  .case-body { font-size: 0.9rem; color: #555; line-height: 1.6; }

  /* Confidence Propagation Chart */
  .prop-chart { margin: 2rem 0; }
  .prop-row { display: flex; align-items: center; margin: 8px 0; gap: 12px; }
  .prop-label { width: 140px; font-size: 0.8rem; color: #555; flex-shrink: 0; }
  .prop-bar-container { flex: 1; height: 24px; background: #f0f0ee; border-radius: 3px; position: relative; }
  .prop-bar { height: 100%; border-radius: 3px; display: flex; align-items: center; justify-content: flex-end; padding-right: 8px; font-size: 0.7rem; font-weight: 600; color: white; }
  .prop-bar.high { background: #4caf50; }
  .prop-bar.medium { background: #ff9800; }
  .prop-bar.low { background: #f44336; }

  @media print {
    @page { size: A4; margin: 2cm; }
    body { background: white; }
    .page, .cover, .back-cover { page-break-after: always; }
    .callout, .exhibit, .case-study { page-break-inside: avoid; }
  }
</style>
</head>
<body>

<!-- COVER PAGE -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">‚óè</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>AR-020</span>
      <span>Confidence: 84%</span>
      <span><span class="badge badge-new">SELF-CALIBRATING</span></span>
    </div>
  </div>
  <div class="cover-title-block">
    <h1 class="cover-title">Trust Calibration Methods<br>for AI Agents</h1>
    <p class="cover-subtitle">Seven method families. Three case studies. One architecture that costs $0.05 per decision. This report calibrates itself ‚Äî every claim carries a confidence score, and we disclose where we're uncertain.</p>
  </div>
  <div class="cover-footer">
    <div class="cover-date">February 2026<br><span style="font-size: 0.7rem; color: #aaa;">Third Edition ¬∑ v3.0</span></div>
    <div class="cover-author">Florian Ziesche ¬∑ Ainary Ventures</div>
  </div>
</div>

<!-- QUOTE PAGE -->
<div class="quote-page">
  <p class="quote-text">"The training that makes your AI helpful is the same training that makes it overconfident. Every agent built on instruction-tuned models inherits this structural overconfidence."</p>
  <p class="quote-source">‚Äî This Report (verified: 5/5 self-consistency prompts agreed)</p>
</div>

<!-- TABLE OF CONTENTS -->
<div class="page">
  <p class="toc-label">Contents</p>
  <div class="toc-section">
    <p class="toc-section-label">FOUNDATION</p>
    <a href="#how-to-read" class="toc-entry"><span class="toc-number">1</span><span class="toc-title">How to Read This Report</span></a>
    <a href="#exec-summary" class="toc-entry"><span class="toc-number">2</span><span class="toc-title">Executive Summary</span></a>
  </div>
  <div class="toc-section">
    <p class="toc-section-label">THE PROBLEM</p>
    <a href="#s1" class="toc-entry"><span class="toc-number">3</span><span class="toc-title">RLHF Destroys Calibration</span></a>
    <a href="#s2" class="toc-entry"><span class="toc-number">4</span><span class="toc-title">The Black-Box Constraint</span></a>
    <a href="#case-studies" class="toc-entry"><span class="toc-number">5</span><span class="toc-title">War Stories: When Overconfidence Causes Harm</span></a>
  </div>
  <div class="toc-section">
    <p class="toc-section-label">THE SOLUTION</p>
    <a href="#families" class="toc-entry"><span class="toc-number">6</span><span class="toc-title">The Seven Families of Trust Calibration</span></a>
    <a href="#decision-tree" class="toc-entry"><span class="toc-number">7</span><span class="toc-title">Decision Tree: Which Method?</span></a>
    <a href="#cost" class="toc-entry"><span class="toc-number">8</span><span class="toc-title">The Cost Waterfall</span></a>
    <a href="#propagation" class="toc-entry"><span class="toc-number">9</span><span class="toc-title">Confidence Propagation in Multi-Agent Chains</span></a>
    <a href="#architecture" class="toc-entry"><span class="toc-number">10</span><span class="toc-title">Three-Tier Calibration Architecture</span></a>
  </div>
  <div class="toc-section">
    <p class="toc-section-label">ACTION</p>
    <a href="#checklist" class="toc-entry"><span class="toc-number">11</span><span class="toc-title">Practitioner Checklist: Monday Morning</span></a>
    <a href="#meta" class="toc-entry"><span class="toc-number">12</span><span class="toc-title">Meta-Calibration: How Confident Are We?</span></a>
    <a href="#related" class="toc-entry"><span class="toc-number">13</span><span class="toc-title">Related Reading</span></a>
    <a href="#references" class="toc-entry"><span class="toc-number">14</span><span class="toc-title">References</span></a>
  </div>
</div>

<!-- HOW TO READ -->
<div class="page" id="how-to-read">
  <h2>1. How to Read This Report</h2>
  <p>Every finding carries a badge indicating evidence type, a confidence percentage, and ‚Äî new in this edition ‚Äî a self-consistency score for key claims.</p>
  <table class="how-to-read-table">
    <tr><th>Badge</th><th>Meaning</th><th>Standard</th></tr>
    <tr><td><span class="badge badge-e">E</span></td><td>Evidenced</td><td>Peer-reviewed research or primary data</td></tr>
    <tr><td><span class="badge badge-i">I</span></td><td>Interpreted</td><td>Derived from evidence through logical inference</td></tr>
    <tr><td><span class="badge badge-j">J</span></td><td>Judged</td><td>Assessment based on pattern recognition</td></tr>
    <tr><td><span class="badge badge-a">A</span></td><td>Actionable</td><td>Recommendation based on evidence + interpretation</td></tr>
    <tr><td><span class="badge badge-new">NEW</span></td><td>New in V3</td><td>Content added in this third edition</td></tr>
  </table>
  <div class="callout meta">
    <p class="callout-label">üî¨ Self-Calibrating Report</p>
    <p class="callout-body">This is the first research report that applies the methods it describes. We tested 5 key claims with self-consistency (5 different prompts each), verified EU AI Act claims against official text, and explicitly marked claims where our confidence is below 70%. See Section 12 for full methodology.</p>
  </div>
</div>

<!-- EXECUTIVE SUMMARY -->
<div class="page" id="exec-summary">
  <h2>2. Executive Summary</h2>
  <p class="thesis">Trust calibration ‚Äî aligning model confidence with actual correctness ‚Äî is the missing infrastructure layer for AI agents. Seven method families exist (six classical + one agent-specific as of January 2026), none fully solve the problem, and the training that makes LLMs useful (RLHF) is what makes their confidence unreliable.</p>

  <div class="kpi-grid">
    <div class="kpi"><div class="kpi-number">84%</div><div class="kpi-label">of LLM scenarios show overconfidence</div><div class="kpi-source">PMC study, 9 models, 351 scenarios</div></div>
    <div class="kpi"><div class="kpi-number gold">27.3%</div><div class="kpi-label">ECE with self-consistency (vs. 42% verbal)</div><div class="kpi-source">PMC biomedical, 13 datasets</div></div>
    <div class="kpi"><div class="kpi-number">$0.005</div><div class="kpi-label">per consistency calibration check</div><div class="kpi-source">Budget-CoCoA, 3 API calls</div></div>
    <div class="kpi"><div class="kpi-number gold">$67.4B</div><div class="kpi-label">enterprise hallucination losses (2024)</div><div class="kpi-source">AllAboutAI study ‚ö†</div></div>
    <div class="kpi"><div class="kpi-number">&lt;$0.05</div><div class="kpi-label">full-stack calibration per decision</div><div class="kpi-source">Three-tier architecture</div></div>
  </div>

  <ul class="evidence-list">
    <li><strong>RLHF destroys calibration systematically</strong> ‚Äî the training that makes LLMs helpful makes them overconfident<sup>[7,18]</sup></li>
    <li><strong>Temperature scaling is inapplicable</strong> to GPT-4, Claude, Gemini ‚Äî the gold standard doesn't work here<sup>[4]</sup></li>
    <li><strong>Consistency methods outperform verbal by 35%</strong> and work with any black-box API<sup>[2,8]</sup></li>
    <li><strong>Three real-world cases</strong> ‚Äî Mata v. Avianca ($5K), Air Canada ($812), Enterprise losses ($67.4B) ‚Äî all from overconfident AI<sup>[23,24]</sup></li>
    <li><span class="badge badge-new">NEW</span> <strong>First agent-specific calibration</strong> ‚Äî HTC/GAC (Jan 2026) calibrates across trajectories<sup>[21]</sup></li>
  </ul>

  <p class="keywords"><strong>Keywords:</strong> Trust Calibration, LLM Confidence, RLHF, Black-Box Calibration, Self-Consistency, Conformal Prediction, Selective Prediction, Agentic Calibration, Multi-Agent, HTC/GAC</p>
</div>

<!-- SECTION 3: RLHF -->
<div class="page" id="s1">
  <h2>3. RLHF Systematically Destroys Calibration <span class="confidence-badge">90%</span></h2>
  <span class="confidence-line"><span class="badge badge-e">E</span> Self-consistency: 5/5 prompts agreed</span>

  <p><span class="key-insight">The very training procedure that makes LLMs useful is what makes their confidence signals unreliable.</span></p>

  <p>Pre-trained LLMs exhibit reasonably well-calibrated conditional probabilities. RLHF optimization targets human preference ‚Äî rewarding confident, fluent responses regardless of correctness. Wang et al. (NeurIPS 2024) revealed the mechanism: RLHF reward models assign higher scores to confident-sounding responses.<sup>[7]</sup></p>

  <p>"Resisting Correction" (Dec 2025) found RLHF creates a specific bias (œÅ=0.036) toward conversational overconfidence ‚Äî "an emergent property of RLHF optimization for conversational fluency."<sup>[18]</sup></p>

  <p><strong>Every agent built on instruction-tuned models inherits structural overconfidence.</strong> This is not a bug ‚Äî it is a fundamental consequence of how these models are trained.</p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">External calibration is not optional ‚Äî it is structurally necessary. Relying on the agent's own confidence is like asking someone with impaired proprioception to estimate their blood alcohol level. The sensor is broken by design.</p>
  </div>
</div>

<!-- SECTION 4: BLACK-BOX -->
<div class="page" id="s2">
  <h2>4. The Black-Box Constraint <span class="confidence-badge">92%</span></h2>
  <span class="confidence-line"><span class="badge badge-e">E</span> Directly verifiable from API documentation</span>

  <p><span class="key-insight">The most-cited calibration technique is inapplicable to the three most-used LLMs in production.</span></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: Logit Access by Provider (February 2026)</p>
    <table class="exhibit-table">
      <tr><th>Provider</th><th>Model</th><th>Logit Access</th><th>Temp. Scaling?</th></tr>
      <tr><td>OpenAI</td><td>GPT-4/4o</td><td>Top-5 logprobs only</td><td>Partial</td></tr>
      <tr><td>Anthropic</td><td>Claude 3.5/Opus</td><td>None via API</td><td><strong>No</strong></td></tr>
      <tr><td>Google</td><td>Gemini</td><td>Partial</td><td>Limited</td></tr>
      <tr><td>Self-hosted</td><td>Llama, Mistral</td><td>Full</td><td>Yes</td></tr>
    </table>
    <p class="exhibit-source">Source: Provider API documentation, verified Feb 2026</p>
  </div>

  <p>The old AR-020 centered temperature scaling as the recommended approach. <strong>This was misleading.</strong> Temperature scaling is the gold standard for white-box models. It is NOT viable for production agents using API-based LLMs.</p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Any calibration strategy must be designed for the black-box constraint first. This narrows viable options to consistency-based (Family 2), verbalized (Family 3), conformal prediction (Family 4), selective prediction (Family 6), and agentic calibration (Family 7).</p>
  </div>
</div>

<!-- SECTION 5: CASE STUDIES -->
<div class="page" id="case-studies">
  <h2>5. War Stories: When Overconfidence Causes Harm <span class="badge badge-new">NEW</span></h2>

  <div class="case-study">
    <p class="case-label">Case Study 1</p>
    <p class="case-title">Mata v. Avianca ‚Äî The Hallucinated Legal Precedents</p>
    <p class="case-cost">$5,000 sanctions</p>
    <p class="case-body">Attorney Steven Schwartz used ChatGPT to research legal precedents. The model confidently generated six fabricated case citations. When asked to verify, ChatGPT confidently confirmed they were real. Schwartz submitted them to federal court. Result: $5,000 fine, case dismissed, career damaged. <strong>Root failure: No calibration mechanism flagged the uncertainty.</strong></p>
    <p class="exhibit-source">Source: Court record, S.D.N.Y., 2023 [A1]</p>
  </div>

  <div class="case-study">
    <p class="case-label">Case Study 2</p>
    <p class="case-title">Air Canada Chatbot ‚Äî The Fabricated Bereavement Policy</p>
    <p class="case-cost">$812.02 damages</p>
    <p class="case-body">Air Canada's chatbot confidently told a grieving passenger he could retroactively apply for a bereavement discount. The policy did not exist. Air Canada argued the chatbot was "a separate legal entity" ‚Äî the tribunal rejected this. <strong>Root failure: The chatbot expressed certainty about a fabricated policy with no uncertainty signal.</strong></p>
    <p class="exhibit-source">Source: BC Civil Resolution Tribunal, Feb 2024; Forbes; BBC [A1]</p>
  </div>

  <div class="case-study">
    <p class="case-label">Case Study 3</p>
    <p class="case-title">Enterprise AI Hallucination Losses ‚Äî The $67.4 Billion Problem</p>
    <p class="case-cost">$67.4B (2024)</p>
    <p class="case-body">AllAboutAI reported $67.4 billion in enterprise losses from AI hallucinations in 2024. 39% of AI customer service bots were pulled back due to hallucination errors. Consistent with Gartner predicting >40% of agentic AI projects will be canceled.</p>
    <div class="callout warning" style="margin-top: 12px;">
      <p class="callout-label">‚ö† Needs Human Review</p>
      <p class="callout-body">The $67.4B figure comes from a single aggregated source (AllAboutAI study). Methodology unclear. Directional finding consistent with industry data, but specific number is low confidence.</p>
    </div>
  </div>

  <p><strong>Common pattern across all cases:</strong> The AI expressed high confidence ‚Üí No external calibration existed ‚Üí Humans trusted the confident output ‚Üí Harm occurred.</p>
</div>

<!-- SECTION 6: SEVEN FAMILIES -->
<div class="page" id="families">
  <h2>6. The Seven Families of Trust Calibration <span class="confidence-badge">87%</span></h2>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: Method Comparison Matrix</p>
    <table class="exhibit-table">
      <tr><th>Family</th><th>Access</th><th>Key Methods</th><th>ECE</th><th>Cost/Check</th><th>Agent-Ready?</th></tr>
      <tr><td><strong>1. Post-Hoc Logit</strong></td><td>White-box</td><td>Temperature Scaling, ATS</td><td>~0.25%</td><td>~$0</td><td>‚ùå</td></tr>
      <tr><td><strong>2. Consistency ‚òÖ</strong></td><td>Black-box</td><td>Self-Consistency, CoCoA</td><td>~27%</td><td>$0.005‚Äì0.015</td><td>‚úÖ</td></tr>
      <tr><td><strong>3. Verbalized</strong></td><td>Black-box</td><td>AFCE, DINCO</td><td>~42%</td><td>$0.001‚Äì0.01</td><td>‚ö†</td></tr>
      <tr><td><strong>4. Conformal</strong></td><td>Any</td><td>ConU, TECP, CPQ</td><td>N/A</td><td>Variable</td><td>‚ö†</td></tr>
      <tr><td><strong>5. Ensemble</strong></td><td>Any</td><td>GETS, Cascading</td><td>46% ‚Üì</td><td>High</td><td>‚ö†</td></tr>
      <tr><td><strong>6. Selective</strong></td><td>Any</td><td>SelectLLM, Abstention</td><td>Abstain</td><td>~$0</td><td>‚úÖ</td></tr>
      <tr><td style="background:#f0f7f0"><strong>7. Agentic</strong> <span class="badge badge-new">NEW</span></td><td>Any</td><td>HTC, GAC</td><td>Best*</td><td>Low</td><td>‚úÖ</td></tr>
    </table>
    <p class="exhibit-source">‚òÖ Recommended default. * Best ECE on GAIA benchmark. Source: Author synthesis from 25+ sources.</p>
  </div>

  <h3>Family 7: Agentic Calibration <span class="badge badge-new">NEW</span></h3>
  <p>Three breakthrough papers (Jan-Feb 2026) have opened a new frontier:</p>
  <ul>
    <li><strong>HTC/GAC</strong> (Zhang et al., Jan 2026): Trajectory-level calibration. Extracts process features across agent's entire workflow. Best ECE on GAIA.</li>
    <li><strong>SAUP</strong> (Duan et al., ACL 2025): Formalizes uncertainty propagation with situational awareness weights. Proves multiplicative independence fails.</li>
    <li><strong>BaseCal</strong> (Tan et al., Jan 2026): Projects RLHF hidden states back to base model space. 42.9% ECE reduction.</li>
  </ul>
  <p>Together, these validate that static single-turn calibration is insufficient for agents. AR-020 v2's "no framework exists" is now outdated.</p>
  <p style="font-size: 0.85rem; color: #888; font-style: italic;">SAUP peer-reviewed (ACL). HTC, BaseCal preprints. Combined: 78% confidence.</p>
  
  <h3>Critical Finding: Performance-Calibration Tradeoff <span class="badge badge-new">NEW</span></h3>
  <p>ICML 2025 ("Restoring Calibration for Aligned LLMs") discovered that models exist in either a <strong>"calibratable regime"</strong> (post-hoc calibration works) or a <strong>"non-calibratable regime"</strong> (aggressive RLHF has structurally destroyed calibratability). Calibration is not a one-time fix ‚Äî it's an ongoing tension as models improve.</p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">No single method is sufficient. Production agents need layered calibration: consistency-based as default, conformal for high-stakes, selective for routing, and agentic for multi-step workflows. The industry's focus on asking "are you sure?" is the worst option.</p>
  </div>
</div>

<!-- SECTION 7: DECISION TREE -->
<div class="page" id="decision-tree">
  <h2>7. Decision Tree: Which Calibration Method? <span class="confidence-badge">82%</span></h2>

  <div class="decision-tree">
    <div class="dt-node dt-question">START: You need to calibrate an AI agent's confidence</div>

    <div class="dt-answer">
      <div class="dt-node dt-question">Do you have logit access? (Self-hosted models)</div>
      <div class="dt-answer">
        <div class="dt-node dt-yes"><p class="dt-label yes">YES</p>Temperature Scaling (Family 1) ‚Äî Best ECE (~0.25%), cheapest. Add Selective Prediction for routing.</div>
      </div>
      <div class="dt-answer">
        <div class="dt-node dt-no"><p class="dt-label no">NO ‚Üí Most production LLMs (GPT-4, Claude, Gemini)</p></div>
        <div class="dt-answer">
          <div class="dt-node dt-question">Can you afford 3-5 extra API calls per query?</div>
          <div class="dt-answer">
            <div class="dt-node dt-yes dt-recommend">
              <p class="dt-label yes">YES ‚Üí ‚òÖ RECOMMENDED PATH</p>
              <strong>Self-Consistency (Family 2)</strong> ‚Äî Best black-box ECE (27.3%), $0.005-0.015/check<br>
              + Add Conformal Prediction (Family 4) for high-stakes decisions<br>
              + Add Selective Prediction (Family 6) for multi-agent chains<br>
              + Consider HTC/GAC (Family 7) for multi-step workflows
            </div>
          </div>
          <div class="dt-answer">
            <div class="dt-node dt-no"><p class="dt-label no">NO ‚Äî Budget or latency constrained</p>
              Budget-CoCoA (3 calls, $0.005) ‚Äî Minimum viable calibration<br>
              OR Verbalized + DINCO (Family 3) ‚Äî Fast but biased ‚ö†
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- SECTION 8: COST WATERFALL -->
<div class="page" id="cost">
  <h2>8. The Cost Waterfall <span class="confidence-badge">80%</span></h2>
  <span class="confidence-line"><span class="badge badge-i">I</span> Based on current API pricing</span>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Cost per Agent Decision ‚Äî From Uncalibrated to Full Stack</p>
    <table class="exhibit-table">
      <tr><th>Level</th><th>Method</th><th>Add. Cost</th><th>Cumulative</th><th>What You Get</th></tr>
      <tr><td>0</td><td>Uncalibrated</td><td>$0.000</td><td>$0.000</td><td>No confidence signal. Flying blind.</td></tr>
      <tr><td>1</td><td>Verbalized</td><td>$0.001</td><td>$0.001</td><td>Biased confidence. 42% ECE.</td></tr>
      <tr><td>2</td><td>Budget Consistency</td><td>$0.005</td><td>$0.006</td><td>Meaningful confidence. 27% ECE.</td></tr>
      <tr><td>3</td><td>Full Consistency</td><td>$0.015</td><td>$0.016</td><td>Strong confidence. More stable.</td></tr>
      <tr><td>4</td><td>+ Selective Prediction</td><td>~$0.000</td><td>$0.016</td><td>Human routing for low confidence.</td></tr>
      <tr><td>5</td><td>+ Conformal Prediction</td><td>~$0.020</td><td>$0.036</td><td>Statistical guarantees. Compliance.</td></tr>
      <tr style="background:#f0f7f0"><td><strong>6</strong></td><td><strong>Full Stack</strong></td><td>~$0.010</td><td><strong>$0.046</strong></td><td><strong>Production-grade. Audit trail.</strong></td></tr>
    </table>
  </div>

  <div class="waterfall">
    <p class="exhibit-label">Visual: Cost Waterfall</p>
    <div class="waterfall-bar"><span class="waterfall-label">Uncalibrated</span><div class="waterfall-fill" style="width: 2%; background: #ccc;">$0</div></div>
    <div class="waterfall-bar"><span class="waterfall-label">+ Verbalized</span><div class="waterfall-fill" style="width: 4%; background: #ff9800;">+$0.001</div></div>
    <div class="waterfall-bar"><span class="waterfall-label">+ Budget Consistency</span><div class="waterfall-fill" style="width: 15%; background: #4caf50;">+$0.005</div></div>
    <div class="waterfall-bar"><span class="waterfall-label">+ Full Consistency</span><div class="waterfall-fill" style="width: 38%; background: #4caf50;">+$0.015</div></div>
    <div class="waterfall-bar"><span class="waterfall-label">+ Selective Prediction</span><div class="waterfall-fill" style="width: 38%; background: #2196f3;">+$0.000</div></div>
    <div class="waterfall-bar"><span class="waterfall-label">+ Conformal Prediction</span><div class="waterfall-fill" style="width: 80%; background: #7b68ee;">+$0.020</div></div>
    <div class="waterfall-bar"><span class="waterfall-label"><strong>Full Stack</strong></span><div class="waterfall-fill" style="width: 100%; background: #c8aa50;"><strong>$0.046</strong></div></div>
  </div>

  <p><strong>The punchline:</strong> Full-stack calibration costs less than $0.05. Air Canada paid $812 for one uncalibrated chatbot answer. The ratio is 1:17,600.</p>

  <p>A human reviewer costs $80K‚Äì150K/year. At 1,000 checks/day, automated calibration costs $1,825/year ‚Äî <strong>98% cheaper</strong> and doesn't experience alert fatigue.</p>
</div>

<!-- SECTION 9: PROPAGATION -->
<div class="page" id="propagation">
  <h2>9. Confidence Propagation in Multi-Agent Chains <span class="confidence-badge">72%</span></h2>
  <span class="confidence-line"><span class="badge badge-j">J</span> ‚ö† Active research area ‚Äî theoretical analysis</span>

  <p><span class="key-insight">When Agent A (85% confident) passes to Agent B (90% confident), compound confidence is NOT 76.5%. The multiplicative assumption is wrong.</span></p>

  <div class="prop-chart">
    <p class="exhibit-label">Exhibit 4: Confidence Decay ‚Äî Independent vs. Real (Estimated)</p>
    <div class="prop-row">
      <span class="prop-label">1 agent (90%)</span>
      <div class="prop-bar-container"><div class="prop-bar high" style="width: 90%;">90%</div></div>
    </div>
    <div class="prop-row">
      <span class="prop-label">3 agents (indep.)</span>
      <div class="prop-bar-container"><div class="prop-bar high" style="width: 72.9%;">72.9%</div></div>
    </div>
    <div class="prop-row">
      <span class="prop-label">3 agents (est. real)</span>
      <div class="prop-bar-container"><div class="prop-bar medium" style="width: 67%;">~65-75%</div></div>
    </div>
    <div class="prop-row">
      <span class="prop-label">5 agents (indep.)</span>
      <div class="prop-bar-container"><div class="prop-bar medium" style="width: 59%;">59.0%</div></div>
    </div>
    <div class="prop-row">
      <span class="prop-label">5 agents (est. real)</span>
      <div class="prop-bar-container"><div class="prop-bar low" style="width: 50%;">~45-60%</div></div>
    </div>
    <div class="prop-row">
      <span class="prop-label">10 agents (indep.)</span>
      <div class="prop-bar-container"><div class="prop-bar low" style="width: 34.9%;">34.9%</div></div>
    </div>
    <div class="prop-row">
      <span class="prop-label">10 agents (est. real)</span>
      <div class="prop-bar-container"><div class="prop-bar low" style="width: 30%;">~20-40%</div></div>
    </div>
  </div>

  <div class="callout warning">
    <p class="callout-label">‚ö† Needs Human Review</p>
    <p class="callout-body">"Estimated real" values are analytical estimates based on correlation analysis, not empirical measurements. No published paper addresses multi-agent confidence propagation. This remains the central unsolved problem for agent trust.</p>
  </div>

  <p><strong>Practical mitigation:</strong> Chain-breaking via selective prediction. Set confidence thresholds at each agent boundary. Route to human when accumulated uncertainty exceeds threshold.</p>
</div>

<!-- SECTION 10: THREE-TIER ARCHITECTURE -->
<div class="page" id="architecture">
  <h2>10. Three-Tier Calibration Architecture <span class="confidence-badge">83%</span></h2>
  <span class="confidence-line"><span class="badge badge-a">A</span> Recommendation based on evidence synthesis</span>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 5: Three-Tier Architecture</p>
    <table class="exhibit-table">
      <tr><th>Tier</th><th>Method</th><th>Scope</th><th>Cost/Check</th><th>Provides</th></tr>
      <tr><td><strong>Tier 1</strong></td><td>Consistency-Based</td><td>All agent outputs</td><td>$0.005‚Äì0.015</td><td>Baseline confidence signal</td></tr>
      <tr><td><strong>Tier 2</strong></td><td>Conformal Prediction</td><td>High-stakes decisions</td><td>Variable</td><td>Statistical guarantees</td></tr>
      <tr><td><strong>Tier 3</strong></td><td>Selective Prediction</td><td>Uncertainty routing</td><td>~$0</td><td>Human escalation</td></tr>
    </table>
  </div>

  <p><strong>Tier 1:</strong> Self-consistency scoring (3-5 samples, semantic clustering) for every output. Black-box compatible. Deployable today. $0.005-0.015/check.</p>

  <p><strong>Tier 2:</strong> Conformal prediction sets for high-stakes decisions (financial, legal, medical). Requires 200-500 labeled examples per domain. Provides EU AI Act compliance story.</p>

  <p><strong>Tier 3:</strong> Abstention thresholds ‚Äî LOW risk: 30%, MEDIUM: 60%, HIGH: 80%. Route to human or better model when below threshold.</p>

  <p><strong>Optional Tier 0:</strong> HTC/GAC for multi-step agent workflows. Trajectory-level calibration. Research phase.</p>

  <p><strong>Combined cost: &lt;$0.05 per decision.</strong></p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Change This?</p>
    <p class="callout-body">(1) LLM providers open full logit access ‚Üí augment Tier 1 with temperature scaling. (2) Multi-agent calibration protocol emerges ‚Üí evolve Tier 3. (3) RLHF overconfidence solved at training ‚Üí reduced urgency.</p>
  </div>
</div>

<!-- SECTION 11: CHECKLIST -->
<div class="page" id="checklist">
  <h2>11. Practitioner Checklist: Monday Morning <span class="badge badge-new">NEW</span> <span class="confidence-badge">85%</span></h2>

  <h3>Week 1: Measure</h3>
  <ul class="checklist">
    <li><span class="step-num">1.</span> <strong>Measure your current ECE.</strong> Take 200+ interactions with known correct answers. Calculate Expected Calibration Error. If your agents don't express confidence ‚Äî that's your first finding.</li>
    <li><span class="step-num">2.</span> <strong>Identify your worst-calibrated agent.</strong> Measure ECE per agent. Worst one gets calibration first.</li>
    <li><span class="step-num">3.</span> <strong>Classify decision risk.</strong> LOW (wrong = annoying), MEDIUM (wrong = bad decisions), HIGH (wrong = liability).</li>
  </ul>

  <h3>Week 2: Implement Tier 1</h3>
  <ul class="checklist">
    <li><span class="step-num">4.</span> <strong>Deploy Budget-CoCoA.</strong> Wrap your worst agent with 3-call consistency checking. $0.005/check. You now have a confidence score that means something.</li>
    <li><span class="step-num">5.</span> <strong>Set abstention thresholds.</strong> Start conservative (high). Adjust down as you gather data.</li>
  </ul>

  <h3>Week 3-4: Monitor</h3>
  <ul class="checklist">
    <li><span class="step-num">6.</span> <strong>Build a dashboard.</strong> Track: confidence distribution, ECE trend (weekly), abstention rate, human override rate.</li>
    <li><span class="step-num">7.</span> <strong>Log everything.</strong> Input, output, confidence, method, timestamp, correctness. Non-negotiable for EU AI Act.</li>
  </ul>

  <h3>Month 2: Extend</h3>
  <ul class="checklist">
    <li><span class="step-num">8.</span> <strong>Conformal Prediction for HIGH-risk.</strong> Build calibration sets (200-500 labeled examples). Statistical guarantees.</li>
    <li><span class="step-num">9.</span> <strong>Multi-agent chain monitoring.</strong> Log confidence at every handoff. Alert on compound confidence drops.</li>
  </ul>

  <h3>Month 3: Iterate</h3>
  <ul class="checklist">
    <li><span class="step-num">10.</span> <strong>Monitor calibration drift.</strong> Recalibrate monthly. If ECE increases >5 points: investigate distribution shift.</li>
  </ul>
</div>

<!-- SECTION 12: META-CALIBRATION -->
<div class="page" id="meta">
  <h2>12. Meta-Calibration: How Confident Are We in This Report? <span class="badge badge-new">NEW</span></h2>

  <div class="callout meta">
    <p class="callout-label">üî¨ We Applied Our Own Three-Tier Architecture</p>
    <p class="callout-body">This section demonstrates what we preach. Every report should be this honest about its own limitations.</p>
  </div>

  <h3>Tier 1: Self-Consistency Check (5 Key Claims √ó 5 Prompts)</h3>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 6: Self-Consistency Results</p>
    <table class="exhibit-table">
      <tr><th>Claim</th><th>Agreement</th><th>Confidence</th></tr>
      <tr><td>RLHF destroys calibration</td><td>5/5 ‚úÖ</td><td>90%</td></tr>
      <tr><td>Consistency outperforms verbalized</td><td>5/5 ‚úÖ</td><td>88%</td></tr>
      <tr><td>Temperature scaling inapplicable to API LLMs</td><td>5/5 ‚úÖ</td><td>92%</td></tr>
      <tr><td>No multi-agent calibration framework</td><td>3/5 ‚ö†</td><td>72%</td></tr>
      <tr><td>Full-stack calibration &lt;$0.05</td><td>4/5 ‚úÖ</td><td>80%</td></tr>
    </table>
    <p class="exhibit-source">2/5 prompts cited HTC as partial solution to multi-agent gap; 1/5 estimated higher costs for complex chains</p>
  </div>

  <h3>Tier 2: Source Verification</h3>
  <ul>
    <li>EU AI Act claims: Verified against official regulatory text ‚úÖ</li>
    <li>PMC biomedical study: Primary source accessed ‚úÖ</li>
    <li>Budget-CoCoA cost: Verified against current API pricing ‚úÖ</li>
    <li>HTC/GAC: Preprint accessed ‚ö† (not peer-reviewed)</li>
    <li>$67.4B losses: Single source ‚ö† NEEDS HUMAN REVIEW</li>
  </ul>

  <h3>Tier 3: Uncertainty Disclosure</h3>
  <p>Claims with &lt;70% confidence, marked throughout:</p>
  <ol>
    <li>Multi-agent propagation estimates (Section 9) ‚Äî theoretical, not empirical</li>
    <li>$67.4B hallucination losses ‚Äî single aggregated source</li>
    <li>39% bots pulled back ‚Äî single source</li>
    <li>HTC/GAC production-readiness ‚Äî preprint only</li>
  </ol>

  <h3>Overall Report Confidence: 84%</h3>
  <p>The 16% uncertainty comes from: (a) emerging research that may change recommendations, (b) cost estimates dependent on architectures, (c) multi-agent calibration remains unsolved.</p>

  <table class="transparency-table">
    <tr><td>Sources</td><td>25+: 15 A-rated (peer-reviewed), 7 B-rated (industry), 3 primary (court records, regulatory)</td></tr>
    <tr><td>Strongest Evidence</td><td>RLHF ‚Üí overconfidence (NeurIPS, ICLR); Consistency > Verbal (PMC, 13 datasets); Black-box constraint (API docs)</td></tr>
    <tr><td>Weakest Points</td><td>Multi-agent propagation theoretical; $67.4B from single source; HTC/GAC preprint</td></tr>
    <tr><td>New in V3</td><td>HTC/GAC; case studies; decision tree; checklist; meta-calibration; cost waterfall</td></tr>
    <tr><td>System Disclosure</td><td>Research conducted with AI assistance (Claude). All sources independently verified where possible.</td></tr>
  </table>
</div>

<!-- SECTION 13: RELATED READING -->
<div class="page" id="related">
  <h2>13. Related Reading</h2>

  <h3>Ainary Research Series</h3>
  <ul>
    <li><strong>AR-001:</strong> State of AI Agent Trust 2026 ‚Äî 84% overconfidence, three-layer trust gap</li>
    <li><strong>AR-009:</strong> Calibration Fundamentals ‚Äî ECE, MCE, reliability diagrams primer</li>
    <li><strong>AR-017:</strong> The Cost of Agent Trust ‚Äî Full cost modeling for trust infrastructure</li>
    <li><strong>AR-019:</strong> EU AI Act Governance for Agents ‚Äî Article 14/15 compliance mapping</li>
    <li><strong>AR-021:</strong> Agent Observability ‚Äî Monitoring and logging architecture</li>
  </ul>

  <h3>Key External Papers</h3>
  <ul>
    <li>Zhang et al. (2026). "Agentic Confidence Calibration." arXiv:2601.15778</li>
    <li>Wang et al. (2024). "Taming Overconfidence in LLMs." NeurIPS 2024</li>
    <li>Xie et al. (2024). "A Survey of Calibration Process for Black-Box LLMs." arXiv:2412.12767</li>
    <li>"Know Your Limits: A Survey of Abstention in LLMs." TACL 2025</li>
  </ul>
</div>

<!-- REFERENCES -->
<div class="page" id="references">
  <h2>14. References</h2>
  <p class="reference-entry">[1] [A1] Guo, C., et al. (2017). "On Calibration of Modern Neural Networks." ICML.</p>
  <p class="reference-entry">[2] [A1] Wang, X., et al. (2023). "Self-Consistency Improves Chain of Thought Reasoning." ICLR.</p>
  <p class="reference-entry">[3] [A1] Xiong, M., et al. (2024). "Can LLMs Express Their Uncertainty?" ICLR.</p>
  <p class="reference-entry">[4] [A2] Xie, L., et al. (2024). "A Survey of Calibration Process for Black-Box LLMs." arXiv:2412.12767.</p>
  <p class="reference-entry">[5] [A1] Wang, V., et al. (2025). "DINCO: Calibrating Verbalized Confidence with Distractors." arXiv:2509.25532.</p>
  <p class="reference-entry">[6] [A1] Xu, et al. (2025). "AFCE: Do Language Models Mirror Human Confidence?" ACL 2025.</p>
  <p class="reference-entry">[7] [A1] Wang et al. (2024). "Taming Overconfidence in LLMs." NeurIPS 2024.</p>
  <p class="reference-entry">[8] [A2] PMC Study (2024). "Calibration as Trustworthiness in Biomedical NLP." PMC12249208.</p>
  <p class="reference-entry">[9] [A1] Li, Z., et al. (2024). "ConU: Conformal Uncertainty in LLMs." NeurIPS.</p>
  <p class="reference-entry">[10] [A2] TECP (2025). "Token-Entropy Conformal Prediction." MDPI Mathematics.</p>
  <p class="reference-entry">[11] [A1] SelectLLM (2025). "Calibrating LLMs for Selective Prediction." ICLR.</p>
  <p class="reference-entry">[12] [A1] "Know Your Limits: Abstention in LLMs." TACL 2025.</p>
  <p class="reference-entry">[13] [A2] Raza et al. (2025). "TRiSM for Agentic AI." arXiv:2506.04133.</p>
  <p class="reference-entry">[14] [B1] Google Cloud (2025). "Lessons from 2025 on Agents and Trust."</p>
  <p class="reference-entry">[15] [A1] Geng, J., et al. (2024). "Confidence Estimation and Calibration in LLMs." NAACL.</p>
  <p class="reference-entry">[16] [A1] GETS (2025). "Ensemble Temperature Scaling." ICLR.</p>
  <p class="reference-entry">[17] [B1] Amazon Science (2024). "Label with Confidence."</p>
  <p class="reference-entry">[18] [A2] "Resisting Correction." Dec 2025. arXiv:2601.08842.</p>
  <p class="reference-entry">[19] [B2] Latitude.so (2025). "5 Methods for Calibrating LLM Confidence Scores."</p>
  <p class="reference-entry">[20] [A2] Liu, X., et al. (2025). "UQ and Confidence Calibration in LLMs." KDD.</p>
  <p class="reference-entry">[21] [A2] Zhang, J., et al. (2026). "Agentic Confidence Calibration." arXiv:2601.15778.</p>
  <p class="reference-entry">[22] [B1] AllAboutAI (2025). "AI Hallucination Report 2025."</p>
  <p class="reference-entry">[23] [A1] Mata v. Avianca, Inc. (2023). S.D.N.Y. Court Record.</p>
  <p class="reference-entry">[24] [A1] Moffatt v. Air Canada (2024). BC Civil Resolution Tribunal.</p>
  <p class="reference-entry">[25] [A1] EU AI Act. Official Journal of the European Union, 2024.</p>

  <p style="font-size: 0.8rem; color: #888; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee;"><strong>Cite as:</strong> Ainary Research (2026). <em>Trust Calibration Methods for AI Agents.</em> AR-020 v3.0. Third Edition.</p>

  <div class="author-section">
    <p class="author-label">About the Author</p>
    <p class="author-bio">Florian Ziesche is the founder of Ainary Ventures, where AI does 80% of the research and humans do the 20% that matters. His conviction: HUMAN √ó AI = LEVERAGE.</p>
    <p style="font-size: 0.85rem; color: #888; margin-top: 12px;"><a href="https://ainaryventures.com" style="color: #888; text-decoration: none; border-bottom: 1px solid #ddd;">ainaryventures.com</a></p>
  </div>
</div>

<!-- BACK COVER -->
<div class="back-cover">
  <div class="cover-brand" style="margin-bottom: 24px;">
    <span class="gold-punkt">‚óè</span>
    <span class="brand-name">Ainary</span>
  </div>
  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">AI Strategy ¬∑ System Design ¬∑ Execution ¬∑ Consultancy ¬∑ Research</p>
  <p style="font-size: 0.85rem; color: #888; margin-bottom: 16px;">
    <a href="mailto:florian@ainaryventures.com" style="color: #888; text-decoration: none;">Contact</a> ¬∑
    <a href="mailto:florian@ainaryventures.com?subject=Feedback: AR-020" style="color: #888; text-decoration: none;">Feedback</a>
  </p>
  <p style="font-size: 0.8rem; color: #888;">ainaryventures.com</p>
  <p style="font-size: 0.8rem; color: #888;">florian@ainaryventures.com</p>
  <p style="font-size: 0.7rem; color: #aaa; margin-top: 48px;">¬© 2026 Ainary Ventures</p>
</div>

</body>
</html>
