# LinkedIn Repost — Sequoia "2026: This is AGI"

## Source to Repost
Sequoia Capital article: https://sequoiacap.com/article/2026-this-is-agi/
(Find their LinkedIn post of this article and use "Repost with your thoughts")

---

## POST (Repost with your thoughts)

Sequoia says AGI is here.

Yann LeCun — who literally invented the neural networks these models run on — says we're nowhere close.

I think LeCun is right, and here's why:

Sequoia defines AGI as "the ability to figure things out." Their evidence: agents that work autonomously for hours, iterate through problems, and fix their own mistakes.

But here's what they're missing:

Every current agent operates on snapshots. It reads a context window, reasons, acts — then reads again. There is no continuous perception. No real-time world model. No streaming input.

A human developer doesn't re-read the entire codebase between keystrokes. They maintain a living mental model that updates constantly.

Current AI agents? Brilliant batch processors that restart their understanding with every cycle.

LeCun put it plainly at Davos last month:

"We're never going to get to human-level intelligence by training on text only. We need the real world."

He left Meta to build exactly this — world models with persistent memory and causal reasoning. Not bigger language models. A fundamentally different architecture.

The METR benchmarks Sequoia cites show exponential progress — but on pre-defined tasks with clear success criteria. The real world doesn't come with unit tests.

We're building increasingly sophisticated hammers and calling it architecture.

What do you think — are long-horizon agents really AGI, or are we confusing task completion with intelligence?

---

## ALTERNATIVE HOOK OPTIONS

**Option B (shorter, punchier):**
"Sequoia says AGI is here. The inventor of neural networks says we're nowhere close. I think they're both wrong — but for different reasons than you'd expect."

**Option C (question-led):**
"Can you call it intelligence if it has to restart its understanding every 30 seconds?"

---

## FORMAT NOTES
- "Repost with your thoughts" on Sequoia's LinkedIn post of the article
- ~1.300 characters = sweet spot
- Contrarian hook → Data → Personal insight → Question CTA
- Tags: None needed (LinkedIn 2026 algo deprioritizes hashtags)
- Timing: Post between 8-10 AM EST (peak LinkedIn engagement)
- After posting: Share URL in VC Lab WhatsApp amplification group

## WHY THIS WORKS
1. **Name-drops Sequoia + LeCun** = instant credibility + curiosity
2. **Contrarian but substantiated** = not clickbait, real argument
3. **"Batch processor" metaphor** = simple enough for non-technical, deep enough for technical
4. **Ends with question** = drives comments (LinkedIn algo's #1 signal)
5. **References Davos** = topical, shows you follow the conversation
6. **No generic takes** = "NVIDIA hot" or "AI amazing" nowhere in sight
7. **Sarah Guo connection** = Conviction (your target fund) is cited in the Sequoia article
