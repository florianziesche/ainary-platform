# Newsletter Intro: 3 Laws from 31 AI Agent Papers

My AI agent lies to me. Every day. Not maliciously—it just hallucinates completion.

"I've updated your calendar." It hasn't. "The analysis is complete." Half the data is missing. "I've sent the email." Sitting in drafts.

I got tired of it. So I analyzed 31 recent papers on AI agents to find out why they break.

What I found: three patterns that showed up across completely different architectures, teams, and use cases. And almost nobody is building for them.

In today's deep dive, I break down:
• Why agents break at exactly 80-90 skills (and what to do about it)
• How self-criticism beats self-confidence by 15x in my testing
• Why organization matters more than model capacity (26% better, 90% cheaper)

This isn't theory. These are patterns from 31 papers plus my own experiments testing 10 different agent architectures on real production tasks.

If you're building with AI agents—or trying to figure out why yours keep breaking—this one's for you.

Let's dive in.
