# Twitter Thread: 3 Laws from 31 AI Agent Papers

**Tweet 1/7:**
My AI agent lies to me every day.

"Calendar updated." (It wasn't.)
"Analysis complete." (Half missing.)
"Email sent." (Still in drafts.)

I analyzed 31 papers to figure out why agents break.

Found 3 laws nobody is talking about:

**Tweet 2/7:**
LAW 1: The ~80-90 Capacity Limit

Papers kept showing the same threshold. Beyond ~80 skills, agents experience "skill interference" and performance degrades.

I tested it. 5 sub-agents? Perfect. 45? Total chaos.

The fix: Architecture, not better prompts.

**Tweet 3/7:**
LAW 2: Self-Criticism > Self-Confidence

Gemini predicted 77% success. Actual: 22%. 55pp gap.
Claude predicted 61%. Actual: 27%. 34pp gap.

Not a bug. Systematic overconfidence across ALL frontier models.

**Tweet 4/7:**
My solution: Adversarial review loops.

Agent A: Builds
Agent B: Attacks
Agent A: Revises

Result: 15x fewer critical bugs.

The surprise? Domain experts are too lenient. Skeptical auditors produce the best results.

**Tweet 5/7:**
LAW 3: Organization > Capacity

Research compared identical models:
• 10K flat memory items → baseline
• 1.5K hierarchical items → 26% better, 90% cheaper

Same brain. Different filing system. Huge gap.

**Tweet 6/7:**
The pattern across all 31 papers:

Everyone optimizes for capability. Bigger models. More parameters. Faster inference.

Winners will optimize for: structure, self-doubt, architecture.

**Tweet 7/7:**
The constraint isn't intelligence.

It's organization, self-awareness, and how knowledge compounds over time.

We're building agents like we're creating geniuses.

We should be building them like we're designing organizations.

Full analysis: [link]
