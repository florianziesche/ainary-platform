1/ I ran 10 groups of AI agents. Same question, 10 completely different thinking strategies. 33,000 words of analysis. Six insights appeared in nearly all of them. This is what converged.

2/ Law 1: Files = Intelligence (10/10 groups). AI doesn't improve by getting smarter. It improves by getting better-informed. Your AI's intelligence lives in a folder on your hard drive, not a data center. Better files > bigger models.

3/ Law 2: The Pair is the Unit (9/10 groups). You can't optimize AI in isolation. Human changes in response to agent. Agent changes in response to human. They co-evolve. AI alignment isn't just safety. It's a relationship problem.

4/ Law 3: Multi-Timescale Loops (8/10 groups). Need feedback at every timescale: per-interaction (seconds), per-session (hours), weekly, monthly, quarterly. Most AI has one loop: the chat. Everything above that is lost.

5/ Law 4: Legibility > Optimization (8/10 groups). Transparency beats performance. An agent you can see through is more valuable than one that performs better but opaquely. If you're building AI tools, the most important feature isn't accuracy. It's showing your work.

6/ Law 5: Failures = Signal (8/10 groups). Corrections contain more information than successes. "Perfect" tells you nothing. "No, I meant X" tells you exactly where the gap is. One group called this Kintsugi: repair broken pottery with gold. Make errors visible.

7/ Law 6: Specificity Engine (7/10 groups). Agent improves by getting MORE specific to this human, not more generally capable. After 6 months learning one person's patterns, irreplaceable. Industry optimizes for generality. Value moves opposite.

8/ Cost: few dollars in API calls. Time: one afternoon. Output: 33,000 words that would take a research team weeks. The 10 thinking strategies aren't competing. They're a toolkit. Use first principles for new domains. Use inversion when stuck. Use adversarial when beliefs accumulate.