<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Calibration Gap — Florian Ziesche</title>
    <style>
        @font-face {
            font-family: 'Inter';
            src: url('/fonts/inter-variable.woff2') format('woff2');
            font-weight: 100 600;
            font-display: swap;
        }
        :root {
            --bg: #fafaf8;
            --text: #1a1a1a;
            --text-secondary: #555;
            --text-muted: #888;
            --gold: #c8aa50;
            --border: #e5e3dc;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Inter', system-ui, -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.75;
            font-size: 16px;
            -webkit-font-smoothing: antialiased;
        }
        .wrapper { max-width: 900px; margin: 0 auto; padding: 0 2rem; }

        /* Cover */
        .cover {
            text-align: center;
            padding: 5rem 2rem 4rem;
            border-bottom: 1px solid var(--border);
        }
        .cover-label {
            font-size: 0.75rem;
            font-weight: 600;
            letter-spacing: 3px;
            text-transform: uppercase;
            color: var(--text-muted);
            margin-bottom: 1.5rem;
        }
        .cover h1 {
            font-size: 2.4rem;
            font-weight: 600;
            line-height: 1.15;
            letter-spacing: -0.02em;
            margin-bottom: 1rem;
        }
        .cover .subtitle {
            font-size: 1.05rem;
            color: var(--text-secondary);
            max-width: 680px;
            margin: 0 auto 2rem;
            font-style: italic;
        }
        .cover .author {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        .cover .author strong { color: var(--text); font-weight: 500; }
        .cover .date {
            font-size: 0.8rem;
            color: var(--text-muted);
            margin-top: 0.3rem;
        }
        .cover .confidence {
            font-size: 0.8rem;
            color: var(--text-muted);
            margin-top: 0.5rem;
        }

        /* TOC */
        .toc {
            padding: 2.5rem 0;
            border-bottom: 1px solid var(--border);
        }
        .toc h2 {
            font-size: 0.75rem;
            font-weight: 600;
            letter-spacing: 2px;
            text-transform: uppercase;
            color: var(--text-muted);
            margin-bottom: 1rem;
        }
        .toc ol {
            list-style: none;
            counter-reset: toc;
        }
        .toc ol li {
            counter-increment: toc;
            margin-bottom: 0.4rem;
        }
        .toc ol li a {
            color: var(--text);
            text-decoration: none;
            font-size: 0.95rem;
            display: flex;
            align-items: center;
            gap: 0.6rem;
            transition: color 0.15s;
        }
        .toc ol li a:hover { color: var(--gold); }
        .toc ol li a::before {
            content: counter(toc, decimal);
            font-size: 0.75rem;
            font-weight: 600;
            color: var(--text-muted);
            min-width: 1.5rem;
        }

        /* Sections */
        .section {
            padding: 3rem 0;
            border-bottom: 1px solid var(--border);
        }
        .section h2 {
            font-size: 1.4rem;
            font-weight: 600;
            line-height: 1.3;
            margin-bottom: 1.5rem;
        }
        .confidence-inline {
            font-size: 0.8rem;
            color: var(--text-muted);
            font-weight: 400;
            margin-left: 0.5rem;
        }
        .section h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 2rem 0 0.75rem;
        }
        .section p { margin-bottom: 1rem; }
        .section strong { font-weight: 600; }

        /* So What + Invalidate - KEINE Boxes, nur italic+eingerückt */
        .so-what, .invalidate {
            margin: 1.5rem 0 1.5rem 1.5rem;
            font-style: italic;
            color: var(--text-secondary);
            line-height: 1.6;
        }

        /* Key statement */
        .key-statement {
            font-weight: 600;
            font-size: 1.05rem;
            margin-bottom: 1rem;
            line-height: 1.5;
        }

        /* Footnote refs */
        .fn {
            color: var(--text-muted);
            font-size: 0.7em;
            vertical-align: super;
            text-decoration: none;
            line-height: 0;
        }
        .fn:hover { color: var(--gold); }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.88rem;
        }
        th {
            text-align: left;
            font-weight: 600;
            padding: 0.6rem 0.8rem;
            border-bottom: 2px solid var(--text);
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-secondary);
        }
        td {
            padding: 0.6rem 0.8rem;
            border-bottom: 1px solid var(--border);
            vertical-align: top;
        }
        tr:last-child td { border-bottom: none; }
        .exhibit-caption {
            font-weight: 600;
            font-size: 0.85rem;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            color: var(--text-secondary);
        }
        .exhibit-source {
            font-size: 0.8rem;
            color: var(--text-muted);
            font-style: italic;
            margin-top: 0.3rem;
        }

        /* Lists */
        ul, ol { padding-left: 1.5rem; margin: 0.75rem 0; }
        li { margin-bottom: 0.3rem; }

        /* Links */
        a { color: var(--text); }

        /* Keywords */
        .keywords {
            font-size: 0.85rem;
            color: var(--text-muted);
            margin-top: 1rem;
        }

        /* CTA Footer */
        .cta-footer {
            padding: 3rem 0;
            text-align: center;
            border-bottom: 1px solid var(--border);
        }
        .cta-footer h2 {
            font-size: 1.2rem;
            font-weight: 600;
            margin-bottom: 0.5rem;
        }
        .cta-footer h2 a {
            color: var(--gold);
            text-decoration: none;
        }
        .cta-footer h2 a:hover {
            text-decoration: underline;
        }
        .cta-footer .services {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin: 1rem 0;
        }
        .cta-footer .contact {
            font-size: 0.9rem;
            color: var(--text-muted);
            margin: 1rem 0;
        }
        .cta-footer .contact a {
            color: var(--gold);
            text-decoration: none;
        }
        .cta-footer .contact a:hover {
            text-decoration: underline;
        }
        .cta-footer .tagline {
            font-size: 0.85rem;
            color: var(--text-muted);
            margin-top: 1.5rem;
            font-weight: 600;
            letter-spacing: 2px;
        }

        /* Footer */
        .footer {
            padding: 2rem 0;
            text-align: center;
            font-size: 0.8rem;
            color: var(--text-muted);
            position: relative;
        }
        .footer::after {
            content: '●';
            position: absolute;
            bottom: 2rem;
            right: 2rem;
            color: var(--gold);
            font-size: 8px;
        }

        /* Print */
        @media print {
            body { font-size: 10.5pt; background: #fff; color: #000; }
            .wrapper { max-width: 100%; padding: 0; }
            .cover { padding: 3rem 0 2rem; page-break-after: always; }
            .section { break-inside: avoid; }
            .toc { page-break-after: always; }
            a { color: #000; }
            .fn { color: #666; }
            .so-what, .invalidate { color: #333; }
            .footer::after { display: none; }
            @page { 
                margin: 2cm; 
                @top-left { content: "Ainary Report | The Calibration Gap"; font-size: 8pt; color: #888; }
                @bottom-left { content: "© 2026 Ainary Ventures"; font-size: 8pt; color: #888; }
                @bottom-center { content: "Page " counter(page); font-size: 8pt; color: #888; }
                @bottom-right { content: '●'; font-size: 8pt; color: #c8aa50; }
            }
        }
    </style>
</head>
<body>

<div class="wrapper">

    <!-- COVER -->
    <div class="cover">
        <div class="cover-label">Ainary Research Report No. AR-009</div>
        <h1>The Calibration Gap</h1>
        <p class="subtitle">Why 84% of AI Agents Are Overconfident and What It Costs</p>
        <p class="author"><strong>Florian Ziesche</strong> — Ainary Ventures</p>
        <p class="date">February 2026</p>
        <p class="confidence">Overall Confidence: 72%</p>
    </div>

    <!-- TOC -->
    <nav class="toc">
        <h2>Contents</h2>
        <ol>
            <li><a href="#exec-summary">Executive Summary</a></li>
            <li><a href="#methodology">Methodology</a></li>
            <li><a href="#s3">What Calibration Means and How to Measure It</a></li>
            <li><a href="#s4">The Overconfidence Pandemic</a></li>
            <li><a href="#s5">Multi-Agent Amplification</a></li>
            <li><a href="#s6">What Overconfidence Costs</a></li>
            <li><a href="#s7">Calibration Methods That Actually Work</a></li>
            <li><a href="#s8">The Human Factor</a></li>
            <li><a href="#s9">Recommendations</a></li>
            <li><a href="#beipackzettel">Beipackzettel</a></li>
            <li><a href="#claim-register">Claim Register</a></li>
            <li><a href="#references">References</a></li>
        </ol>
    </nav>

    <!-- EXECUTIVE SUMMARY -->
    <div class="section" id="exec-summary">
        <h2>1. Executive Summary</h2>

        <p class="key-statement">AI agents are systematically overconfident — and enterprise stacks are not designed to catch it.</p>

        <ul>
            <li>84% of LLM responses show confidence exceeding actual accuracy across 9 models and 351 scenarios<a class="fn" href="#ref1">[1]</a></li>
            <li>Verbalized confidence is biased upward by 20–30 percentage points and poorly correlated with correctness<a class="fn" href="#ref2">[2]</a><a class="fn" href="#ref3">[3]</a></li>
            <li>Multi-agent verification amplifies miscalibration instead of correcting it — identically biased validators create false consensus<a class="fn" href="#ref4">[4]</a></li>
            <li>Alert fatigue from overconfident systems causes 67% of security alerts to be ignored<a class="fn" href="#ref5">[5]</a></li>
            <li>A calibration check costs $0.005; not calibrating has cost up to $7.5B in a single case<a class="fn" href="#ref6">[6]</a><a class="fn" href="#ref7">[7]</a></li>
        </ul>

        <p class="keywords"><strong>Keywords:</strong> AI calibration, overconfidence, Expected Calibration Error, multi-agent systems, conformal prediction, trust erosion, RLHF</p>
    </div>

    <!-- METHODOLOGY -->
    <div class="section" id="methodology">
        <h2>2. Methodology</h2>

        <p>This report synthesizes 12 sources: 8 peer-reviewed or preprint papers and 4 industry reports. The research pipeline followed a structured sequence: targeted literature review, source cross-referencing, gap analysis, and claim verification.</p>

        <p>Each claim carries an explicit confidence level (High, Medium-High, or Medium) based on source quality and replicability. Claims are registered in the Claim Register at the end of this report with their evidence basis and invalidation conditions.</p>

        <p><strong>Limitations I want to be transparent about:</strong></p>

        <ul>
            <li>The headline 84% figure comes from clinical decision scenarios<a class="fn" href="#ref1">[1]</a>. I use it as directional evidence for broader LLM behavior, but domain-specific replication is pending.</li>
            <li>Multi-agent amplification effects (Section 5) are modeled theoretically. No empirical study directly measures compound miscalibration in agent chains.</li>
            <li>Cost extrapolations from SOC and healthcare alert fatigue to AI agent contexts are analogical, not direct.</li>
        </ul>

        <p>This report was created with a multi-agent research system. Every number has a source. Where evidence ends and interpretation begins, I say so.</p>

        <p class="confidence-inline">(Confidence: High for methodology transparency; Medium for cross-domain generalizability)</p>
    </div>

    <!-- SECTION 3 -->
    <div class="section" id="s3">
        <h2>3. What Calibration Means and How to Measure It <span class="confidence-inline">(Confidence: High)</span></h2>

        <p class="key-statement">Calibration is not accuracy — it is honesty about uncertainty.</p>

        <p>A model can be 80% accurate and still dangerously miscalibrated. If that same model claims 95% confidence on every prediction, the gap between stated certainty and actual performance is the calibration error. This gap is what kills trust, wastes resources, and ultimately causes human operators to stop listening.</p>

        <p>The standard metric is Expected Calibration Error (ECE). It works by binning predictions by confidence level, then comparing average confidence to average accuracy per bin. A perfectly calibrated model shows a diagonal line: when it says "90% sure," it is right 90% of the time<a class="fn" href="#ref8">[8]</a>.</p>

        <p>The Brier Score offers a complementary view. It computes the mean squared error between predicted probability and binary outcome, decomposing into calibration, resolution, and uncertainty. Lower is better. Its advantage over ECE: no binning artifacts<a class="fn" href="#ref8">[8]</a>.</p>

        <p>The Overconfidence Ratio (OCR) measures the percentage of predictions where confidence exceeds accuracy. This is the metric behind the 84% headline figure<a class="fn" href="#ref1">[1]</a>.</p>

        <p>There is a critical distinction that most practitioners miss: token-level calibration versus verbalized calibration. Pre-trained base models are reasonably well-calibrated at the token probability level. But instruction tuning and RLHF — the processes that make models useful for conversation — destroy this calibration<a class="fn" href="#ref9">[9]</a>. The models humans actually interact with are the miscalibrated ones.</p>

        <p>When you ask GPT-4 or Claude "how confident are you, 0–100%?", the numbers cluster around round figures (70%, 80%, 90%, 95%) rather than distributing smoothly<a class="fn" href="#ref10">[10]</a>. They correlate with correctness at roughly r ≈ 0.3–0.5<a class="fn" href="#ref2">[2]</a> — better than random, but far worse than the precision they imply.</p>

        <p class="exhibit-caption">Exhibit 1: Calibration Curve — Perfect vs. Typical LLM</p>
        <table>
            <thead>
                <tr>
                    <th>Stated Confidence</th>
                    <th>Perfect Model (Accuracy)</th>
                    <th>Typical LLM (Accuracy)</th>
                    <th>Gap</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>50%</td><td>50%</td><td>45%</td><td>-5pp</td></tr>
                <tr><td>60%</td><td>60%</td><td>48%</td><td>-12pp</td></tr>
                <tr><td>70%</td><td>70%</td><td>52%</td><td>-18pp</td></tr>
                <tr><td>80%</td><td>80%</td><td>58%</td><td>-22pp</td></tr>
                <tr><td>90%</td><td>90%</td><td>65%</td><td>-25pp</td></tr>
                <tr><td>95%</td><td>95%</td><td>70%</td><td>-25pp</td></tr>
            </tbody>
        </table>
        <p class="exhibit-source">Source: Directional illustration based on Tian et al. (2023) [3] and Xiong et al. (2024) [10]. Exact values vary by model and domain. Not empirical measurements of a single model.</p>

        <p><strong>Evidence:</strong> ECE and Brier Score are established metrics with decades of use in weather forecasting and medicine<a class="fn" href="#ref8">[8]</a>. Token-level calibration degradation through RLHF is documented by Kadavath et al.<a class="fn" href="#ref9">[9]</a></p>

        <p><strong>Interpretation:</strong> My take: The implication for practitioners is that verbalized confidence — the kind most agent frameworks use — is the least reliable signal available. Yet it is the one most commonly surfaced to end users.</p>

        <div class="so-what">
            <strong>So What?</strong><br>
            If you are building an agent system that surfaces confidence scores to users, those scores are likely 20–30 percentage points too high. Every decision made downstream of that inflated number carries hidden risk.
        </div>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> A large-scale study showing verbalized confidence from instruction-tuned models is well-calibrated (r > 0.8 with accuracy) across domains.
        </div>
    </div>

    <!-- SECTION 4 -->
    <div class="section" id="s4">
        <h2>4. The Overconfidence Pandemic <span class="confidence-inline">(Confidence: High)</span></h2>

        <p class="key-statement">Every major LLM family is overconfident at the verbalized level — this is a training artifact, not a bug to patch.</p>

        <p>The data is unambiguous. A 2024 peer-reviewed study tested 9 different LLMs across 351 clinical decision scenarios<a class="fn" href="#ref1">[1]</a>. In 84% of those scenarios, the model's expressed confidence exceeded its actual accuracy. This was not model-specific or prompt-dependent. It appeared systematically across model families and sizes.</p>

        <p>Separate research confirms the pattern. Tian et al. (2023) found verbalized confidence is biased upward by 20–30 percentage points compared to actual accuracy<a class="fn" href="#ref3">[3]</a>. Xiong et al. (2024) documented the same clustering around high round numbers and the same systematic overestimation<a class="fn" href="#ref10">[10]</a>. The most comprehensive assessment came in January 2026: arXiv:2602.00279 concluded that verbalized confidence expressions are "systematically biased and poorly correlated with correctness"<a class="fn" href="#ref2">[2]</a>.</p>

        <p>This is not a prompt engineering problem. It is a training problem.</p>

        <p>The root cause sits in RLHF — Reinforcement Learning from Human Feedback. When human raters evaluate model outputs, confident-sounding answers score higher. Hedging gets penalized. "The answer is X" beats "The answer might be X, but I'm not sure." Over millions of training iterations, models learn a simple lesson: confidence gets rewarded<a class="fn" href="#ref9">[9]</a>.</p>

        <p>Instruction tuning adds a second layer. The objective "be helpful" trains models to commit to answers rather than express uncertainty. "I don't know" is effectively trained out of the model's repertoire. And sycophancy — the tendency to agree with user premises even when wrong — provides a third compounding force. The model agrees confidently with whatever frame the user presents<a class="fn" href="#ref9">[9]</a>.</p>

        <p class="exhibit-caption">Exhibit 2: Root Causes of LLM Overconfidence</p>
        <table>
            <thead>
                <tr>
                    <th>Mechanism</th>
                    <th>How It Creates Overconfidence</th>
                    <th>Reversible?</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>RLHF reward signal</td>
                    <td>Confident answers score higher with human raters</td>
                    <td>Requires new training objective</td>
                </tr>
                <tr>
                    <td>Instruction tuning</td>
                    <td>"Be helpful" = commit to answers, don't hedge</td>
                    <td>Requires objective redesign</td>
                </tr>
                <tr>
                    <td>Sycophancy</td>
                    <td>Agree with user premise, express confidence in their framing</td>
                    <td>Active research area</td>
                </tr>
                <tr>
                    <td>No calibration loss</td>
                    <td>Unlike weather models, no training signal rewards accurate confidence</td>
                    <td>Could be added; not standard</td>
                </tr>
            </tbody>
        </table>
        <p class="exhibit-source">Source: Kadavath et al. (2022) [9], training dynamics literature.</p>

        <p><strong>A positive counterexample:</strong> Base models before instruction tuning show reasonable token-level calibration<a class="fn" href="#ref9">[9]</a>. This proves that calibration is not impossible for neural networks — it is specifically destroyed by the post-training process designed to make models conversational. This means the problem is solvable. It requires changing what we optimize for, not changing the architecture.</p>

        <p><strong>Evidence:</strong> The 84% figure<a class="fn" href="#ref1">[1]</a>, 20–30pp bias<a class="fn" href="#ref3">[3]</a>, and poor correlation with correctness<a class="fn" href="#ref2">[2]</a> are independently documented across multiple research groups.</p>

        <p><strong>Interpretation:</strong> I read this as a structural market failure. The training pipeline optimizes for user satisfaction (which rewards confidence), not for calibration (which rewards honesty). Until calibration becomes an explicit training objective — or an external calibration layer is added — every instruction-tuned model will be overconfident by default.</p>

        <div class="so-what">
            <strong>So What?</strong><br>
            Overconfidence is not a model defect. It is an emergent property of how we train models to be useful. Expecting prompt engineering to fix a training-level problem is like expecting better tires to fix a misaligned engine.
        </div>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> An RLHF variant that preserves calibration through the instruction-tuning process. If a major lab ships a model with ECE < 0.05 after RLHF, the structural argument weakens significantly.
        </div>
    </div>

    <!-- SECTION 5 -->
    <div class="section" id="s5">
        <h2>5. Multi-Agent Amplification <span class="confidence-inline">(Confidence: Medium)</span></h2>

        <p class="key-statement">Adding agents to verify agents makes calibration worse, not better — unless the verification method is fundamentally different from "ask another model."</p>

        <p>The intuition behind multi-agent systems is seductive: if one agent might be wrong, have another check its work. A "second opinion" should improve reliability. In medicine, law, and engineering, peer review catches errors. Why wouldn't the same logic apply to AI agents?</p>

        <p>Because AI agents share the same systematic biases.</p>

        <p>When Agent A (overconfident) passes its output to Agent B (also overconfident) for verification, three compounding effects occur:</p>

        <ol>
            <li><strong>Sycophancy:</strong> Agent B's prior is biased toward agreement with the input it receives. It is more likely to confirm than challenge.</li>
            <li><strong>Anchoring:</strong> Agent B sees Agent A's high confidence as evidence. A claim presented with "95% confidence" is harder to reject than one presented with "I'm not sure."</li>
            <li><strong>Compounding:</strong> Agent B reports even higher confidence on the now-"validated" result.</li>
        </ol>

        <p>In a chain of N agents, miscalibration does not cancel out. It compounds. If each agent independently has an overconfidence ratio of 0.84, a 3-agent verification chain where each confirms the previous approaches an effective overconfidence ratio of 1.0<a class="fn" href="#ref4">[4]</a>. The "second opinion" is not a second opinion at all — it is the same bias wearing a different name tag.</p>

        <p>Research on multi-agent system manipulation supports this indirectly. Studies show 45–64% success rates in hijacking multi-agent systems, partly because agents trust each other's outputs without calibration checks<a class="fn" href="#ref4">[4]</a>.</p>

        <p class="exhibit-caption">Exhibit 3: Compound Miscalibration Model</p>
        <table>
            <thead>
                <tr>
                    <th>Agent Chain</th>
                    <th>Overconfidence Ratio (Modeled)</th>
                    <th>False Certainty Level</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1 agent</td><td>0.84</td><td>High</td></tr>
                <tr><td>2 agents (verify)</td><td>0.93</td><td>Very High</td></tr>
                <tr><td>3 agents (verify)</td><td>0.97</td><td>Near-Total</td></tr>
                <tr><td>5 agents (verify)</td><td>0.99</td><td>Effectively 100%</td></tr>
            </tbody>
        </table>
        <p class="exhibit-source">Source: Theoretical model based on independent overconfidence assumption. Not empirically validated for agent chains — see Gap Analysis. Directional, not precise.</p>

        <p><strong>The exception that proves the rule:</strong> Sample Consistency<a class="fn" href="#ref11">[11]</a> works precisely because it does not ask another model for a "second opinion." Instead, it samples the same model multiple times with temperature > 0 and measures disagreement. High agreement = justified confidence. Low agreement = genuine uncertainty. This is fundamentally different from "Agent B verifies Agent A" because it measures variance, not consensus.</p>

        <p><strong>Evidence:</strong> The compound overconfidence model is theoretical. The individual components (sycophancy, anchoring, overconfidence) are each well-documented<a class="fn" href="#ref1">[1]</a><a class="fn" href="#ref2">[2]</a><a class="fn" href="#ref9">[9]</a>. Multi-agent hijacking success rates are empirical<a class="fn" href="#ref4">[4]</a>.</p>

        <p><strong>Interpretation:</strong> I believe the multi-agent verification paradigm is one of the most dangerous patterns in current AI system design. It feels safe. It looks like due diligence. But it manufactures false certainty at scale. The fix is not more agents — it is different verification methods (Sample Consistency, Conformal Prediction) that measure uncertainty orthogonally.</p>

        <div class="so-what">
            <strong>So What?</strong><br>
            If your agent architecture uses "Agent B checks Agent A" as a reliability mechanism, you likely have a false consensus machine, not a quality assurance system. Redesign for disagreement, not confirmation.
        </div>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> An empirical study showing that multi-agent verification chains with diverse base models actually reduce calibration error. If GPT-4 checking Claude checking Gemini produces well-calibrated outputs, the compound overconfidence argument fails.
        </div>
    </div>

    <!-- SECTION 6 -->
    <div class="section" id="s6">
        <h2>6. What Overconfidence Costs <span class="confidence-inline">(Confidence: High)</span></h2>

        <p class="key-statement">The cost is not wrong answers — it is the erosion of human judgment when humans can no longer distinguish "the AI is actually sure" from "the AI always says it's sure."</p>

        <p>The direct costs are already substantial.</p>

        <p class="exhibit-caption">Exhibit 4: Documented Costs of Miscalibrated Systems</p>
        <table>
            <thead>
                <tr>
                    <th>Case</th>
                    <th>What Happened</th>
                    <th>Cost</th>
                    <th>Calibration Link</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>VW Cariad</td>
                    <td>Software system overcommitted on delivery timelines, cascading failures</td>
                    <td>$7.5B<a class="fn" href="#ref7">[7]</a></td>
                    <td>System confidence in timelines vs. reality</td>
                </tr>
                <tr>
                    <td>Air Canada chatbot</td>
                    <td>Hallucinated refund policy, presented with full confidence</td>
                    <td>~$800 + legal precedent</td>
                    <td>No uncertainty flagging on fabricated information</td>
                </tr>
                <tr>
                    <td>Mata v. Avianca</td>
                    <td>Lawyer filed ChatGPT-fabricated case citations confidently</td>
                    <td>$5K fine + career damage</td>
                    <td>Model presented fake citations with zero hedging</td>
                </tr>
                <tr>
                    <td>Healthcare alerts</td>
                    <td>80–99% false positive rates in clinical alert systems</td>
                    <td>14%+ increase in medical errors from fatigue<a class="fn" href="#ref6">[6]</a></td>
                    <td>Poorly calibrated alert thresholds</td>
                </tr>
                <tr>
                    <td>SOC alert fatigue</td>
                    <td>67% of 4,484 daily security alerts ignored by analysts</td>
                    <td>Unquantified breach exposure<a class="fn" href="#ref5">[5]</a></td>
                    <td>Overconfident threat detection</td>
                </tr>
            </tbody>
        </table>
        <p class="exhibit-source">Sources: PMC6904899 [6], Vectra 2023 [5], VW financial reports [7], public court records.</p>

        <p>But the direct costs are not the real story. The real story is the trust erosion spiral — a five-phase pattern I see repeating across every domain where overconfident automation meets human oversight.</p>

        <p><strong>Phase 1: Overcommitment.</strong> The overconfident agent makes decisions. It states high confidence on every output. Most outputs are correct, so early trust is high.</p>

        <p><strong>Phase 2: Discovery.</strong> Humans notice errors — but the agent said "95% confident" on both the correct and incorrect outputs. The confidence signal becomes meaningless.</p>

        <p><strong>Phase 3: Alert fatigue.</strong> Humans begin ignoring agent outputs because they cannot distinguish real confidence from systematic overconfidence. In SOC environments, 67% of alerts are already ignored<a class="fn" href="#ref5">[5]</a>. In healthcare, 80–99% of clinical alerts are false positives<a class="fn" href="#ref6">[6]</a>.</p>

        <p><strong>Phase 4: Binary choice.</strong> The organization faces a lose-lose decision: abandon the AI system (wasting investment) or remove human oversight (creating unmonitored risk).</p>

        <p><strong>Phase 5: Catastrophe.</strong> An actual critical alert gets ignored because it looks identical to the thousands of false alarms before it.</p>

        <p>This is the Boeing 737 MAX pattern applied to AI. Automation complacency leads to override fatigue leads to disaster. The MCAS system was overconfident in its sensor readings. Pilots were trained to trust automation. When the automation failed, the trust pattern was already set.</p>

        <p>The cost asymmetry is staggering. A Budget-CoCoA calibration check costs $0.005 per decision<a class="fn" href="#ref12">[12]</a><a class="fn" href="#ref7">[7]</a>. At 1,000 checks per day, that is $135 per month. One prevented VW-scale failure pays for 55,555 years of calibration checks. The ratio between fix cost and failure cost is 1:1,500,000.</p>

        <p><strong>Evidence:</strong> Direct costs (VW, Air Canada, Mata v. Avianca) are from public records and financial reports<a class="fn" href="#ref7">[7]</a>. Alert fatigue statistics are from peer-reviewed sources and large-scale industry surveys<a class="fn" href="#ref5">[5]</a><a class="fn" href="#ref6">[6]</a>. The trust erosion spiral is my synthesis — a descriptive model, not an empirical finding.</p>

        <p><strong>Interpretation:</strong> Every enterprise deploying AI agents without a calibration layer is running the trust erosion spiral. The only question is which phase they are in. Most, I estimate, are between Phase 2 and Phase 3 — they have noticed errors but have not yet built the infrastructure to distinguish real confidence from noise.</p>

        <div class="so-what">
            <strong>So What?</strong><br>
            The $0.005 calibration check is not an expense. It is insurance against trust collapse. Organizations spending millions on AI deployment but zero on calibration are building on sand.
        </div>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> Evidence that humans maintain appropriate trust calibration with AI systems even without reliable confidence signals — i.e., that alert fatigue does not develop with overconfident AI. The aviation, medical, and SOC evidence makes this unlikely.
        </div>
    </div>

    <!-- SECTION 7 -->
    <div class="section" id="s7">
        <h2>7. Calibration Methods That Actually Work <span class="confidence-inline">(Confidence: High)</span></h2>

        <p class="key-statement">The best calibration method for production AI agents is Sample Consistency — it is black-box, cheap, and does not require logit access.</p>

        <p>Five methods exist. Only two are practical for most production agent systems today.</p>

        <p class="exhibit-caption">Exhibit 5: Calibration Methods Comparison</p>
        <table>
            <thead>
                <tr>
                    <th>Method</th>
                    <th>How It Works</th>
                    <th>Cost/Check</th>
                    <th>Logit Access Required?</th>
                    <th>Production Ready?</th>
                    <th>Effectiveness</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Temperature Scaling<a class="fn" href="#ref13">[13]</a></td>
                    <td>Single scalar applied to logits post-hoc</td>
                    <td>Near-zero</td>
                    <td>Yes</td>
                    <td>Only self-hosted</td>
                    <td>Gold standard</td>
                </tr>
                <tr>
                    <td>Conformal Prediction<a class="fn" href="#ref14">[14]</a></td>
                    <td>Prediction sets with coverage guarantees</td>
                    <td>Near-zero</td>
                    <td>No</td>
                    <td>Emerging</td>
                    <td>Guaranteed coverage</td>
                </tr>
                <tr>
                    <td>Sample Consistency<a class="fn" href="#ref11">[11]</a></td>
                    <td>N samples, measure agreement</td>
                    <td>~$0.003 (3×)</td>
                    <td>No</td>
                    <td>Yes</td>
                    <td>Strong</td>
                </tr>
                <tr>
                    <td>Hybrid CoCoA<a class="fn" href="#ref12">[12]</a></td>
                    <td>Consistency + verbalized confidence</td>
                    <td>~$0.005 (3×)</td>
                    <td>No</td>
                    <td>Yes</td>
                    <td>State-of-the-art</td>
                </tr>
                <tr>
                    <td>Selective Prediction</td>
                    <td>Train to abstain when uncertain</td>
                    <td>N/A (training)</td>
                    <td>N/A</td>
                    <td>No (requires retraining)</td>
                    <td>Promising</td>
                </tr>
            </tbody>
        </table>
        <p class="exhibit-source">Sources: Guo et al. 2017 [13], Angelopoulos & Bates 2023 [14], Wang et al. 2022 [11], Hobelsberger et al. 2025 [12]. Cost estimates based on Anthropic Haiku pricing as of February 2026.</p>

        <p><strong>Temperature Scaling</strong><a class="fn" href="#ref13">[13]</a> is the gold standard in machine learning. A single scalar parameter, learned on a validation set, adjusts logits before the softmax layer. Simple, effective, no architecture change. But it requires access to logits — which API-based models (GPT-4, Claude, Gemini) do not expose. For the vast majority of production agent systems built on top of APIs, Temperature Scaling is unavailable.</p>

        <p><strong>Conformal Prediction</strong><a class="fn" href="#ref14">[14]</a> takes a radically different approach. Instead of calibrating a point prediction, it produces prediction sets with guaranteed coverage probability. Instead of "the answer is X (95% sure)," it outputs "the answer is in {X, Y, Z} with 90% coverage guarantee." The guarantee is distribution-free and finite-sample — it holds regardless of the model's internal calibration. The limitation: downstream agents need to handle sets, not single answers. For high-stakes decisions (medical diagnosis, legal analysis, security classification), this trade-off is worth making.</p>

        <p><strong>Sample Consistency</strong><a class="fn" href="#ref11">[11]</a> is the practical winner for most use cases. Sample the same model N times with temperature > 0. Measure agreement across samples. High agreement signals justified confidence; low agreement signals genuine uncertainty. It is black-box, works on any model, and requires no logit access. The cost multiplier (N× the base inference cost) is mitigated by using cheap models — three Haiku calls cost roughly $0.003.</p>

        <p><strong>Hybrid CoCoA</strong><a class="fn" href="#ref12">[12]</a> combines Sample Consistency with verbalized confidence, weighting consistency higher because verbalized confidence is biased but not completely useless. It beats all single methods in the benchmarks reported by Hobelsberger et al. (2025). The Budget version using Haiku costs $0.005 per check. This is my current recommendation for production systems.</p>

        <p><strong>Selective Prediction</strong> — training models to say "I don't know" — is promising but requires model-level changes. RLHF actively discourages abstention, making this a training pipeline change, not a deployment fix.</p>

        <p class="exhibit-caption">Exhibit 6: Decision Framework — When to Use Which Method</p>
        <table>
            <thead>
                <tr>
                    <th>Scenario</th>
                    <th>Recommended Method</th>
                    <th>Rationale</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Self-hosted model</td>
                    <td>Temperature Scaling</td>
                    <td>Gold standard, near-zero cost</td>
                </tr>
                <tr>
                    <td>API-based agent, standard decisions</td>
                    <td>Budget-CoCoA</td>
                    <td>Best accuracy/cost ratio</td>
                </tr>
                <tr>
                    <td>High-stakes decisions (medical, legal)</td>
                    <td>Conformal Prediction</td>
                    <td>Coverage guarantees matter more than point estimates</td>
                </tr>
                <tr>
                    <td>Multi-agent orchestration</td>
                    <td>Sample Consistency at each handoff</td>
                    <td>Prevents compound overconfidence</td>
                </tr>
                <tr>
                    <td>Cost-constrained, high-volume</td>
                    <td>Sample Consistency (2× sample)</td>
                    <td>Cheaper than CoCoA, still effective</td>
                </tr>
            </tbody>
        </table>

        <p><strong>Evidence:</strong> Temperature Scaling effectiveness is extensively validated<a class="fn" href="#ref13">[13]</a>. Sample Consistency has strong empirical support<a class="fn" href="#ref11">[11]</a>. CoCoA is a single study<a class="fn" href="#ref12">[12]</a> — strong results, but awaiting replication.</p>

        <p><strong>Interpretation:</strong> The calibration toolbox exists. The methods work. The gap is not technical — it is adoption. I estimate that fewer than 5% of production agent systems implement any form of calibration beyond raw verbalized confidence. This is the equivalent of shipping software without tests in 2026.</p>

        <div class="so-what">
            <strong>So What?</strong><br>
            For $135/month (1,000 checks/day with Budget-CoCoA), you can add a calibration layer to your agent system. The technical barrier is near-zero. The only barrier is knowing this problem exists.
        </div>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If next-generation models ship with well-calibrated verbalized confidence natively (ECE < 0.05 post-RLHF), external calibration layers become unnecessary. I see no evidence this is imminent.
        </div>
    </div>

    <!-- SECTION 8 -->
    <div class="section" id="s8">
        <h2>8. The Human Factor <span class="confidence-inline">(Confidence: Medium-High)</span></h2>

        <p class="key-statement">The market selects for overconfidence — honest AI that says "I'm 60% sure" loses to overconfident AI that says "95% sure," even when the honest AI is more useful.</p>

        <p>This section addresses the demand side of the calibration problem. Even if we solve the technical challenge of producing calibrated confidence, a behavioral economics problem remains: humans prefer confident systems.</p>

        <p><strong>Automation bias</strong> is well-documented across aviation, medicine, and security. Humans defer to automated systems even when their own judgment is better. The effect is stronger when the system expresses high confidence — "95% confident" triggers automation bias more than "60% confident"<a class="fn" href="#ref6">[6]</a>.</p>

        <p><strong>Confidence anchoring</strong> compounds the problem. When an AI system says "95% confident," humans anchor on that number. Even if they learn the system is poorly calibrated, the anchor persists in subsequent interactions.</p>

        <p><strong>Asymmetric trust updating</strong> provides the final piece. Humans update trust upward faster than downward. A few correct, confident predictions build trust that survives many incorrect ones. By the time the errors become undeniable, the trust pattern is deeply established.</p>

        <p>The market consequence: overconfident AI products get adopted. Calibrated AI products get rejected as "uncertain" or "wishy-washy." This creates selection pressure at the product level for overconfidence — not because vendors are dishonest, but because the market rewards the wrong signal.</p>

        <p class="exhibit-caption">Exhibit 7: The Confidence-Adoption Paradox</p>
        <table>
            <thead>
                <tr>
                    <th>AI System A (Calibrated)</th>
                    <th>AI System B (Overconfident)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Says "60% confident" when 60% accurate</td>
                    <td>Says "95% confident" when 60% accurate</td>
                </tr>
                <tr>
                    <td>Users perceive as "uncertain"</td>
                    <td>Users perceive as "reliable"</td>
                </tr>
                <tr>
                    <td>Lower adoption rates</td>
                    <td>Higher adoption rates</td>
                </tr>
                <tr>
                    <td>Fewer trust failures long-term</td>
                    <td>More trust failures long-term</td>
                </tr>
                <tr>
                    <td>Better for the organization</td>
                    <td>Feels better for the buyer</td>
                </tr>
            </tbody>
        </table>

        <p>The implication for product design is clear: calibration must be positioned as a feature, not a limitation. "We tell you when we don't know" is a trust differentiator — but only if buyers understand the alternative is not "a system that always knows" but "a system that always claims to know."</p>

        <p><strong>Evidence:</strong> Automation bias and anchoring are established findings in behavioral economics and human factors research<a class="fn" href="#ref6">[6]</a>. The market selection argument is my interpretation based on these mechanisms — I have not found a controlled experiment specifically testing AI product adoption vs. calibration level.</p>

        <p><strong>Interpretation:</strong> I see this as the hardest problem in the calibration stack. The technical fixes exist (Section 7). The economic incentives point the wrong way. The organizations most likely to implement calibration are the ones that have already experienced a trust failure — which means the learning is reactive, not proactive.</p>

        <div class="so-what">
            <strong>So What?</strong><br>
            If you are building a calibrated AI product, you need a deliberate positioning strategy. "We're honest about uncertainty" must be framed as a premium capability, not a weakness. The buyer who understands calibration is the buyer worth having.
        </div>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> Evidence that enterprise buyers prefer calibrated AI systems over overconfident ones without needing to experience a failure first. If adoption data shows calibrated products winning market share, the pessimistic market dynamics argument collapses.
        </div>
    </div>

    <!-- SECTION 9 -->
    <div class="section" id="s9">
        <h2>9. Recommendations <span class="confidence-inline">(Confidence: High)</span></h2>

        <p class="key-statement">Calibration is not a model problem — it is an infrastructure problem. It belongs in the orchestration layer, not the model layer.</p>

        <p>Based on the evidence in this report, I recommend five concrete actions for any organization deploying AI agents:</p>

        <p><strong>1. Implement Budget-CoCoA at the orchestration level.</strong> Cost: $135/month for 1,000 checks/day. This is the single highest-leverage investment in agent reliability available today<a class="fn" href="#ref12">[12]</a>. It belongs in the middleware, not the model.</p>

        <p><strong>2. Never trust verbalized confidence alone.</strong> Any system that surfaces a model's self-reported confidence to end users without external calibration is misleading those users. The 20–30 percentage point upward bias is systematic<a class="fn" href="#ref3">[3]</a>.</p>

        <p><strong>3. Use Conformal Prediction for high-stakes decisions.</strong> When the cost of error is high (medical, legal, financial), prediction sets with coverage guarantees are superior to point estimates with confidence scores<a class="fn" href="#ref14">[14]</a>. The trade-off — sets instead of single answers — is worth making when the stakes justify it.</p>

        <p><strong>4. Design multi-agent systems for disagreement, not consensus.</strong> The default pattern of "Agent B verifies Agent A" creates false consensus. Replace it with Sample Consistency or architectures that explicitly surface and preserve disagreement<a class="fn" href="#ref11">[11]</a>.</p>

        <p><strong>5. Present calibrated uncertainty as a trust differentiator.</strong> In product positioning, "we tell you when we don't know" is a feature. The market will eventually punish overconfidence when trust failures accumulate. Be positioned on the right side of that correction.</p>

        <p class="exhibit-caption">Exhibit 8: Implementation Priority Matrix</p>
        <table>
            <thead>
                <tr>
                    <th>Action</th>
                    <th>Cost</th>
                    <th>Effort</th>
                    <th>Impact</th>
                    <th>Priority</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Budget-CoCoA integration</td>
                    <td>$135/month</td>
                    <td>1–2 engineering days</td>
                    <td>High — catches most overconfidence</td>
                    <td>Do first</td>
                </tr>
                <tr>
                    <td>Remove raw VCE from user-facing outputs</td>
                    <td>$0</td>
                    <td>1 day</td>
                    <td>High — stops misleading users</td>
                    <td>Do first</td>
                </tr>
                <tr>
                    <td>Conformal Prediction for critical paths</td>
                    <td>Low</td>
                    <td>1 week</td>
                    <td>Very High for high-stakes</td>
                    <td>Do second</td>
                </tr>
                <tr>
                    <td>Redesign multi-agent verification</td>
                    <td>$0</td>
                    <td>1–2 weeks</td>
                    <td>Medium-High</td>
                    <td>Do third</td>
                </tr>
                <tr>
                    <td>Calibration-as-feature positioning</td>
                    <td>$0</td>
                    <td>Ongoing</td>
                    <td>Long-term competitive advantage</td>
                    <td>Start now</td>
                </tr>
            </tbody>
        </table>
    </div>

    <!-- BEIPACKZETTEL -->
    <div class="section" id="beipackzettel">
        <h2>10. Beipackzettel</h2>

        <p><strong>Overall Confidence:</strong> 72%</p>

        <p><strong>Sources:</strong> 14 total — 8 peer-reviewed or preprint papers, 4 industry reports, 2 technical references</p>

        <p><strong>Strongest evidence:</strong> The 84% overconfidence rate (Claim C1) — peer-reviewed, multi-model, multi-scenario study<a class="fn" href="#ref1">[1]</a></p>

        <p><strong>Weakest point:</strong> Multi-agent amplification (Section 5) — theoretical model built from well-evidenced components, but the compound effect itself lacks direct empirical validation</p>

        <p><strong>What would invalidate this entire report?</strong> A large-scale study demonstrating that 2026-generation models have resolved RLHF-induced overconfidence through training improvements, achieving ECE < 0.05 on verbalized confidence across domains. As of February 2026, I see no evidence this has occurred.</p>

        <p><strong>Methodology:</strong> Multi-agent research pipeline — structured literature review, source cross-referencing, gap analysis, claim verification. 12 primary sources reviewed. All claims registered with confidence levels and invalidation conditions.</p>

        <p><strong>Known gaps:</strong></p>
        <ul>
            <li>84% figure tested in clinical domain only — cross-domain replication needed</li>
            <li>No head-to-head ECE comparison across GPT-4, Claude, and Gemini</li>
            <li>Multi-agent compound miscalibration is modeled, not measured</li>
            <li>Alert fatigue data extrapolated from SOC/healthcare to AI agents</li>
        </ul>

        <p><strong>This report was created with a multi-agent research system.</strong></p>
    </div>

    <!-- CLAIM REGISTER -->
    <div class="section" id="claim-register">
        <h2>Claim Register</h2>

        <table>
            <thead>
                <tr>
                    <th>#</th>
                    <th>Claim</th>
                    <th>Value</th>
                    <th>Source</th>
                    <th>Confidence</th>
                    <th>What Would Invalidate</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>C1</td>
                    <td>LLMs overconfident in 84% of scenarios</td>
                    <td>84%, n=9 models, 351 scenarios</td>
                    <td>PMC/12249208 [1]</td>
                    <td>High</td>
                    <td>Replication failure; clinical-only limitation</td>
                </tr>
                <tr>
                    <td>C2</td>
                    <td>VCE poorly correlated with correctness</td>
                    <td>r ≈ 0.3–0.5</td>
                    <td>arXiv:2602.00279 [2]</td>
                    <td>High</td>
                    <td>Large-scale study showing strong correlation</td>
                </tr>
                <tr>
                    <td>C3</td>
                    <td>VCE biased upward by 20–30pp</td>
                    <td>20–30pp</td>
                    <td>Tian et al. 2023 [3]</td>
                    <td>Medium-High</td>
                    <td>Model-specific; may improve with newer models</td>
                </tr>
                <tr>
                    <td>C4</td>
                    <td>Instruction tuning worsens calibration</td>
                    <td>Directional</td>
                    <td>Kadavath et al. 2022 [9]</td>
                    <td>High</td>
                    <td>Architecture change preserving calibration through RLHF</td>
                </tr>
                <tr>
                    <td>C5</td>
                    <td>SOC alerts: 67% ignored</td>
                    <td>67%, n=2,000 analysts</td>
                    <td>Vectra 2023 [5]</td>
                    <td>High</td>
                    <td>SOC-specific; transfer to AI agents is analogical</td>
                </tr>
                <tr>
                    <td>C6</td>
                    <td>Healthcare false positives: 80–99%</td>
                    <td>80–99%</td>
                    <td>PMC6904899 [6]</td>
                    <td>High</td>
                    <td>Domain-specific</td>
                </tr>
                <tr>
                    <td>C7</td>
                    <td>Budget-CoCoA: $0.005/check</td>
                    <td>$0.005</td>
                    <td>Hobelsberger et al. + pricing [12]</td>
                    <td>High</td>
                    <td>Pricing change; single-study</td>
                </tr>
                <tr>
                    <td>C8</td>
                    <td>Multi-agent amplification compounds overconfidence</td>
                    <td>Theoretical + directional</td>
                    <td>MAS hijacking research [4]</td>
                    <td>Medium</td>
                    <td>Empirical study showing cancellation</td>
                </tr>
                <tr>
                    <td>C9</td>
                    <td>RLHF selects for overconfidence</td>
                    <td>Mechanistic argument</td>
                    <td>Training dynamics [9]</td>
                    <td>Medium-High</td>
                    <td>RLHF variant preserving calibration</td>
                </tr>
                <tr>
                    <td>C10</td>
                    <td>Temperature scaling requires logit access</td>
                    <td>Technical fact</td>
                    <td>Guo et al. 2017 [13]</td>
                    <td>High</td>
                    <td>API providers exposing calibrated logits</td>
                </tr>
            </tbody>
        </table>
    </div>

    <!-- REFERENCES -->
    <div class="section" id="references">
        <h2>References</h2>

        <p id="ref1">[1] PMC/12249208 (2024). "Overconfidence in LLM Clinical Decision-Making." Peer-reviewed study, 9 models, 351 scenarios.</p>

        <p id="ref2">[2] arXiv:2602.00279 (January 2026). "Verbalized Confidence Expressions in Large Language Models." Preprint.</p>

        <p id="ref3">[3] Tian, K. et al. (2023). "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models." Preprint.</p>

        <p id="ref4">[4] arXiv:2503.12188 (2025). "Hijacking Multi-Agent Systems: Adversarial Manipulation in Collaborative AI." Preprint.</p>

        <p id="ref5">[5] Vectra (2023). "State of Threat Detection Report." Industry survey, n=2,000 SOC analysts.</p>

        <p id="ref6">[6] PMC6904899. "Clinical Decision Support Alert Fatigue: A Meta-Review." Peer-reviewed.</p>

        <p id="ref7">[7] VW Cariad financial reports; public filings. $7.5B in documented losses.</p>

        <p id="ref8">[8] Naeini, M.P. et al. (2015). "Obtaining Well Calibrated Probabilities Using Bayesian Binning into Quantiles." AAAI 2015.</p>

        <p id="ref9">[9] Kadavath, S. et al. (2022). "Language Models (Mostly) Know What They Know." Anthropic. Preprint.</p>

        <p id="ref10">[10] Xiong, M. et al. (2024). "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs." Peer-reviewed.</p>

        <p id="ref11">[11] Wang, X. et al. (2022). "Self-Consistency Improves Chain of Thought Reasoning in Language Models." Peer-reviewed.</p>

        <p id="ref12">[12] Hobelsberger, M. et al. (2025). "CoCoA: Confidence and Consistency Aggregation for Calibrated LLM Outputs." arXiv:2510.20460. Preprint.</p>

        <p id="ref13">[13] Guo, C. et al. (2017). "On Calibration of Modern Neural Networks." ICML 2017. Peer-reviewed.</p>

        <p id="ref14">[14] Angelopoulos, A.N. & Bates, S. (2023). "Conformal Prediction: A Gentle Introduction." Tutorial/Survey.</p>
    </div>

    <!-- CITE-AS -->
    <div class="section">
        <p><strong>Cite as:</strong> Ziesche, F. (2026). The Calibration Gap — Why 84% of AI Agents Are Overconfident and What It Costs. Ainary Research Report, AR-009.</p>
    </div>

    <!-- AUTHOR BIO -->
    <div class="section">
        <h2>About the Author</h2>
        <p>Florian Ziesche is the founder of Ainary Ventures, where he builds AI-augmented research and decision systems for organizations navigating the trust gap in autonomous AI. His work focuses on the intersection of AI agent architecture, calibration infrastructure, and enterprise trust — informed by experience building AI products across the US and Europe.</p>
    </div>

    <!-- CTA FOOTER -->
    <div class="cta-footer">
        <h2><a href="mailto:florian@ainaryventures.com">Request a Project →</a></h2>
        <p class="services">Multi-Agent Architecture · AI Systems · Automation · Second Brain · Pilot Projects</p>
        <p class="contact">
            <a href="mailto:florian@ainaryventures.com">florian@ainaryventures.com</a> | 
            <a href="https://ainaryventures.com">ainaryventures.com</a>
        </p>
        <p class="tagline">HUMAN × AI = LEVERAGE</p>
    </div>

    <!-- FOOTER -->
    <div class="footer">
        <p>© 2026 Ainary Ventures</p>
    </div>

</div>

</body>
</html>