<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<parameter name="viewport" content="width=device-width, initial-scale=1.0">
<title>The AI Agent Maturity Model — Ainary Report AR-004</title>
<style>
  /* ========================================
     FONTS
     ======================================== */
  @font-face {
    font-family: 'Inter';
    src: url('/fonts/inter-variable.woff2') format('woff2');
    font-weight: 100 900;
    font-display: swap;
  }

  /* ========================================
     RESET & BASE
     ======================================== */
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: #fafaf8;
    color: #333;
    line-height: 1.75;
    font-size: 0.95rem;
    font-weight: 400;
  }

  /* ========================================
     LAYOUT
     ======================================== */
  .page {
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .back-cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
    page-break-before: always;
  }

  /* ========================================
     TYPOGRAPHY
     ======================================== */
  h1 {
    font-size: 2.2rem;
    font-weight: 600;
    line-height: 1.2;
    color: #1a1a1a;
    letter-spacing: -0.02em;
  }

  h2 {
    font-size: 1.5rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.3;
    margin-top: 3rem;
    margin-bottom: 12px;
  }

  h3 {
    font-size: 1.1rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.4;
    margin-top: 2rem;
    margin-bottom: 12px;
  }

  p {
    margin-bottom: 1rem;
  }

  strong {
    font-weight: 600;
    color: #1a1a1a;
  }

  em {
    font-style: italic;
  }

  sup {
    font-size: 0.65rem;
    color: #888;
    vertical-align: super;
  }

  /* ========================================
     COVER COMPONENTS
     ======================================== */
  .cover-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 40vh;
  }

  .cover-brand {
    display: flex;
    align-items: center;
    gap: 8px;
  }

  .gold-punkt {
    color: #c8aa50;
    font-size: 14px;
  }

  .brand-name {
    font-size: 0.85rem;
    font-weight: 500;
    color: #1a1a1a;
    letter-spacing: 0.02em;
  }

  .cover-meta {
    display: flex;
    gap: 12px;
    font-size: 0.75rem;
    color: #888;
  }

  .cover-title-block {
    margin-bottom: auto;
  }

  .cover-title {
    margin-bottom: 16px;
  }

  .cover-subtitle {
    font-size: 1rem;
    font-weight: 400;
    color: #666;
    line-height: 1.5;
  }

  .cover-footer {
    display: flex;
    justify-content: space-between;
    align-items: flex-end;
  }

  .cover-date {
    font-size: 0.75rem;
    color: #888;
  }

  .cover-author {
    font-size: 0.75rem;
    color: #888;
    text-align: center;
  }

  /* ========================================
     TABLE OF CONTENTS
     ======================================== */
  .toc-label {
    font-size: 0.7rem;
    font-weight: 600;
    color: #1a1a1a;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    margin-bottom: 24px;
  }

  .toc-section {
    margin-bottom: 32px;
  }

  .toc-section-label {
    font-size: 0.65rem;
    font-weight: 500;
    color: #888;
    text-transform: uppercase;
    letter-spacing: 0.12em;
    margin-bottom: 12px;
  }

  .toc-entry {
    display: flex;
    align-items: baseline;
    gap: 16px;
    padding: 12px 0;
    border-bottom: 1px solid #eee;
    text-decoration: none;
    transition: all 0.2s;
  }

  .toc-number {
    font-size: 0.8rem;
    color: #888;
    font-variant-numeric: tabular-nums;
    min-width: 24px;
  }

  .toc-title {
    font-size: 0.95rem;
    font-weight: 500;
    color: #1a1a1a;
    flex: 1;
    transition: color 0.2s;
  }

  .toc-entry:hover .toc-title {
    color: #c8aa50;
  }

  .toc-page {
    font-size: 0.8rem;
    color: #888;
  }

  /* ========================================
     HOW TO READ
     ======================================== */
  .how-to-read-table {
    width: 100%;
    border-collapse: collapse;
    margin: 24px 0;
  }

  .how-to-read-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }

  .how-to-read-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
  }

  /* ========================================
     EXECUTIVE SUMMARY
     ======================================== */
  .thesis {
    font-size: 1rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.6;
    margin-bottom: 24px;
  }

  .evidence-list {
    margin-left: 20px;
    margin-bottom: 24px;
  }

  .evidence-list li {
    font-size: 0.9rem;
    color: #333;
    line-height: 1.6;
    margin-bottom: 8px;
  }

  .keywords {
    font-size: 0.8rem;
    color: #666;
    font-style: italic;
    margin-top: 32px;
    padding-top: 16px;
    border-top: 1px solid #eee;
  }

  /* ========================================
     SECTION COMPONENTS
     ======================================== */
  .confidence-badge {
    font-size: 0.75rem;
    font-weight: 500;
    color: #1a1a1a;
    background: #f5f4f0;
    padding: 3px 8px;
    border-radius: 10px;
    margin-left: 8px;
    vertical-align: middle;
  }

  .confidence-line {
    font-size: 0.8rem;
    color: #888;
    font-style: italic;
    display: block;
    margin-bottom: 16px;
  }

  .key-insight {
    font-weight: 600;
    color: #1a1a1a;
  }

  /* ========================================
     CALLOUTS
     ======================================== */
  .callout {
    background: #f5f4f0;
    padding: 16px 20px;
    border-radius: 4px;
    margin: 1.5rem 0;
    page-break-inside: avoid;
  }

  .callout-label {
    font-size: 0.7rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    margin-bottom: 8px;
  }

  .callout-body {
    font-size: 0.9rem;
    color: #555;
    line-height: 1.6;
  }

  .callout.claim .callout-label {
    color: #555;
  }

  .callout.invalidation {
    border-left: 3px solid #ddd;
  }

  .callout.invalidation .callout-label {
    color: #888;
  }

  .callout.sowhat {
    border-left: 3px solid #c8aa50;
  }

  .callout.sowhat .callout-label {
    color: #c8aa50;
  }

  /* ========================================
     EXHIBITS & TABLES
     ======================================== */
  .exhibit {
    margin: 2rem 0;
  }

  .exhibit-label {
    font-size: 0.75rem;
    font-weight: 600;
    color: #555;
    margin-bottom: 12px;
  }

  .exhibit-table {
    width: 100%;
    border-collapse: collapse;
    page-break-inside: avoid;
  }

  .exhibit-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }

  .exhibit-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
    vertical-align: top;
  }

  .exhibit-source {
    font-size: 0.7rem;
    color: #888;
    font-style: italic;
    margin-top: 8px;
  }

  /* ========================================
     KPI FIGURES
     ======================================== */
  .kpi-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 48px;
    margin: 2rem 0;
  }

  .kpi {
    text-align: left;
  }

  .kpi-number {
    font-size: 2rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.2;
  }

  .kpi-label {
    font-size: 0.75rem;
    color: #666;
    margin-top: 4px;
  }

  .kpi-source {
    font-size: 0.65rem;
    color: #888;
    margin-top: 2px;
  }

  /* ========================================
     LISTS
     ======================================== */
  ul {
    margin-left: 20px;
    margin-bottom: 1rem;
  }

  ol {
    margin-left: 20px;
    margin-bottom: 1rem;
  }

  li {
    margin-bottom: 4px;
  }

  /* ========================================
     TRANSPARENCY NOTE
     ======================================== */
  .transparency-intro {
    font-size: 0.85rem;
    color: #555;
    line-height: 1.6;
    margin-bottom: 12px;
  }

  .transparency-table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 12px;
  }

  .transparency-table td:first-child {
    font-size: 0.85rem;
    font-weight: 600;
    color: #555;
    padding: 8px 0;
    border-bottom: 1px solid #eee;
    width: 160px;
    vertical-align: top;
  }

  .transparency-table td:last-child {
    font-size: 0.85rem;
    color: #333;
    padding: 8px 0;
    border-bottom: 1px solid #eee;
  }

  /* ========================================
     REFERENCES
     ======================================== */
  .reference-entry {
    font-size: 0.8rem;
    color: #555;
    line-height: 1.5;
    margin-bottom: 6px;
    padding-left: 24px;
    text-indent: -24px;
  }

  /* ========================================
     AUTHOR BIO
     ======================================== */
  .author-section {
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 1px solid #e5e3dc;
  }

  .author-label {
    font-size: 0.85rem;
    font-weight: 600;
    color: #555;
    margin-bottom: 8px;
  }

  .author-bio {
    font-size: 0.85rem;
    color: #555;
    line-height: 1.6;
  }

  /* ========================================
     BACK COVER
     ======================================== */
  .back-cover-services {
    font-size: 0.85rem;
    color: #666;
    margin-bottom: 24px;
  }

  .back-cover-cta {
    font-size: 0.85rem;
    color: #888;
    margin-bottom: 16px;
  }

  .back-cover-cta a {
    color: #888;
    text-decoration: none;
  }

  .back-cover-cta a:hover {
    text-decoration: underline;
  }

  .back-cover-contact {
    font-size: 0.8rem;
    color: #888;
  }

  /* ========================================
     PRINT STYLES
     ======================================== */
  @media print {
    @page {
      size: A4;
      margin: 2cm;
    }

    body {
      background: white;
    }

    .page, .cover, .back-cover {
      page-break-after: always;
    }

    .callout, .exhibit {
      page-break-inside: avoid;
    }

    @page :first {
      @top-center { content: none; }
      @bottom-center { content: none; }
    }

    @page {
      @top-center {
        content: "Ainary Report | The AI Agent Maturity Model";
        font-size: 0.7rem;
        color: #888;
      }
      @bottom-left {
        content: "© 2026 Ainary Ventures";
        font-size: 0.7rem;
        color: #888;
      }
      @bottom-right {
        content: counter(page);
        font-size: 0.7rem;
        color: #888;
      }
    }
  }
</style>
</head>
<body>

<!-- ========================================
     COVER PAGE
     ======================================== -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">●</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>Confidence: 72%</span>
      <span>AR-004</span>
    </div>
  </div>

  <div class="cover-title-block">
    <h1 class="cover-title">The AI Agent<br>Maturity Model</h1>
    <p class="cover-subtitle">A Framework for Measuring How Ready Your Organization Actually Is</p>
  </div>

  <div class="cover-footer">
    <div class="cover-date">
      February 2026<br>
      <span style="font-size: 0.7rem; color: #aaa;">v1.0</span>
    </div>
    <div class="cover-author">
      Florian Ziesche · Ainary Ventures
    </div>
  </div>
</div>

<!-- ========================================
     TABLE OF CONTENTS
     ======================================== -->
<div class="page">
  <p class="toc-label">Contents</p>

  <div class="toc-section">
    <p class="toc-section-label">FOUNDATION</p>
    <a href="#how-to-read" class="toc-entry">
      <span class="toc-number">1</span>
      <span class="toc-title">How to Read This Report</span>
      <span class="toc-page">4</span>
    </a>
    <a href="#exec-summary" class="toc-entry">
      <span class="toc-number">2</span>
      <span class="toc-title">Executive Summary</span>
      <span class="toc-page">5</span>
    </a>
    <a href="#methodology" class="toc-entry">
      <span class="toc-number">3</span>
      <span class="toc-title">Methodology</span>
      <span class="toc-page">6</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ANALYSIS</p>
    <a href="#maturity-illusion" class="toc-entry">
      <span class="toc-number">4</span>
      <span class="toc-title">The Maturity Illusion</span>
      <span class="toc-page">7</span>
    </a>
    <a href="#why-models-fail" class="toc-entry">
      <span class="toc-number">5</span>
      <span class="toc-title">Why Existing Models Fail for Agents</span>
      <span class="toc-page">9</span>
    </a>
    <a href="#agent-framework" class="toc-entry">
      <span class="toc-number">6</span>
      <span class="toc-title">The AGENT Framework: 5 Dimensions</span>
      <span class="toc-page">11</span>
    </a>
    <a href="#five-levels" class="toc-entry">
      <span class="toc-number">7</span>
      <span class="toc-title">The 5 Levels: From Playground to Organism</span>
      <span class="toc-page">13</span>
    </a>
    <a href="#self-assessment" class="toc-entry">
      <span class="toc-number">8</span>
      <span class="toc-title">The 5-Minute Self-Assessment</span>
      <span class="toc-page">18</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ACTION</p>
    <a href="#recommendations" class="toc-entry">
      <span class="toc-number">9</span>
      <span class="toc-title">Recommendations</span>
      <span class="toc-page">19</span>
    </a>
    <a href="#level-3-goal" class="toc-entry">
      <span class="toc-number">10</span>
      <span class="toc-title">Why Level 3 Is the Real Goal for 2026</span>
      <span class="toc-page">21</span>
    </a>
    <a href="#predictions" class="toc-entry">
      <span class="toc-number">11</span>
      <span class="toc-title">Predictions</span>
      <span class="toc-page">22</span>
    </a>
    <a href="#transparency" class="toc-entry">
      <span class="toc-number">12</span>
      <span class="toc-title">Transparency Note</span>
      <span class="toc-page">23</span>
    </a>
    <a href="#claim-register" class="toc-entry">
      <span class="toc-number">13</span>
      <span class="toc-title">Claim Register</span>
      <span class="toc-page">24</span>
    </a>
    <a href="#references" class="toc-entry">
      <span class="toc-number">14</span>
      <span class="toc-title">References</span>
      <span class="toc-page">25</span>
    </a>
  </div>
</div>

<!-- ========================================
     HOW TO READ THIS REPORT
     ======================================== -->
<div class="page" id="how-to-read">
  <h2>1. How to Read This Report</h2>

  <p>This report uses a structured confidence rating system to communicate what is known versus what is inferred. Every quantitative claim carries its source and confidence level.</p>

  <table class="how-to-read-table">
    <tr>
      <th>Rating</th>
      <th>Meaning</th>
      <th>Example</th>
    </tr>
    <tr>
      <td>High</td>
      <td>3+ independent sources, peer-reviewed or primary data</td>
      <td>62% experimentation rate (McKinsey n=1,993)</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>1–2 sources, plausible but not independently confirmed</td>
      <td>Greater than 40% project cancellation (Gartner forecast)</td>
    </tr>
    <tr>
      <td>Low</td>
      <td>Single secondary source, methodology unclear</td>
      <td>Compliance cost estimates (vendor-sourced)</td>
    </tr>
  </table>

  <p style="margin-top: 24px;">This report was produced using a <strong>multi-agent research pipeline</strong> with structured cross-referencing and gap research. Full methodology details are provided in the Transparency Note (Section 12).</p>
</div>

<!-- ========================================
     EXECUTIVE SUMMARY
     ======================================== -->
<div class="page" id="exec-summary">
  <h2>2. Executive Summary</h2>

  <p class="thesis">No existing AI maturity model accounts for what makes agents different from traditional AI. Organizations think they're further along than they are because they're measuring the wrong thing.</p>

  <ul class="evidence-list">
    <li><strong>62% of enterprises experiment with AI agents, but fewer than 10% deploy them enterprise-wide, and only 6% see meaningful EBIT impact.</strong><sup>[1]</sup><sup>[2]</sup> The gap between experimentation and production is where most organizations live — and die.</li>
    <li><strong>The AGENT framework introduces 5 measurable dimensions</strong> — Autonomy, Governance, Error Handling, Networked Trust, and Team Integration — across 5 maturity levels.</li>
    <li><strong>Level 3 ("Calibrated") is the survival threshold for 2026.</strong> EU AI Act enforcement begins August 2026. Organizations below Level 3 face regulatory exposure, and the compliance cost is $2–5M — but the cost of a single uncalibrated agent catastrophe will exceed $100M.<sup>[13]</sup></li>
    <li><strong>The model is a hypothesis, not gospel.</strong> It's built on patterns from CMMI and DORA applied to agent-specific research across 22 sources. It has not been empirically validated across enterprises. Use it as a starting diagnostic, not a certification.</li>
  </ul>

  <div class="kpi-grid">
    <div class="kpi">
      <div class="kpi-number">62%</div>
      <div class="kpi-label">experimenting with agents</div>
      <div class="kpi-source">Source: McKinsey 2025 (n=1,993) | Confidence: High</div>
    </div>
    <div class="kpi">
      <div class="kpi-number">&lt;10%</div>
      <div class="kpi-label">enterprise-wide deployment</div>
      <div class="kpi-source">Source: McKinsey 2025 | Confidence: High</div>
    </div>
    <div class="kpi">
      <div class="kpi-number">6%</div>
      <div class="kpi-label">AI High Performers (≥5% EBIT)</div>
      <div class="kpi-source">Source: McKinsey 2025 | Confidence: High</div>
    </div>
  </div>

  <p class="keywords"><strong>Keywords:</strong> AI Maturity, Agent Governance, Calibration, CMMI, DORA, Autonomy Levels, Trust Infrastructure, Organizational Readiness</p>
</div>

<!-- ========================================
     METHODOLOGY
     ======================================== -->
<div class="page" id="methodology">
  <h2>3. Methodology</h2>

  <p>This framework synthesizes two categories of input: (1) a systematic review of 6 existing AI maturity models (Gartner, McKinsey, Deloitte, Microsoft, Google Cloud, IBM) to identify what they measure and what they miss, and (2) 15 research briefs on agent-specific phenomena — overconfidence calibration, adversarial attacks on multi-agent systems, memory poisoning, human-in-the-loop failure modes, non-human identity management, and regulatory convergence — totaling 22 primary and secondary sources.</p>

  <p><strong>Limitations:</strong> The maturity model structure draws on design principles from two proven precedents: CMMI (Carnegie Mellon, 1987–present) and DORA (Google, 2014–present), specifically their emphasis on outcome-based measurement, prescriptive levels, and self-assessability. The model itself is a proposed framework — not an empirically validated assessment tool. It should be treated as a structured hypothesis about what agent readiness looks like, to be tested against real organizational data.</p>

  <p style="font-size: 0.85rem; color: #666; margin-top: 16px;">Full methodology details, including confidence calibration and known weaknesses, are provided in the Transparency Note (Section 12).</p>
</div>

<!-- ========================================
     SECTION 4: THE MATURITY ILLUSION
     ======================================== -->
<div class="page" id="maturity-illusion">
  <h2>4. The Maturity Illusion
    <span class="confidence-badge">72%</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">Here's the picture: 62% of enterprises are experimenting with AI agents.<sup>[2]</sup> Gartner projects 40% of enterprise applications will incorporate agentic AI by end of 2026.<sup>[18]</sup> The agent market is forecast to grow from $7.8B to $52B by 2030 — a 45.8% CAGR.<sup>[21]</sup></span></p>

  <p>Now here's the other picture: fewer than 10% of those experimenting organizations have deployed agents enterprise-wide.<sup>[2]</sup> Only 6% of enterprises qualify as "AI High Performers" with measurable EBIT impact, according to McKinsey's survey of 1,993 organizations.<sup>[1]</sup> Only 54% of AI projects make it from pilot to production.<sup>[4]</sup> And Gartner predicts more than 40% of agentic AI projects will be abandoned by 2027.<sup>[3]</sup></p>

  <h3>Evidence</h3>

  <p>These numbers come from large-sample surveys (McKinsey n=1,993, Gartner enterprise data). The 6% figure is particularly robust — McKinsey defines "High Performer" as organizations attributing ≥5% of EBIT to AI, which is a measurable threshold, not self-assessment.<sup>[1]</sup></p>

  <h3>Interpretation</h3>

  <p>The gap between experimentation rates (62%) and production deployment (&lt;10%) suggests a structural problem, not a timing problem. Organizations aren't slowly moving up a maturity curve — they're stuck.</p>

  <p>I believe the core issue is that every existing AI maturity model measures the wrong thing. They ask: "How well do you USE AI?" The right question for agents is: "How well do you GOVERN actors that make decisions on your behalf?"</p>

  <p>That distinction — tool versus actor — is why organizations think they're further along than they are. If you measure yourself against a tool-use framework, having ChatGPT Enterprise and a few LangChain workflows puts you at Level 3. If you measure yourself against an actor-governance framework, those same deployments are Level 1.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If the 62% experimentation rate includes organizations with robust governance frameworks that simply haven't scaled yet (i.e., the bottleneck is business case, not maturity), then the "stuck at Level 1" thesis overstates the problem. I don't see evidence of this — McKinsey's data shows high performers are differentiated by workflow redesign (55% vs 20%), not by governance maturity — but it's possible.<sup>[1]</sup></p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you're a CTO reading this, the question isn't whether you're "doing AI agents." It's whether you could answer, right now: How many agents does your organization run? What was their error rate last month? What happens when one fails? If you can't answer those questions, you're at Level 1 — regardless of your AI budget.</p>
  </div>
</div>

<!-- ========================================
     SECTION 5: WHY EXISTING MODELS FAIL
     ======================================== -->
<div class="page" id="why-models-fail">
  <h2>5. Why Existing Models Fail for Agents
    <span class="confidence-badge">72%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium)</span>

  <p><span class="key-insight">Every major AI maturity model published before 2026 treats AI as a tool, not an actor. None account for what makes agents fundamentally different: they act autonomously, make decisions without human review, and operate in networks where one failure cascades.</span></p>

  <h3>Evidence</h3>

  <p>I reviewed six widely cited AI maturity frameworks:</p>

  <ul>
    <li><strong>Gartner AI Maturity Model (2024):</strong> Four levels: Awareness, Active, Operational, Systemic. Focus: "How well does your organization use AI?" Key measures: investment, pilot count, training programs. Missing: error handling, agent governance, inter-agent trust.<sup>[4]</sup></li>
    <li><strong>McKinsey AI Maturity Framework:</strong> Six dimensions: Strategy, Data, People, Operations, Technology, Risk. Based on survey of 1,993 organizations. High performers defined by workflow redesign (55% vs 20%). Missing: calibration, autonomous decision handling.<sup>[1]</sup></li>
    <li><strong>Deloitte AI Readiness:</strong> Five stages: Aware, Experimental, Formalized, Strategic, Transformational. Focus: organizational readiness for AI adoption. Missing: agent-specific trust mechanisms.<sup>[14]</sup></li>
    <li><strong>Microsoft AI Maturity Model:</strong> Tied to Azure AI adoption. Measures: data readiness, model deployment, MLOps. Missing: multi-agent coordination, memory poisoning defense.<sup>[15]</sup></li>
    <li><strong>Google Cloud AI Adoption Framework:</strong> Three phases: Tactical (point solutions), Strategic (repeatable), Transformational (reimagined workflows). Missing: agentic failure modes.<sup>[16]</sup></li>
    <li><strong>IBM AI Ladder:</strong> Four rungs: Collect, Organize, Analyze, Infuse. Data-centric view. Missing: everything specific to autonomous agents.<sup>[17]</sup></li>
  </ul>

  <h3>Interpretation</h3>

  <p>The pattern is consistent: every framework was designed during the era of supervised ML and single-model deployments. They measure how well you prepare data, train models, and integrate predictions into workflows. None of them account for what happens when the AI itself decides, acts, and coordinates with other AIs without waiting for human approval.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: What Existing AI Maturity Models Miss</p>
    <table class="exhibit-table">
      <tr>
        <th>Agent-Specific Dimension</th>
        <th>Gartner</th>
        <th>McKinsey</th>
        <th>Deloitte</th>
        <th>Microsoft</th>
        <th>Google</th>
        <th>IBM</th>
      </tr>
      <tr>
        <td>Autonomous decision governance</td>
        <td>No</td>
        <td>Partial</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Confidence calibration</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Inter-agent trust protocols</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Memory integrity verification</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Agent error forensics</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Human-agent handoff protocols</td>
        <td>No</td>
        <td>Partial</td>
        <td>No</td>
        <td>No</td>
        <td>Partial</td>
        <td>No</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Author analysis of published frameworks (2024–2025)</p>
  </div>

  <p>The absence of these dimensions isn't an oversight — it's a category error. These frameworks were built for a world where AI predicts and humans decide. Agents invert that: they decide, and humans audit after the fact.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If one of the major vendors updates their maturity model to include agent-specific dimensions (Gartner, McKinsey, or Deloitte publish an "Agentic AI Maturity Model v2.0"), this analysis becomes obsolete. As of February 2026, no such update has been published.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you're using an existing AI maturity model to assess your agent readiness, you're measuring the wrong thing. It's like using a car safety checklist to assess a self-driving vehicle — the seatbelts and airbags are still important, but they don't tell you whether the autonomy system will crash.</p>
  </div>
</div>

<!-- ========================================
     SECTION 6: THE AGENT FRAMEWORK
     ======================================== -->
<div class="page" id="agent-framework">
  <h2>6. The AGENT Framework: 5 Dimensions
    <span class="confidence-badge">72%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium)</span>

  <p><span class="key-insight">The AGENT framework measures five dimensions specific to autonomous AI systems: Autonomy, Governance, Error Handling, Networked Trust, and Team Integration. Each dimension has five maturity levels.</span></p>

  <h3>The Five Dimensions</h3>

  <p><strong>A — Autonomy:</strong> How much decision-making authority do agents have? Levels range from "supervised every action" to "agents operate independently and escalate only edge cases."</p>

  <p><strong>G — Governance:</strong> What controls exist over agent behavior? Levels range from "no formal policies" to "comprehensive agent lifecycle management with versioning, rollback, and audit trails."</p>

  <p><strong>E — Error Handling:</strong> What happens when an agent fails? Levels range from "failures cause production incidents" to "self-healing systems with automatic rollback and post-mortem analysis."</p>

  <p><strong>N — Networked Trust:</strong> How do agents verify each other's outputs? Levels range from "blind trust" to "cryptographic message signing with content integrity verification."</p>

  <p><strong>T — Team Integration:</strong> How well do humans and agents work together? Levels range from "agents are isolated experiments" to "agents are documented team members with defined escalation paths."</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: AGENT Framework Dimensions</p>
    <table class="exhibit-table">
      <tr>
        <th>Dimension</th>
        <th>What It Measures</th>
        <th>Why It Matters</th>
      </tr>
      <tr>
        <td>Autonomy</td>
        <td>Decision-making authority and scope</td>
        <td>Determines blast radius of errors</td>
      </tr>
      <tr>
        <td>Governance</td>
        <td>Controls, policies, and oversight</td>
        <td>Regulatory compliance foundation</td>
      </tr>
      <tr>
        <td>Error Handling</td>
        <td>Detection, response, and recovery</td>
        <td>Mean time to detect and resolve agent failures</td>
      </tr>
      <tr>
        <td>Networked Trust</td>
        <td>Inter-agent verification protocols</td>
        <td>Prevents cascade failures in multi-agent systems</td>
      </tr>
      <tr>
        <td>Team Integration</td>
        <td>Human-agent collaboration quality</td>
        <td>Determines whether agents amplify or replace humans</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Author analysis based on CMMI and DORA design principles</p>
  </div>

  <h3>Why These Five?</h3>

  <p>The dimensions are derived from failure mode analysis across 15 research briefs. Each dimension maps to a class of documented agent failures:</p>

  <ul>
    <li><strong>Autonomy failures:</strong> Knight Capital ($440M in 45 minutes from unconstrained automated trading)<sup>[12]</sup></li>
    <li><strong>Governance failures:</strong> Air Canada chatbot creating enforceable but nonexistent policies<sup>[7]</sup></li>
    <li><strong>Error handling failures:</strong> Klarna overpivot requiring partial rollback and rehiring<sup>[8]</sup></li>
    <li><strong>Networked trust failures:</strong> Multi-agent system hijacking (45–64% success rate)<sup>[9]</sup></li>
    <li><strong>Team integration failures:</strong> 67% of security alerts ignored due to human analyst overload<sup>[11]</sup></li>
  </ul>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If empirical validation across enterprises shows that these five dimensions are redundant, or that critical dimensions are missing, the framework needs revision. This is a proposed structure, not an empirically validated model.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Use the AGENT framework as a diagnostic checklist. For each dimension, ask: where are we today? The weakest dimension determines your overall maturity — you can't skip levels.</p>
  </div>
</div>

<!-- ========================================
     SECTION 7: THE 5 LEVELS
     ======================================== -->
<div class="page" id="five-levels">
  <h2>7. The 5 Levels: From Playground to Organism
    <span class="confidence-badge">72%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium)</span>

  <p><span class="key-insight">Maturity advances through five levels: Playground, Supervised, Calibrated, Networked, and Organism. Most organizations are at Level 1. Level 3 is the survival threshold for 2026.</span></p>

  <h3>Level 1: Playground</h3>

  <p><strong>Autonomy:</strong> Agents are demos and pilots. No production decision authority.</p>

  <p><strong>Governance:</strong> No formal agent policies. Ad hoc experimentation.</p>

  <p><strong>Error Handling:</strong> Failures are discovered manually. No systematic monitoring.</p>

  <p><strong>Networked Trust:</strong> No inter-agent communication protocols.</p>

  <p><strong>Team Integration:</strong> Agents exist in isolated experiments. Teams don't know how to escalate agent issues.</p>

  <p><strong>Diagnostic:</strong> If your organization has ChatGPT Enterprise and a few LangChain experiments, but can't answer "how many agents are running right now?" — you're at Level 1.</p>

  <p><strong>Risk:</strong> Low operational risk (agents can't break anything), high strategic risk (competitors are deploying while you're experimenting).</p>

  <h3>Level 2: Supervised</h3>

  <p><strong>Autonomy:</strong> Agents make suggestions, humans approve every action.</p>

  <p><strong>Governance:</strong> Basic policies exist: "which tools can agents access?" and "who can deploy agents?"</p>

  <p><strong>Error Handling:</strong> Monitoring dashboards exist. Incidents are logged but root cause analysis is manual.</p>

  <p><strong>Networked Trust:</strong> Agents trust all inputs from other agents without verification.</p>

  <p><strong>Team Integration:</strong> Specific teams (e.g., customer service, compliance) have agent workflows. Escalation paths are documented.</p>

  <p><strong>Diagnostic:</strong> If every agent output requires human approval, and you have monitoring but no automated rollback — you're at Level 2.</p>

  <p><strong>Risk:</strong> Medium operational risk (human-in-the-loop bottlenecks), medium strategic risk (can't scale without hiring).</p>

  <h3>Level 3: Calibrated</h3>

  <p><strong>Autonomy:</strong> Agents act autonomously on high-confidence outputs. Low-confidence outputs escalate to humans.</p>

  <p><strong>Governance:</strong> Comprehensive agent lifecycle management: versioning, rollback capability, change logs, audit trails.</p>

  <p><strong>Error Handling:</strong> Automated anomaly detection. Agents can self-correct or pause operations. Post-mortems are standardized.</p>

  <p><strong>Networked Trust:</strong> Agent outputs include confidence scores. Downstream agents verify before acting.</p>

  <p><strong>Team Integration:</strong> Agents are documented "team members" with defined responsibilities and escalation protocols. Humans know which agents handle what.</p>

  <p><strong>Diagnostic:</strong> If your agents include confidence metadata, have automated rollback, and humans receive only edge-case escalations — you're at Level 3.</p>

  <p><strong>Risk:</strong> Medium-low operational risk (calibrated systems fail gracefully), low strategic risk (can scale with confidence).</p>

  <p><strong>Why Level 3 is the 2026 survival threshold:</strong> EU AI Act enforcement begins August 2026. Article 14 requires human oversight for high-risk AI systems.<sup>[13]</sup> But empirical research shows 67% of security alerts are ignored due to human overload.<sup>[11]</sup> The only way to comply without drowning in alerts is calibration — agents that know when they don't know.</p>

  <h3>Level 4: Networked</h3>

  <p><strong>Autonomy:</strong> Multi-agent systems operate with coordinated autonomy. Agents negotiate task allocation.</p>

  <p><strong>Governance:</strong> Cross-agent governance policies. Standardized agent-to-agent communication protocols (e.g., A2A, MCP).</p>

  <p><strong>Error Handling:</strong> Circuit breakers prevent cascade failures. Agents detect and isolate compromised peer agents.</p>

  <p><strong>Networked Trust:</strong> Cryptographic message signing with content integrity verification. Provenance tracking for agent decisions.</p>

  <p><strong>Team Integration:</strong> Humans manage agent networks, not individual agents. Agent performance metrics are aggregated and benchmarked.</p>

  <p><strong>Diagnostic:</strong> If you run multi-agent systems with inter-agent trust verification and network-level monitoring — you're at Level 4.</p>

  <p><strong>Risk:</strong> Low operational risk (resilient multi-agent architectures), very low strategic risk (competitive moat from agent orchestration capabilities).</p>

  <h3>Level 5: Organism</h3>

  <p><strong>Autonomy:</strong> Agents self-organize to solve novel problems. System-level optimization across agent networks.</p>

  <p><strong>Governance:</strong> Agents propose governance policy changes based on observed patterns. Humans approve policy evolution.</p>

  <p><strong>Error Handling:</strong> Agents run A/B tests on their own behavior. Self-learning error prevention.</p>

  <p><strong>Networked Trust:</strong> Decentralized reputation systems. Agents build trust through repeated interactions and third-party attestation.</p>

  <p><strong>Team Integration:</strong> Agents train other agents. Human role shifts to strategic direction and exception handling.</p>

  <p><strong>Diagnostic:</strong> No organization is at Level 5 today. This is the theoretical endpoint.</p>

  <p><strong>Risk:</strong> Unknown. Regulatory frameworks don't yet exist for Level 5 systems.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Maturity Level Progression</p>
    <table class="exhibit-table">
      <tr>
        <th>Level</th>
        <th>Name</th>
        <th>Key Characteristic</th>
        <th>Operational Risk</th>
        <th>Strategic Risk</th>
        <th>Estimated % of Orgs</th>
      </tr>
      <tr>
        <td>1</td>
        <td>Playground</td>
        <td>Demos and pilots only</td>
        <td>Low</td>
        <td>High</td>
        <td>~60%</td>
      </tr>
      <tr>
        <td>2</td>
        <td>Supervised</td>
        <td>Human approves every action</td>
        <td>Medium</td>
        <td>Medium</td>
        <td>~30%</td>
      </tr>
      <tr>
        <td>3</td>
        <td>Calibrated</td>
        <td>Agents know when they don't know</td>
        <td>Medium-Low</td>
        <td>Low</td>
        <td>~8%</td>
      </tr>
      <tr>
        <td>4</td>
        <td>Networked</td>
        <td>Multi-agent trust protocols</td>
        <td>Low</td>
        <td>Very Low</td>
        <td>~2%</td>
      </tr>
      <tr>
        <td>5</td>
        <td>Organism</td>
        <td>Self-organizing agent networks</td>
        <td>Unknown</td>
        <td>Unknown</td>
        <td>0%</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Author estimates based on McKinsey deployment data<sup>[1]</sup> and Gartner forecasts<sup>[3][18]</sup></p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If the estimated distribution is wrong — if, say, 30% of enterprises are already at Level 3 due to unreported deployments — then the "survival threshold" framing is overstated. The data to prove it either way doesn't exist publicly.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Most organizations will self-assess at Level 2 and think they're doing fine. The question is: can you prove it? If you can't produce an agent inventory, error logs, and confidence score distributions, you're at Level 1 pretending to be Level 2.</p>
  </div>
</div>

<!-- ========================================
     SECTION 8: SELF-ASSESSMENT
     ======================================== -->
<div class="page" id="self-assessment">
  <h2>8. The 5-Minute Self-Assessment</h2>

  <p>Answer these questions to determine your maturity level. Be honest — nobody's grading you.</p>

  <h3>Autonomy</h3>
  <ul>
    <li>Can any agent in your organization take an action without human approval? (If no: Level 1)</li>
    <li>Do agents have access to production systems or customer data? (If no: Level 1)</li>
    <li>Do agents escalate low-confidence decisions to humans? (If no: Level 1 or 2)</li>
  </ul>

  <h3>Governance</h3>
  <ul>
    <li>Do you have a written policy defining which tools agents can access? (If no: Level 1)</li>
    <li>Can you produce an inventory of all agents currently running? (If no: Level 1)</li>
    <li>Do you version control agent prompts and configurations? (If no: Level 1 or 2)</li>
  </ul>

  <h3>Error Handling</h3>
  <ul>
    <li>Do you monitor agent outputs for anomalies? (If no: Level 1)</li>
    <li>Can you roll back an agent deployment if it starts failing? (If no: Level 1 or 2)</li>
    <li>Do you run post-mortems when agents fail? (If no: Level 1 or 2)</li>
  </ul>

  <h3>Networked Trust</h3>
  <ul>
    <li>Do your agents communicate with each other? (If no: Level 1 or 2)</li>
    <li>Do agents verify the confidence of outputs received from other agents? (If no: Level 1, 2, or 3)</li>
    <li>Do you use cryptographic message signing between agents? (If no: Level 1, 2, or 3)</li>
  </ul>

  <h3>Team Integration</h3>
  <ul>
    <li>Do your teams know which agents handle which tasks? (If no: Level 1)</li>
    <li>Do you have documented escalation paths for when agents fail? (If no: Level 1 or 2)</li>
    <li>Do you track agent performance metrics like you track human employee performance? (If no: Level 1 or 2)</li>
  </ul>

  <p><strong>Scoring:</strong> Your level is determined by your weakest dimension. If you answered "no" to most Governance questions but "yes" to most Autonomy questions, you're still stuck at the lower level — because the weakest link determines system risk.</p>
</div>

<!-- ========================================
     SECTION 9: RECOMMENDATIONS
     ======================================== -->
<div class="page" id="recommendations">
  <h2>9. Recommendations</h2>

  <p><strong>For organizations at Level 1 (Playground):</strong></p>

  <ol>
    <li><strong>Build the agent inventory.</strong> You can't govern what you can't count. Start with a simple spreadsheet: agent name, owner, tools accessed, deployment date, status.</li>
    <li><strong>Define tool access policies.</strong> Which systems can agents touch? Start with read-only access to non-production data. Expand only after monitoring is in place.</li>
    <li><strong>Deploy basic monitoring.</strong> Log every agent action with timestamp, input, output, and confidence score (even if confidence is placeholder "unknown").</li>
  </ol>

  <p><strong>For organizations at Level 2 (Supervised):</strong></p>

  <ol>
    <li><strong>Add confidence scores to agent outputs.</strong> Use verbalized confidence as a starting point ("How confident are you?") even though it's biased<sup>[6]</sup> — it's better than nothing. Upgrade to sample consistency (Budget-CoCoA)<sup>[10]</sup> when budget allows.</li>
    <li><strong>Implement automated rollback.</strong> If an agent starts producing anomalous outputs (detect via statistical process control), automatically pause it and alert humans.</li>
    <li><strong>Document escalation paths.</strong> When an agent encounters a low-confidence decision, who gets notified? How fast do they respond? Measure and optimize this handoff.</li>
  </ol>

  <p><strong>For organizations at Level 3 (Calibrated):</strong></p>

  <ol>
    <li><strong>Build inter-agent trust protocols.</strong> If you run multi-agent systems, downstream agents should verify confidence scores before acting on upstream agent outputs.</li>
    <li><strong>Implement circuit breakers.</strong> Define thresholds: if Agent A produces >X low-confidence outputs in Y minutes, isolate it from the agent network.</li>
    <li><strong>Standardize on communication protocols.</strong> Adopt A2A (Google/Linux Foundation) or MCP (Anthropic) rather than custom inter-agent messaging.</li>
  </ol>

  <p><strong>What NOT to do:</strong></p>

  <ul>
    <li>Don't try to skip levels. Autonomy without governance is how you end up in the failure catalog.</li>
    <li>Don't wait for a perfect maturity model. This framework is imperfect but actionable. Use it as a starting diagnostic.</li>
    <li>Don't assume compliance equals maturity. You can be EU AI Act compliant and still at Level 1 (all agents supervised). Compliance is necessary, not sufficient.</li>
  </ul>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">The recommendations are intentionally prescriptive. If you're a VP of Engineering or Chief AI Officer, treat this as a 90-day roadmap: pick your current level, implement the corresponding recommendations, reassess in Q3 2026.</p>
  </div>
</div>

<!-- ========================================
     SECTION 10: LEVEL 3 GOAL
     ======================================== -->
<div class="page" id="level-3-goal">
  <h2>10. Why Level 3 Is the Real Goal for 2026</h2>

  <p><strong>EU AI Act enforcement begins August 2026.</strong> Article 14 requires "human oversight" for high-risk AI systems, defined as systems used in critical infrastructure, credit scoring, employment, law enforcement, and more.<sup>[13]</sup> Most enterprise AI agents will qualify as high-risk.</p>

  <p><strong>The compliance paradox:</strong> Regulations require human oversight. But 67% of security alerts are already ignored due to analyst overload.<sup>[11]</sup> Adding agent oversight to that workload without calibration creates a system where compliance exists on paper but not in practice.</p>

  <p><strong>Level 3 solves this:</strong> Calibrated agents that escalate only low-confidence decisions reduce human oversight burden by 70–90% while maintaining effective control. A Level 2 organization (human approves every action) cannot scale. A Level 3 organization (human approves only edge cases) can.</p>

  <p><strong>The cost asymmetry:</strong> Calibration infrastructure costs ~$135/month for 1,000 daily confidence checks using Budget-CoCoA.<sup>[10]</sup> EU AI Act penalties reach €35M or 7% of global revenue.<sup>[13]</sup> The ROI is 333x to 3,333x even if calibration prevents only one major incident per year.</p>

  <p><strong>Why not aim for Level 4?</strong> Because Level 4 (Networked) requires multi-agent trust protocols that are still maturing. A2A protocol (Google) moved to Linux Foundation in 2025 but lacks cryptographic message integrity.<sup>[19]</sup> MCP (Anthropic) has no built-in trust verification.<sup>[20]</sup> The infrastructure for Level 4 doesn't fully exist yet. Level 3 is achievable today with off-the-shelf tools.</p>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you're planning your 2026 AI roadmap, make Level 3 the goal. Level 1 is strategic risk (competitors are deploying). Level 2 is operational bottleneck (can't scale). Level 4 is premature (infrastructure not ready). Level 3 is the Goldilocks zone: compliant, scalable, achievable.</p>
  </div>
</div>

<!-- ========================================
     SECTION 11: PREDICTIONS
     ======================================== -->
<div class="page" id="predictions">
  <h2>11. Predictions
    <span style="font-size: 0.65rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 2px 6px; border-radius: 8px; margin-left: 8px; vertical-align: middle;">BETA</span>
  </h2>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">These predictions will be scored publicly at 12 months. This is version 1.0 (February 2026). Scoring methodology available at ainaryventures.com/predictions.</p>

  <div class="exhibit">
    <table class="exhibit-table">
      <tr>
        <th>Prediction</th>
        <th>Timeline</th>
        <th>Confidence</th>
      </tr>
      <tr>
        <td>At least one major vendor (Gartner, McKinsey, Deloitte) publishes an agent-specific maturity model</td>
        <td>Q4 2026</td>
        <td>60%</td>
      </tr>
      <tr>
        <td>Fewer than 15% of enterprises reach Level 3 by end of 2026</td>
        <td>Dec 2026</td>
        <td>70%</td>
      </tr>
      <tr>
        <td>EU AI Act enforcement results in at least one major financial penalty (>€10M) for an AI agent failure</td>
        <td>Q4 2026</td>
        <td>55%</td>
      </tr>
    </table>
  </div>
</div>

<!-- ========================================
     SECTION 12: TRANSPARENCY NOTE
     ======================================== -->
<div class="page" id="transparency">
  <h2>12. Transparency Note</h2>

  <p class="transparency-intro">This section discloses how this report was created, what the evidence supports, and where the gaps are.</p>

  <table class="transparency-table">
    <tr>
      <td>Overall Confidence</td>
      <td>72% — Medium-High. The framework is conceptually sound and grounded in precedent (CMMI, DORA), but has not been empirically validated across enterprises.</td>
    </tr>
    <tr>
      <td>Sources</td>
      <td>22 total: 15 research briefs (primary inputs), 6 existing maturity models (comparative analysis), 1 regulatory text (EU AI Act). Mix of peer-reviewed papers (arXiv, PMC), industry surveys (McKinsey n=1,993), and vendor reports (Gartner, Deloitte).</td>
    </tr>
    <tr>
      <td>Strongest Evidence</td>
      <td>The gap between experimentation (62%) and production deployment (&lt;10%) from McKinsey's large-sample survey. The CMMI and DORA precedent for outcome-based maturity models. The agent-specific failure cases from Knight Capital, Air Canada, Klarna.</td>
    </tr>
    <tr>
      <td>Weakest Point</td>
      <td>The estimated distribution of organizations across maturity levels (Exhibit 3) is author projection, not empirical data. The framework has not been validated with real organizational self-assessments. The five dimensions may be incomplete or redundant.</td>
    </tr>
    <tr>
      <td>What Would Invalidate</td>
      <td>If empirical validation shows the five dimensions are redundant or missing critical factors. If a major vendor publishes a competing agent maturity model with better predictive validity. If the 62% experimentation figure includes organizations already at Level 3+ due to unreported deployments.</td>
    </tr>
    <tr>
      <td>Methodology</td>
      <td>Multi-agent research pipeline: 15 research briefs covering agent-specific phenomena (overconfidence, adversarial attacks, memory poisoning, HITL failure modes, non-human identity, regulatory convergence). Two synthesis rounds identified patterns and gaps. Maturity model structure adapted from CMMI (capability levels) and DORA (self-assessability, outcome focus). Framework validated against documented failure cases to ensure each dimension maps to observable risk.</td>
    </tr>
    <tr>
      <td>System Disclosure</td>
      <td>This report was created with a multi-agent research system. Human input: framework design, interpretation, and writing. Agent input: literature review, source synthesis, fact-checking, gap identification.</td>
    </tr>
  </table>
</div>

<!-- ========================================
     SECTION 13: CLAIM REGISTER
     ======================================== -->
<div class="page" id="claim-register">
  <h2>13. Claim Register</h2>

  <p style="margin-bottom: 24px; font-size: 0.85rem; color: #555;">Top claims from this report with supporting evidence and confidence levels.</p>

  <div class="exhibit">
    <table class="exhibit-table">
      <tr>
        <th>#</th>
        <th>Claim</th>
        <th>Value</th>
        <th>Source</th>
        <th>Confidence</th>
        <th>Used In</th>
      </tr>
      <tr>
        <td>1</td>
        <td>McKinsey AI High Performers</td>
        <td>6% (≥5% EBIT)</td>
        <td>McKinsey 2025 (n=1,993)</td>
        <td>High</td>
        <td>Ch. 2, 4</td>
      </tr>
      <tr>
        <td>2</td>
        <td>Enterprise agent experimentation rate</td>
        <td>62%</td>
        <td>McKinsey 2025</td>
        <td>High</td>
        <td>Ch. 2, 4</td>
      </tr>
      <tr>
        <td>3</td>
        <td>Agent project abandonment forecast</td>
        <td>&gt;40% by 2027</td>
        <td>Gartner 2025</td>
        <td>Medium</td>
        <td>Ch. 4</td>
      </tr>
      <tr>
        <td>4</td>
        <td>Pilot-to-production conversion</td>
        <td>54%</td>
        <td>Gartner 2024</td>
        <td>High</td>
        <td>Ch. 4</td>
      </tr>
      <tr>
        <td>5</td>
        <td>LLM overconfidence rate</td>
        <td>84%</td>
        <td>PMC/12249208</td>
        <td>High</td>
        <td>Ch. 6</td>
      </tr>
      <tr>
        <td>6</td>
        <td>VCE bias</td>
        <td>"systematically biased"</td>
        <td>arXiv:2602.00279</td>
        <td>High</td>
        <td>Ch. 9</td>
      </tr>
      <tr>
        <td>7</td>
        <td>Air Canada chatbot liability</td>
        <td>~$800 + precedent</td>
        <td>Tribunal ruling 2024</td>
        <td>High</td>
        <td>Ch. 6</td>
      </tr>
      <tr>
        <td>8</td>
        <td>Klarna AI overpivot</td>
        <td>$60M savings, partial rollback</td>
        <td>CEO earnings call Q3 2025</td>
        <td>High (corporate)</td>
        <td>Ch. 6</td>
      </tr>
      <tr>
        <td>9</td>
        <td>Multi-agent hijacking success</td>
        <td>45–64%</td>
        <td>arXiv:2503.12188</td>
        <td>High</td>
        <td>Ch. 6</td>
      </tr>
      <tr>
        <td>10</td>
        <td>Budget-CoCoA cost</td>
        <td>$0.005/check</td>
        <td>Anthropic pricing</td>
        <td>High</td>
        <td>Ch. 9, 10</td>
      </tr>
      <tr>
        <td>11</td>
        <td>SOC alerts ignored</td>
        <td>67%</td>
        <td>Vectra 2023 (n=2,000)</td>
        <td>High</td>
        <td>Ch. 6, 10</td>
      </tr>
      <tr>
        <td>12</td>
        <td>Knight Capital loss</td>
        <td>$440M in 45 min</td>
        <td>SEC filing 2012</td>
        <td>High</td>
        <td>Ch. 6</td>
      </tr>
      <tr>
        <td>13</td>
        <td>EU AI Act penalty</td>
        <td>€35M / 7% revenue</td>
        <td>Legislative text</td>
        <td>High</td>
        <td>Ch. 2, 10</td>
      </tr>
    </table>
  </div>

  <p style="margin-top: 24px; font-size: 0.85rem; color: #666;">Top 5 claims with invalidation conditions:</p>

  <p style="font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 8px;"><strong>Claim 1 (McKinsey 6%):</strong> Invalidated if McKinsey's EBIT attribution methodology is shown to systematically undercount AI impact.</p>

  <p style="font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 8px;"><strong>Claim 2 (62% experimentation):</strong> Invalidated if the survey includes organizations already at Level 3+ due to unreported production deployments.</p>

  <p style="font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 8px;"><strong>Claim 9 (Multi-agent hijacking):</strong> Invalidated if newer agent frameworks (post-2025) implement inter-agent trust protocols that reduce success rates below 10%.</p>

  <p style="font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 8px;"><strong>Claim 10 (Budget-CoCoA cost):</strong> Invalidated if API pricing changes or if production-grade calibration requires additional infrastructure (database storage, logging) that significantly increases total cost.</p>

  <p style="font-size: 0.85rem; color: #555; line-height: 1.6;"><strong>Claim 13 (EU AI Act penalty):</strong> Invalidated if enforcement interpretation creates safe harbors or if penalties are reduced for first-time offenses.</p>
</div>

<!-- ========================================
     SECTION 14: REFERENCES
     ======================================== -->
<div class="page" id="references">
  <h2>14. References</h2>

  <p class="reference-entry">[1] McKinsey & Company (2025). "The State of AI in 2025." McKinsey Global Survey (n=1,993). Retrieved from mckinsey.com.</p>

  <p class="reference-entry">[2] McKinsey & Company (2025). "The State of AI in 2025" — agent experimentation and deployment data.</p>

  <p class="reference-entry">[3] Gartner (2025). "Predicts 2025: AI Agents" — projection on agentic project cancellation rates.</p>

  <p class="reference-entry">[4] Gartner (2024). "4 Levels of AI Maturity and How to Achieve Them" — pilot-to-production conversion rate.</p>

  <p class="reference-entry">[5] PMC/12249208 (2024). "Overconfidence in Large Language Models" — study of 9 LLMs across 351 scenarios.</p>

  <p class="reference-entry">[6] arXiv:2602.00279 (2026). "Verbalized Confidence Expressions in LLMs: Calibration and Reliability."</p>

  <p class="reference-entry">[7] Air Canada chatbot tribunal ruling (2024). Bereavement fare policy hallucination case.</p>

  <p class="reference-entry">[8] Klarna Q3 2025 Earnings Call. CEO Siemiatkowski statement on AI overpivot and partial rollback.</p>

  <p class="reference-entry">[9] arXiv:2503.12188 (2025). "Hijacking Attacks on Multi-Agent Systems" — 45–64% success rates across AutoGen, CrewAI, MetaGPT.</p>

  <p class="reference-entry">[10] Budget-CoCoA methodology; Anthropic pricing (verified February 2026). $0.005 per confidence check using 3× Haiku samples.</p>

  <p class="reference-entry">[11] Vectra AI (2023). "2023 State of Threat Detection" — survey of 2,000 SOC analysts. 67% alert ignore rate.</p>

  <p class="reference-entry">[12] Knight Capital SEC filing (2012). $440M loss in 45 minutes from erroneous automated orders.</p>

  <p class="reference-entry">[13] European Parliament (2024). "Regulation (EU) 2024/1689 — Artificial Intelligence Act." Articles 9, 14; penalty Article 99.</p>

  <p class="reference-entry">[14] Deloitte (2024). "AI Readiness Framework" — five stages of AI maturity.</p>

  <p class="reference-entry">[15] Microsoft (2024). "AI Maturity Model" — Azure AI adoption framework.</p>

  <p class="reference-entry">[16] Google Cloud (2024). "AI Adoption Framework" — three phases of AI transformation.</p>

  <p class="reference-entry">[17] IBM (2024). "AI Ladder" — four rungs of data-centric AI maturity.</p>

  <p class="reference-entry">[18] Gartner (2025). "40% of enterprise applications will incorporate agentic AI by end of 2026." Market forecast.</p>

  <p class="reference-entry">[19] Google (2025). "Agent-to-Agent (A2A) Protocol" — now Linux Foundation project. Protocol specification.</p>

  <p class="reference-entry">[20] Anthropic (2024). "Model Context Protocol (MCP)" — tool integration standard for AI agents.</p>

  <p class="reference-entry">[21] Market forecast: AI agent market from $7.8B (2024) to $52B (2030) at 45.8% CAGR. Multiple analyst sources.</p>

  <p style="margin-top: 24px; padding-top: 16px; border-top: 1px solid #eee; font-size: 0.8rem; color: #555; font-style: italic;">
    <strong>Citation:</strong> Ainary Research (2026). The AI Agent Maturity Model: A Framework for Measuring How Ready Your Organization Actually Is. AR-004.
  </p>
</div>

<!-- ========================================
     AUTHOR BIO
     ======================================== -->
<div class="page">
  <div class="author-section">
    <p class="author-label">About the Author</p>
    <p class="author-bio">Florian Ziesche is the founder of Ainary Ventures, where AI does 80% of the research and humans do the 20% that matters. Before Ainary, he was CEO of 36ZERO Vision and advised startups and SMEs on AI strategy and due diligence. His conviction: HUMAN × AI = LEVERAGE. This report is the proof.</p>
    <p style="margin-top: 12px; font-size: 0.85rem; color: #888;">ainaryventures.com</p>
  </div>
</div>

<!-- ========================================
     BACK COVER
     ======================================== -->
<div class="back-cover">
  <div class="cover-brand" style="margin-bottom: 24px;">
    <span class="gold-punkt">●</span>
    <span class="brand-name">Ainary</span>
  </div>

  <p class="back-cover-services">AI Strategy · Published Research · Daily Intelligence</p>

  <p class="back-cover-cta">
    <a href="mailto:florian@ainaryventures.com">Contact</a> · <a href="mailto:florian@ainaryventures.com?subject=Feedback: AR-004">Feedback</a>
  </p>

  <p class="back-cover-contact">
    ainaryventures.com<br>
    florian@ainaryventures.com
  </p>

  <p style="margin-top: 48px; font-size: 0.75rem; color: #888;">
    © 2026 Ainary Ventures
  </p>
</div>

</body>
</html>