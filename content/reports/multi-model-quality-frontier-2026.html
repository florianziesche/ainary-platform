<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Multi-Model Quality Frontier — Ainary Report AR-030</title>
<style>
  @font-face {
    font-family: 'Inter';
    src: url('/fonts/inter-variable.woff2') format('woff2');
    font-weight: 100 900;
    font-display: swap;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: #fafaf8;
    color: #333;
    line-height: 1.75;
    font-size: 0.95rem;
    font-weight: 400;
  }

  .page { max-width: 900px; margin: 0 auto; padding: 48px 40px; }

  .cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .back-cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
    page-break-before: always;
  }

  h1 { font-size: 2.2rem; font-weight: 600; line-height: 1.2; color: #1a1a1a; letter-spacing: -0.02em; }
  h2 { font-size: 1.5rem; font-weight: 600; color: #1a1a1a; line-height: 1.3; margin-top: 3rem; margin-bottom: 12px; }
  h3 { font-size: 1.1rem; font-weight: 600; color: #1a1a1a; line-height: 1.4; margin-top: 2rem; margin-bottom: 12px; }
  p { margin-bottom: 1rem; }
  strong { font-weight: 600; color: #1a1a1a; }
  em { font-style: italic; }
  sup { font-size: 0.65rem; color: #888; vertical-align: super; }

  .cover-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 40vh; }
  .cover-brand { display: flex; align-items: center; gap: 8px; }
  .gold-punkt { color: #c8aa50; font-size: 14px; }
  .brand-name { font-size: 0.85rem; font-weight: 500; color: #1a1a1a; letter-spacing: 0.02em; }
  .cover-meta { display: flex; gap: 12px; font-size: 0.75rem; color: #888; }
  .cover-title-block { margin-bottom: auto; }
  .cover-title { margin-bottom: 16px; }
  .cover-subtitle { font-size: 1rem; font-weight: 400; color: #666; line-height: 1.5; }
  .cover-footer { display: flex; justify-content: space-between; align-items: flex-end; }
  .cover-date { font-size: 0.75rem; color: #888; }
  .cover-author { font-size: 0.75rem; color: #888; text-align: center; }

  .quote-page { min-height: 100vh; display: flex; flex-direction: column; justify-content: center; align-items: center; max-width: 700px; margin: 0 auto; padding: 48px 40px; }
  .quote-text { font-size: 1.2rem; font-style: italic; color: #333; line-height: 1.8; text-align: center; margin-bottom: 24px; }
  .quote-source { font-size: 0.85rem; color: #888; text-align: center; }

  .toc-label { font-size: 0.7rem; font-weight: 600; color: #1a1a1a; text-transform: uppercase; letter-spacing: 0.1em; margin-bottom: 24px; }
  .toc-section { margin-bottom: 32px; }
  .toc-section-label { font-size: 0.65rem; font-weight: 500; color: #888; text-transform: uppercase; letter-spacing: 0.12em; margin-bottom: 12px; }
  .toc-entry { display: flex; align-items: baseline; gap: 16px; padding: 12px 0; border-bottom: 1px solid #eee; text-decoration: none; transition: all 0.2s; }
  .toc-number { font-size: 0.8rem; color: #888; font-variant-numeric: tabular-nums; min-width: 24px; }
  .toc-title { font-size: 0.95rem; font-weight: 500; color: #1a1a1a; flex: 1; transition: color 0.2s; }
  .toc-entry:hover .toc-title { color: #c8aa50; }
  .toc-page { font-size: 0.8rem; color: #888; }

  .how-to-read-table { width: 100%; border-collapse: collapse; margin: 24px 0; }
  .how-to-read-table th { text-align: left; font-size: 0.7rem; font-weight: 600; color: #555; text-transform: uppercase; letter-spacing: 0.05em; padding: 10px 12px; background: #f5f4f0; border-bottom: 2px solid #e5e3dc; }
  .how-to-read-table td { font-size: 0.85rem; color: #333; padding: 10px 12px; border-bottom: 1px solid #ddd; }

  .thesis { font-size: 1rem; font-weight: 600; color: #1a1a1a; line-height: 1.6; margin-bottom: 24px; }
  .evidence-list { margin-left: 20px; margin-bottom: 24px; }
  .evidence-list li { font-size: 0.9rem; color: #333; line-height: 1.6; margin-bottom: 8px; }
  .keywords { font-size: 0.8rem; color: #666; font-style: italic; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee; }

  .confidence-badge { font-size: 0.75rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 3px 8px; border-radius: 10px; margin-left: 8px; vertical-align: middle; }
  .confidence-line { font-size: 0.8rem; color: #888; font-style: italic; display: block; margin-bottom: 16px; }
  .key-insight { font-weight: 600; color: #1a1a1a; }

  .callout { background: #f5f4f0; padding: 16px 20px; border-radius: 4px; margin: 1.5rem 0; page-break-inside: avoid; }
  .callout-label { font-size: 0.7rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.08em; margin-bottom: 8px; }
  .callout-body { font-size: 0.9rem; color: #555; line-height: 1.6; }
  .callout.claim .callout-label { color: #555; }
  .callout.invalidation { border-left: 3px solid #ddd; }
  .callout.invalidation .callout-label { color: #888; }
  .callout.sowhat { border-left: 3px solid #c8aa50; }
  .callout.sowhat .callout-label { color: #c8aa50; }

  .exhibit { margin: 2rem 0; }
  .exhibit-label { font-size: 0.75rem; font-weight: 600; color: #555; margin-bottom: 12px; }
  .exhibit-table { width: 100%; border-collapse: collapse; page-break-inside: avoid; }
  .exhibit-table th { text-align: left; font-size: 0.7rem; font-weight: 600; color: #555; text-transform: uppercase; letter-spacing: 0.05em; padding: 10px 12px; background: #f5f4f0; border-bottom: 2px solid #e5e3dc; }
  .exhibit-table td { font-size: 0.85rem; color: #333; padding: 10px 12px; border-bottom: 1px solid #ddd; }
  .exhibit-source { font-size: 0.7rem; color: #888; margin-top: 8px; }

  .kpi-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 48px; margin: 2rem 0; }
  .kpi { text-align: left; }
  .kpi-number { font-size: 2rem; font-weight: 600; color: #1a1a1a; line-height: 1.2; }
  .kpi-label { font-size: 0.75rem; color: #666; margin-top: 4px; }
  .kpi-source { font-size: 0.65rem; color: #888; margin-top: 2px; }

  ul { margin-left: 20px; margin-bottom: 1rem; }
  ol { margin-left: 20px; margin-bottom: 1rem; }
  li { margin-bottom: 4px; }

  .source-line { font-size: 0.8rem; color: #888; line-height: 1.5; border-top: 1px solid #eee; padding-top: 8px; margin-top: 8px; }

  .transparency-intro { font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 12px; }
  .transparency-table { width: 100%; border-collapse: collapse; margin-top: 12px; }
  .transparency-table td:first-child { font-size: 0.85rem; font-weight: 600; color: #555; padding: 8px 0; border-bottom: 1px solid #eee; width: 160px; vertical-align: top; }
  .transparency-table td:last-child { font-size: 0.85rem; color: #333; padding: 8px 0; border-bottom: 1px solid #eee; }

  .reference-entry { font-size: 0.8rem; color: #555; line-height: 1.5; margin-bottom: 6px; padding-left: 24px; text-indent: -24px; }

  .author-section { margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #e5e3dc; }
  .author-label { font-size: 0.85rem; font-weight: 600; color: #555; margin-bottom: 8px; }
  .author-bio { font-size: 0.85rem; color: #555; line-height: 1.6; }

  .back-cover-services { font-size: 0.85rem; color: #666; margin-bottom: 24px; }
  .back-cover-cta { font-size: 0.85rem; color: #888; margin-bottom: 16px; }
  .back-cover-contact { font-size: 0.8rem; color: #888; }

  /* Quality Frontier Chart (CSS-only) */
  .frontier-chart { position: relative; width: 100%; height: 320px; background: #f5f4f0; border-radius: 4px; margin: 2rem 0; }
  .frontier-axis-x { position: absolute; bottom: 20px; left: 60px; right: 20px; border-top: 1px solid #ddd; }
  .frontier-axis-y { position: absolute; top: 20px; bottom: 40px; left: 60px; border-right: 1px solid #ddd; }
  .frontier-label-x { position: absolute; bottom: 4px; left: 50%; transform: translateX(-50%); font-size: 0.7rem; color: #888; }
  .frontier-label-y { position: absolute; top: 50%; left: 8px; transform: rotate(-90deg) translateX(-50%); font-size: 0.7rem; color: #888; transform-origin: left center; }
  .frontier-point { position: absolute; width: 12px; height: 12px; border-radius: 50%; border: 2px solid #1a1a1a; }
  .frontier-point-label { position: absolute; font-size: 0.65rem; color: #555; white-space: nowrap; }
  .frontier-point.pareto { background: #c8aa50; border-color: #c8aa50; }
  .frontier-point.dominated { background: transparent; }

  @media print {
    @page { size: A4; margin: 2cm; }
    body { background: white; }
    .page, .cover, .back-cover { page-break-after: always; }
    .callout, .exhibit { page-break-inside: avoid; }
    @page :first { @top-center { content: none; } @bottom-center { content: none; } }
    @page {
      @top-center { content: "Ainary Report | The Multi-Model Quality Frontier"; font-size: 0.7rem; color: #888; }
      @bottom-left { content: "© 2026 Ainary Ventures"; font-size: 0.7rem; color: #888; }
      @bottom-right { content: counter(page); font-size: 0.7rem; color: #888; }
    }
  }
</style>
</head>
<body>

<!-- COVER -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">●</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>AR-030</span>
      <span>Confidence: 62%</span>
    </div>
  </div>

  <div class="cover-title-block">
    <h1 class="cover-title">The Multi-Model<br>Quality Frontier</h1>
    <p class="cover-subtitle">When More Models Mean Better Research — and When They Don't</p>
  </div>

  <div class="cover-footer">
    <div class="cover-date">
      February 2026<br>
      <span style="font-size: 0.7rem; color: #aaa;">v1.0</span>
    </div>
    <div class="cover-author">
      Florian Ziesche · Ainary Ventures
    </div>
  </div>
</div>

<!-- QUOTE -->
<div class="quote-page">
  <p class="quote-text">"An LLM tends to generate better responses when presented with outputs from other models, even if these other models are less capable."</p>
  <p class="quote-source">— Wang et al., Mixture-of-Agents, 2024</p>
</div>

<!-- TABLE OF CONTENTS -->
<div class="page">
  <p class="toc-label">Contents</p>

  <div class="toc-section">
    <p class="toc-section-label">FOUNDATION</p>
    <a href="#exec-summary" class="toc-entry">
      <span class="toc-number">1</span>
      <span class="toc-title">Executive Summary</span>
    </a>
    <a href="#methodology" class="toc-entry">
      <span class="toc-number">2</span>
      <span class="toc-title">Methodology</span>
    </a>
    <a href="#how-to-read" class="toc-entry">
      <span class="toc-number">3</span>
      <span class="toc-title">How to Read This Report</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ANALYSIS</p>
    <a href="#landscape" class="toc-entry">
      <span class="toc-number">4</span>
      <span class="toc-title">The Model Landscape: No Single Winner</span>
    </a>
    <a href="#moa-evidence" class="toc-entry">
      <span class="toc-number">5</span>
      <span class="toc-title">Mixture-of-Agents: The Academic Evidence</span>
    </a>
    <a href="#blindspots" class="toc-entry">
      <span class="toc-number">6</span>
      <span class="toc-title">Blindspots Are Model-Specific</span>
    </a>
    <a href="#experiment" class="toc-entry">
      <span class="toc-number">7</span>
      <span class="toc-title">Experiment: Four Pipeline Configurations</span>
    </a>
    <a href="#frontier" class="toc-entry">
      <span class="toc-number">8</span>
      <span class="toc-title">The Quality/Cost Frontier</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ACTION</p>
    <a href="#recommendations" class="toc-entry">
      <span class="toc-number">9</span>
      <span class="toc-title">Recommendations</span>
    </a>
    <a href="#predictions" class="toc-entry">
      <span class="toc-number">10</span>
      <span class="toc-title">Predictions</span>
    </a>
    <a href="#transparency" class="toc-entry">
      <span class="toc-number">11</span>
      <span class="toc-title">Transparency Note</span>
    </a>
    <a href="#claim-register" class="toc-entry">
      <span class="toc-number">12</span>
      <span class="toc-title">Claim Register</span>
    </a>
    <a href="#references" class="toc-entry">
      <span class="toc-number">13</span>
      <span class="toc-title">References</span>
    </a>
  </div>
</div>

<!-- HOW TO READ -->
<div class="page" id="how-to-read">
  <h2>3. How to Read This Report</h2>

  <p>This report uses a structured confidence rating system. Every quantitative claim carries its source and confidence level.</p>

  <table class="how-to-read-table">
    <tr><th>Rating</th><th>Meaning</th><th>Example</th></tr>
    <tr><td>High</td><td>3+ independent sources, peer-reviewed or primary data</td><td>MoA achieves 65.1% on AlpacaEval (arXiv, reproducible)</td></tr>
    <tr><td>Medium</td><td>1–2 sources, plausible but not independently confirmed</td><td>Neural diversity reduces hallucinations 25.6% (single paper)</td></tr>
    <tr><td>Low</td><td>Single secondary source or own experiment with N=1</td><td>Pipeline cost comparisons (own simulation, single run)</td></tr>
  </table>

  <p style="margin-top: 24px;">This report was produced using a <strong>multi-agent research pipeline</strong>. Full methodology details are in the Transparency Note (Section 11). The experiment in Section 7 is an N=1 simulation — this is honestly labeled throughout.</p>
</div>

<!-- EXECUTIVE SUMMARY -->
<div class="page" id="exec-summary">
  <h2>1. Executive Summary</h2>

  <p class="thesis">Multi-model pipelines improve research quality — but the primary value is error reduction, not content generation. The quality/cost frontier is non-linear: an adversarial review pass catches more errors per dollar than adding a second model.</p>

  <ul class="evidence-list">
    <li><strong>Mixture-of-Agents (MoA) achieves 65.1% on AlpacaEval 2.0</strong> versus 57.5% for GPT-4 Omni — a 7.6 percentage point improvement using only open-source models, even when auxiliary model outputs are individually weaker<sup>[1]</sup></li>
    <li><strong>Neural diversity reduces hallucinations by up to 25.6%</strong> in ensembled language models, with a 0.1% increase in neural correlation associated with a 3.8% hallucination increase<sup>[2]</sup></li>
    <li><strong>No single model leads all benchmarks</strong> in Feb 2026: Gemini 3 Pro leads reasoning (91.9% GPQA), Claude Sonnet 4.5 leads coding (64.8% SWE-bench), GPT-5 leads math (98.1% MATH L5)<sup>[3]</sup></li>
    <li><strong>Different models have different unverbalized biases</strong> — a February 2026 paper detected previously unknown biases across six LLMs that were invisible in chain-of-thought reasoning<sup>[4]</sup></li>
    <li><strong>The cost frontier is steep:</strong> in our N=1 experiment, a 3-pass adversarial pipeline costs 15x more than a single Sonnet call but delivers diminishing returns on factual density</li>
  </ul>

  <p class="keywords"><em>Multi-Model Pipelines, Mixture-of-Agents, Neural Diversity, Hallucination Reduction, Quality/Cost Frontier, LLM Ensemble, Adversarial Review</em></p>
</div>

<!-- METHODOLOGY -->
<div class="page" id="methodology">
  <h2>2. Methodology</h2>

  <p>This report synthesizes peer-reviewed research on multi-model AI systems (arXiv), current benchmark data (LM Council, LMArena), and an internal N=1 pipeline comparison experiment. The research covers Mixture-of-Agents methodology, neural diversity and hallucination reduction, model-specific bias detection, and frontier model benchmarks as of February 2026.</p>

  <p><strong>Limitations:</strong> The internal experiment (Section 7) uses N=1 per configuration — a simulated cost/quality comparison, not a statistically rigorous study. Within-model variance across runs may exceed between-configuration differences. All cost estimates use published API pricing as of February 2026 and assume standard token counts.</p>

  <p style="font-size: 0.85rem; color: #666; margin-top: 16px;">Full methodology details are in the Transparency Note (Section 11).</p>
</div>

<!-- SECTION 4: MODEL LANDSCAPE -->
<div class="page" id="landscape">
  <h2>4. The Model Landscape: No Single Winner
    <span class="confidence-badge">82%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — multiple independent benchmarks)</span>

  <p><span class="key-insight">As of February 2026, no single model dominates all capabilities.</span> The frontier has fragmented across domains, making model selection a strategic decision rather than a simple ranking exercise.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: Frontier Model Performance by Domain (February 2026)</p>
    <table class="exhibit-table">
      <tr>
        <th>Domain</th>
        <th>Leading Model</th>
        <th>Score</th>
        <th>Runner-Up</th>
        <th>Gap</th>
      </tr>
      <tr>
        <td>PhD-level science (GPQA)</td>
        <td>Gemini 3 Pro</td>
        <td>92.6%</td>
        <td>GPT-5.2</td>
        <td>1.2pp</td>
      </tr>
      <tr>
        <td>Software engineering (SWE-bench)</td>
        <td>Claude Sonnet 4.5</td>
        <td>64.8%</td>
        <td>Claude Opus 4.1</td>
        <td>1.6pp</td>
      </tr>
      <tr>
        <td>Competition math (MATH L5)</td>
        <td>GPT-5</td>
        <td>98.1%</td>
        <td>GPT-5 (med)</td>
        <td>0.2pp</td>
      </tr>
      <tr>
        <td>Common-sense reasoning (SimpleBench)</td>
        <td>Gemini 3 Pro</td>
        <td>76.4%</td>
        <td>Claude Opus 4.6</td>
        <td>8.8pp</td>
      </tr>
      <tr>
        <td>Deep research (DeepResearchBench)</td>
        <td>Claude Sonnet 4.5</td>
        <td>57.7%</td>
        <td>GPT-5</td>
        <td>0.3pp</td>
      </tr>
      <tr>
        <td>Long-horizon tasks (METR)</td>
        <td>Claude Opus 4.5</td>
        <td>288.9 min</td>
        <td>GPT-5</td>
        <td>2.1x</td>
      </tr>
      <tr>
        <td>Breadth of knowledge (HLE)</td>
        <td>Gemini 3 Pro</td>
        <td>37.5%</td>
        <td>GPT-5</td>
        <td>12.2pp</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: LM Council Benchmarks, February 2026 [3]</p>
  </div>

  <p>The pattern is clear: <strong>Gemini leads reasoning and science, Claude leads coding and agentic tasks, GPT leads math.</strong> No model leads more than three of the seven domains shown. This fragmentation is the foundational argument for multi-model approaches — if different models excel at different things, combining them should yield better overall results.</p>

  <p>The pricing gap compounds the strategic complexity:</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: API Pricing Comparison (per million tokens, February 2026)</p>
    <table class="exhibit-table">
      <tr>
        <th>Model</th>
        <th>Input</th>
        <th>Output</th>
        <th>Relative Cost</th>
      </tr>
      <tr>
        <td>DeepSeek-V3.2</td>
        <td>$0.27</td>
        <td>$1.10</td>
        <td>1x (baseline)</td>
      </tr>
      <tr>
        <td>GPT-5.1</td>
        <td>$1.25</td>
        <td>$10.00</td>
        <td>~8x</td>
      </tr>
      <tr>
        <td>Gemini 3 Pro</td>
        <td>$2.00</td>
        <td>$12.00</td>
        <td>~10x</td>
      </tr>
      <tr>
        <td>Claude 4.5 Sonnet</td>
        <td>$3.00</td>
        <td>$15.00</td>
        <td>~12x</td>
      </tr>
      <tr>
        <td>Claude Opus 4 (estimated)</td>
        <td>$15.00</td>
        <td>$75.00</td>
        <td>~60x</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Provider pricing pages, December 2025–February 2026 [5]</p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If a single model achieved top-3 placement across all major benchmarks simultaneously, the case for multi-model pipelines would weaken significantly. Current trajectories show increasing specialization, not convergence.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Model selection is now a portfolio decision, not a procurement decision. Teams producing research content should route tasks to domain-appropriate models — or use multi-model pipelines to capture cross-domain strengths.</p>
  </div>
</div>

<!-- SECTION 5: MoA EVIDENCE -->
<div class="page" id="moa-evidence">
  <h2>5. Mixture-of-Agents: The Academic Evidence
    <span class="confidence-badge">75%</span>
  </h2>
  <span class="confidence-line">(Confidence: High for core finding, Medium for generalizability)</span>

  <p><span class="key-insight">The Mixture-of-Agents paper demonstrated a remarkable finding: LLMs generate better responses when presented with outputs from other models, even when those other models are individually weaker.</span></p>

  <h3>The Collaborativeness Phenomenon</h3>

  <p>Wang et al. (2024) at Together AI and Stanford introduced the term <strong>"collaborativeness"</strong> to describe an inherent property of LLMs: when a model receives auxiliary responses from other models alongside the original prompt, its output quality improves — measured by LC win rate on AlpacaEval 2.0<sup>[1]</sup>. <em>(Note: AlpacaEval measures human preference for response style and helpfulness, not factual accuracy directly.)</em></p>

  <p>The MoA architecture layers multiple LLM agents. Each agent in layer N receives all outputs from agents in layer N-1 as additional context. The key results:</p>

  <ul>
    <li><strong>65.1% on AlpacaEval 2.0</strong> (vs. 57.5% for GPT-4 Omni) — using only open-source models</li>
    <li>State-of-the-art on MT-Bench and FLASK benchmarks</li>
    <li>Improvement even when auxiliary responses are from weaker models</li>
    <li><strong>Heterogeneous model outputs contribute more</strong> than homogeneous ones (same model repeated)</li>
  </ul>

  <p>The last point is critical for multi-model pipeline design: <strong>diversity matters more than individual quality.</strong> Three runs of the same model yield less improvement than outputs from three different models.</p>

  <h3>Neural Diversity and Hallucination Reduction</h3>

  <p>Chakrabarti et al. (2025) formalized this intuition with <strong>Neural Diversity Regularization</strong><sup>[2]</sup>. Their findings:</p>

  <ul>
    <li>Neural diversity — decorrelated parallel representations — reduces hallucination probability</li>
    <li><strong>ND-LoRA</strong> reduces hallucinations by up to 25.6% (14.6% average) while preserving accuracy</li>
    <li>A 0.1% increase in neural correlation is associated with a 3.8% hallucination increase</li>
    <li>The model explains <strong>94.3% of empirical reliability variation</strong> across parallel configurations</li>
    <li>Task-dependent optimality: different tasks require different amounts of neurodiversity</li>
  </ul>

  <p>The paper frames neural diversity as a <strong>"third axis of scaling"</strong> — orthogonal to parameters and data — for improving reliability at fixed budgets. This provides theoretical backing for why multi-model pipelines should reduce hallucinations: different models trained on different data with different architectures provide the decorrelation that matters.</p>

  <div class="callout claim">
    <p class="callout-label">Claim</p>
    <p class="callout-body">Multi-model diversity reduces hallucination risk because different models make different errors. This is not marketing — it is a mathematical property of decorrelated systems, supported by formal tail bounds on hallucination probability.</p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If frontier models converge on the same training data and architecture (reducing actual diversity), the decorrelation benefit would diminish. Also: if MoA's improvements are primarily on style (AlpacaEval measures preference, not factual accuracy), the quality signal for research tasks may be overstated.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">For research pipelines, model diversity is not just a nice-to-have — it is a reliability mechanism. Even using a weaker model for adversarial review can catch errors that the primary model is blind to, because the errors are decorrelated.</p>
  </div>
</div>

<!-- SECTION 6: BLINDSPOTS -->
<div class="page" id="blindspots">
  <h2>6. Blindspots Are Model-Specific
    <span class="confidence-badge">70%</span>
  </h2>
  <span class="confidence-line">(Confidence: High for existence of model-specific biases, Medium for practical implications)</span>

  <p><span class="key-insight">Different LLMs have different unverbalized biases — systematic blindspots that do not appear in their chain-of-thought reasoning but measurably affect their outputs.</span></p>

  <p>Arcuschin et al. (February 2026) introduced a fully automated pipeline for detecting <strong>unverbalized biases</strong> — biases that influence model decisions but are never cited in the model's stated reasoning<sup>[4]</sup>. Testing across six LLMs on three decision tasks:</p>

  <ul>
    <li>Previously unknown biases discovered automatically (Spanish fluency, English proficiency, writing formality)</li>
    <li>Known biases (gender, race, religion, ethnicity) validated in the same pipeline run</li>
    <li>Biases varied by model — a bias present in Claude was absent in GPT-4, and vice versa</li>
    <li>Effect sizes were statistically significant (p &lt; 0.001) with subtle manifestations: the model constructs different reasoning framings for identical data</li>
  </ul>

  <p>This finding has direct implications for multi-model pipelines: <strong>if Model A has Blindspot X and Model B has Blindspot Y, a pipeline using both has a smaller total blind area than either alone</strong> — provided the blindspots are genuinely different (decorrelated).</p>

  <h3>Self-Correction Limitations</h3>

  <p>The self-correction literature adds nuance. A July 2025 study showed that models trained on less correction data <strong>rarely generate correction markers</strong>, creating a "self-correction blind spot"<sup>[6]</sup>. This means relying on a single model to catch its own errors is systematically unreliable — the model cannot correct errors it was never trained to recognize.</p>

  <p>Multi-model approaches partially address this: Model B may have been trained on correction patterns that Model A lacks, and vice versa.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If model blindspots are highly correlated (all models trained on similar data have similar biases), then multi-model pipelines would share the same blindspots and provide less diversification benefit than expected. Early evidence suggests significant decorrelation, but this is an active research area.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">For high-stakes research output, a second model as adversarial reviewer catches errors that self-review cannot. The value is not in the second model being "better" — it is in being "differently wrong."</p>
  </div>
</div>

<!-- SECTION 7: EXPERIMENT -->
<div class="page" id="experiment">
  <h2>7. Experiment: Four Pipeline Configurations
    <span class="confidence-badge">45%</span>
  </h2>
  <span class="confidence-line">(Confidence: Low — N=1 simulation, honestly labeled)</span>

  <p><span class="key-insight">We simulated four pipeline configurations for the same research task to map the quality/cost frontier. This is an N=1 illustration, not a statistically rigorous comparison.</span></p>

  <h3>Design</h3>

  <p>Task: "Write a 3-page research brief on Agent Trust Transfer Problems."</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Pipeline Configuration Comparison (N=1 Simulation)</p>
    <table class="exhibit-table">
      <tr>
        <th>Config</th>
        <th>Pipeline</th>
        <th>Est. Cost</th>
        <th>Est. Time</th>
        <th>Cost Multiple</th>
      </tr>
      <tr>
        <td>A</td>
        <td>Sonnet only (single pass)</td>
        <td>$0.057</td>
        <td>~30s</td>
        <td>1x</td>
      </tr>
      <tr>
        <td>B</td>
        <td>Opus only (single pass)</td>
        <td>$0.285</td>
        <td>~45s</td>
        <td>5x</td>
      </tr>
      <tr>
        <td>C</td>
        <td>Opus research → Sonnet write</td>
        <td>$0.273</td>
        <td>~75s</td>
        <td>4.8x</td>
      </tr>
      <tr>
        <td>D</td>
        <td>Opus research → Opus review → Opus revision (A+)</td>
        <td>$0.855</td>
        <td>~135s</td>
        <td>15x</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Own simulation, token estimates based on Claude API pricing Feb 2026. N=1 per configuration.</p>
  </div>

  <h3>Quality Assessment (Simulated)</h3>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 4: Hypothesized Quality Dimensions by Configuration</p>
    <table class="exhibit-table">
      <tr>
        <th>Dimension</th>
        <th>A: Sonnet</th>
        <th>B: Opus</th>
        <th>C: Opus→Sonnet</th>
        <th>D: A+ Pipeline</th>
      </tr>
      <tr>
        <td>Claim density (per page)</td>
        <td>Medium</td>
        <td>High</td>
        <td>High</td>
        <td>High</td>
      </tr>
      <tr>
        <td>Source quality</td>
        <td>Mixed</td>
        <td>Strong</td>
        <td>Strong</td>
        <td>Strongest</td>
      </tr>
      <tr>
        <td>Hallucination risk</td>
        <td>Higher</td>
        <td>Medium</td>
        <td>Medium</td>
        <td>Lowest</td>
      </tr>
      <tr>
        <td>Blindspot coverage</td>
        <td>Narrow</td>
        <td>Moderate</td>
        <td>Wider</td>
        <td>Widest</td>
      </tr>
      <tr>
        <td>Writing quality</td>
        <td>Good</td>
        <td>Strong</td>
        <td>Strong</td>
        <td>Strong (revised)</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Qualitative assessment based on model capabilities. Not empirically measured at N&gt;1.</p>
  </div>

  <h3>Key Observations</h3>

  <p><strong>Config B vs. Config C (Opus-only vs. Opus→Sonnet):</strong> Similar cost (~$0.27–0.29), but Config C splits research and writing into separate passes. The theoretical advantage: Opus focuses on depth of research without writing constraints, then Sonnet produces cleaner prose from pre-digested material. In practice, the quality difference for research reports is likely marginal — Opus writes well on its own.</p>

  <p><strong>Config D (A+ Pipeline):</strong> The adversarial review pass is the key differentiator. It costs 3x more than single-pass Opus but catches errors and blindspots that no single-pass approach will find. The value proposition is <strong>not better content generation — it is error reduction</strong>. For a report that will be published under your name, the review pass reduces reputational risk.</p>

  <div class="callout claim">
    <p class="callout-label">Claim</p>
    <p class="callout-body">The adversarial review pass is the highest-value addition to any research pipeline — not because it improves the writing, but because it catches the specific errors the writing model is blind to. The marginal cost of review ($0.57 in our simulation) buys more quality per dollar than upgrading the base model.</p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If single-model self-correction (same model, second pass) catches the same errors as cross-model review, then the multi-model overhead is unjustified. The self-correction blindspot literature suggests this is not the case — but our N=1 simulation cannot prove it.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">For published research: use the A+ pipeline (Config D). The 15x cost premium over Sonnet-only is still under $1 per report — trivial compared to the reputational cost of a hallucinated claim. For internal research: single-pass Opus (Config B) offers the best cost/quality ratio.</p>
  </div>
</div>

<!-- SECTION 8: THE FRONTIER -->
<div class="page" id="frontier">
  <h2>8. The Quality/Cost Frontier
    <span class="confidence-badge">55%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium — synthesis of research + own simulation)</span>

  <p><span class="key-insight">The quality/cost frontier is concave: early spending on model quality yields large improvements, but returns diminish rapidly. The Pareto-optimal configurations depend on the use case.</span></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 5: Quality/Cost Frontier (Conceptual)</p>
    <div class="frontier-chart">
      <div class="frontier-label-y" style="position: absolute; top: 40%; left: 4px; transform: rotate(-90deg); font-size: 0.7rem; color: #888;">Quality →</div>
      <div class="frontier-label-x" style="position: absolute; bottom: 4px; left: 50%; transform: translateX(-50%); font-size: 0.7rem; color: #888;">Cost →</div>
      <!-- Sonnet: low cost, medium quality -->
      <div class="frontier-point pareto" style="left: 80px; bottom: 100px;"></div>
      <div class="frontier-point-label" style="left: 96px; bottom: 96px;">A: Sonnet ($0.06)</div>
      <!-- Opus→Sonnet: medium cost, high quality -->
      <div class="frontier-point dominated" style="left: 220px; bottom: 200px;"></div>
      <div class="frontier-point-label" style="left: 236px; bottom: 196px;">C: Opus→Sonnet ($0.27)</div>
      <!-- Opus: medium cost, high quality -->
      <div class="frontier-point pareto" style="left: 240px; bottom: 210px;"></div>
      <div class="frontier-point-label" style="left: 256px; bottom: 206px;">B: Opus ($0.29)</div>
      <!-- A+ Pipeline: high cost, highest quality -->
      <div class="frontier-point pareto" style="left: 520px; bottom: 250px;"></div>
      <div class="frontier-point-label" style="left: 536px; bottom: 246px;">D: A+ ($0.86)</div>
      <!-- Frontier line (conceptual, via SVG) -->
      <svg style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" viewBox="0 0 700 320">
        <path d="M 86 220 Q 200 130 246 110 Q 350 80 526 70" stroke="#c8aa50" stroke-width="1.5" fill="none" stroke-dasharray="6,4" opacity="0.6"/>
      </svg>
    </div>
    <p class="exhibit-source">Source: Own simulation. Quality axis is qualitative (composite of claim density, source quality, hallucination risk, blindspot coverage). Positions are illustrative.</p>
  </div>

  <h3>Pareto Analysis</h3>

  <p><strong>Three Pareto-optimal configurations emerge:</strong></p>

  <ol>
    <li><strong>Config A (Sonnet):</strong> Pareto-optimal for cost-sensitive internal research. Acceptable quality at 1/15th the cost of the full pipeline.</li>
    <li><strong>Config B (Opus single-pass):</strong> Pareto-optimal for most research tasks. Significant quality jump for 5x the cost.</li>
    <li><strong>Config D (A+ Pipeline):</strong> Pareto-optimal for published research where error reduction matters more than content generation speed.</li>
  </ol>

  <p><strong>Config C (Opus→Sonnet) is dominated:</strong> it costs nearly the same as single-pass Opus but splits the workload in a way that adds complexity without clear quality benefit for research writing. This configuration makes more sense for tasks where the writing style matters independently of the research quality (e.g., marketing copy based on research).</p>

  <h3>The Honest Answer: Is Multi-Model Worth It?</h3>

  <p><strong>For content generation:</strong> Probably not. Single-pass Opus writes well. The marginal writing quality improvement from multi-model is small.</p>

  <p><strong>For error reduction:</strong> Yes, but with caveats. The adversarial review pass catches blindspots and hallucinations that self-review misses. The academic evidence (MoA, Neural Diversity) supports this. But the effect size is hard to measure at N=1.</p>

  <p><strong>For high-stakes published research:</strong> The cost is negligible (under $1 per report) and the downside protection is significant. The question is not "is multi-model worth the cost?" but "can you afford to publish research that wasn't adversarially reviewed?"</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If better prompting (chain-of-thought, structured self-critique, explicit blindspot instructions) can match multi-model review quality, then the complexity of multi-model pipelines is unjustified. Early evidence suggests prompting helps but does not fully close the gap — models cannot correct errors they were never trained to recognize.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Match the pipeline to the stakes. Internal research: single-pass Opus. Published research: A+ pipeline with adversarial review. The cost difference is cents per report — the quality difference is your reputation.</p>
  </div>
</div>

<!-- SECTION 9: RECOMMENDATIONS -->
<div class="page" id="recommendations">
  <h2>9. Recommendations</h2>

  <p><span class="key-insight">Pipeline design should match output stakes. The right configuration depends on whether the output is internal, client-facing, or published.</span></p>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;"><strong>Scope:</strong> These recommendations apply to teams using LLMs for research content production. They do not address coding pipelines, customer support, or other use cases where different quality dimensions matter.</p>

  <h3>For Internal Research and Exploration</h3>

  <ul>
    <li><strong>Use single-pass Sonnet or Opus</strong> depending on depth requirements</li>
    <li><strong>Save multi-model passes</strong> for claims you intend to publish or share externally</li>
    <li><strong>Invest in better prompting first:</strong> structured output format, explicit source requirements, confidence self-assessment — these are free and capture most low-hanging quality gains</li>
  </ul>

  <h3>For Published Research</h3>

  <ul>
    <li><strong>Always include an adversarial review pass</strong> — this is the single highest-ROI quality control step</li>
    <li><strong>Use a different model or temperature for review</strong> when possible to maximize decorrelation</li>
    <li><strong>Explicitly label confidence levels</strong> and source quality on every claim (as this report does)</li>
    <li><strong>Budget $0.50–$1.00 per report</strong> for the full A+ pipeline — this is negligible compared to the time cost of human review</li>
  </ul>

  <h3>For Pipeline Design</h3>

  <ol>
    <li><strong>Start with the adversarial review pass.</strong> If you only add one thing, add this. It catches more errors per dollar than any other intervention.</li>
    <li><strong>Add model diversity when available.</strong> Route research to the model that leads the relevant domain (Gemini for science, Claude for synthesis, GPT for math).</li>
    <li><strong>Measure before optimizing.</strong> Track hallucination rates, claim density, and source quality across configurations. Build your own quality frontier based on your specific use case.</li>
    <li><strong>Don't over-engineer.</strong> The gap between a 2-pass and 5-pass pipeline is smaller than the gap between 1-pass and 2-pass. Diminishing returns hit fast.</li>
  </ol>
</div>

<!-- SECTION 10: PREDICTIONS -->
<div class="page" id="predictions">
  <h2>10. Predictions
    <span style="font-size: 0.65rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 2px 6px; border-radius: 8px; margin-left: 8px; vertical-align: middle;">BETA</span>
  </h2>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">Predictions scored publicly at 12 months. Version 1.0 (February 2026).</p>

  <div class="exhibit">
    <table class="exhibit-table">
      <tr>
        <th>Prediction</th>
        <th>Timeline</th>
        <th>Confidence</th>
      </tr>
      <tr>
        <td>At least one major AI lab ships a native multi-model routing API (select best model per subtask automatically)</td>
        <td>Q4 2026</td>
        <td>60%</td>
      </tr>
      <tr>
        <td>Mixture-of-Agents or similar ensemble approaches become standard in at least 3 production agent frameworks</td>
        <td>Q2 2027</td>
        <td>55%</td>
      </tr>
      <tr>
        <td>Benchmark fragmentation increases: by Q4 2026, no single model leads more than 40% of major benchmarks</td>
        <td>Q4 2026</td>
        <td>70%</td>
      </tr>
    </table>
  </div>

  <p style="font-size: 0.8rem; color: #888; margin-top: 16px; font-style: italic;">Predictions scored publicly at 12 months.</p>
</div>

<!-- TRANSPARENCY NOTE -->
<div class="page" id="transparency">
  <h2>11. Transparency Note</h2>

  <p class="transparency-intro">This section explains the methodology, known limitations, and confidence calibration.</p>

  <table class="transparency-table">
    <tr>
      <td>Overall Confidence</td>
      <td>62%</td>
    </tr>
    <tr>
      <td>Sources</td>
      <td>6 primary (peer-reviewed papers: MoA, Neural Diversity, Blind Spot Biases, Self-Correction), 4 secondary (benchmark aggregators, pricing comparisons), 1 internal experiment (N=1)</td>
    </tr>
    <tr>
      <td>Strongest Evidence</td>
      <td>MoA 65.1% AlpacaEval improvement (arXiv:2406.04692, reproducible, code available) [1]; Neural Diversity 94.3% empirical variance explained (arXiv:2510.20690) [2]; LM Council benchmark data (independently run, multiple evaluators) [3]</td>
    </tr>
    <tr>
      <td>Weakest Point</td>
      <td>Internal experiment is N=1 simulation — cost estimates are derived from published pricing, quality assessments are qualitative. Within-model variance may exceed between-configuration differences. The experiment illustrates the frontier but does not prove it.</td>
    </tr>
    <tr>
      <td>What Would Invalidate This Report?</td>
      <td>If better single-model prompting (structured self-critique, explicit blindspot detection) closes the quality gap to multi-model review, then multi-model pipelines add complexity without sufficient benefit. The self-correction literature suggests this gap exists, but magnitude is uncertain.</td>
    </tr>
    <tr>
      <td>Methodology</td>
      <td>Multi-agent research pipeline. Web search + web fetch for current research. Cross-referenced with AR-009 (Calibration Gap). Pipeline comparison uses published API pricing and standard token count estimates.</td>
    </tr>
    <tr>
      <td><strong>Limitations</strong></td>
      <td>N=1 experiment is illustrative, not statistically rigorous. MoA improvements measured on AlpacaEval (preference-based) may not directly translate to factual research quality. Neural diversity results are from a single paper (December 2025). Benchmark data changes rapidly — specific numbers may be outdated within weeks.</td>
    </tr>
    <tr>
      <td>System Disclosure</td>
      <td>This report was created with a multi-agent research system.</td>
    </tr>
  </table>
</div>

<!-- CLAIM REGISTER -->
<div class="page" id="claim-register">
  <h2>12. Claim Register</h2>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">Key quantitative and qualitative claims with sources and confidence levels.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 6: Claim Register</p>
    <table class="exhibit-table">
      <tr>
        <th>#</th>
        <th>Claim</th>
        <th>Value</th>
        <th>Source</th>
        <th>Confidence</th>
      </tr>
      <tr>
        <td>1</td>
        <td>MoA improvement over GPT-4 Omni on AlpacaEval 2.0</td>
        <td>+7.6pp (65.1% vs 57.5%)</td>
        <td>arXiv:2406.04692 [1]</td>
        <td>High (reproducible)</td>
      </tr>
      <tr>
        <td>2</td>
        <td>Neural diversity hallucination reduction</td>
        <td>Up to 25.6%</td>
        <td>arXiv:2510.20690 [2]</td>
        <td>Medium (single paper)</td>
      </tr>
      <tr>
        <td>3</td>
        <td>Neural correlation ↔ hallucination relationship</td>
        <td>0.1% correlation → 3.8% hallucination increase</td>
        <td>arXiv:2510.20690 [2]</td>
        <td>Medium (single paper)</td>
      </tr>
      <tr>
        <td>4</td>
        <td>Empirical variance explained by neural diversity model</td>
        <td>94.3%</td>
        <td>arXiv:2510.20690 [2]</td>
        <td>Medium (single paper)</td>
      </tr>
      <tr>
        <td>5</td>
        <td>Gemini 3 Pro GPQA Diamond score</td>
        <td>92.6%</td>
        <td>LM Council [3]</td>
        <td>High (independent eval)</td>
      </tr>
      <tr>
        <td>6</td>
        <td>Claude Sonnet 4.5 SWE-bench Verified score</td>
        <td>64.8%</td>
        <td>LM Council [3]</td>
        <td>High (independent eval)</td>
      </tr>
      <tr>
        <td>7</td>
        <td>GPT-5 MATH Level 5 score</td>
        <td>98.1%</td>
        <td>LM Council [3]</td>
        <td>High (independent eval)</td>
      </tr>
      <tr>
        <td>8</td>
        <td>A+ pipeline costs 15x more than Sonnet-only</td>
        <td>$0.855 vs $0.057</td>
        <td>Own simulation [N=1]</td>
        <td>Low (simulation)</td>
      </tr>
      <tr>
        <td>9</td>
        <td>Heterogeneous model outputs contribute more than homogeneous</td>
        <td>Demonstrated</td>
        <td>arXiv:2406.04692 [1]</td>
        <td>High (reproducible)</td>
      </tr>
      <tr>
        <td>10</td>
        <td>Unverbalized biases differ across LLMs</td>
        <td>Demonstrated across 6 models</td>
        <td>arXiv:2602.10117 [4]</td>
        <td>Medium (recent paper)</td>
      </tr>
    </table>
  </div>

  <p style="font-size: 0.85rem; color: #555; margin-top: 24px; line-height: 1.6;"><strong>Top 5 Claims — Invalidation Conditions:</strong></p>
  <ul style="font-size: 0.85rem; color: #555; line-height: 1.6; margin-left: 20px;">
    <li><strong>Claim #1 (MoA +7.6pp):</strong> Invalidated if independent reproductions show &lt;2pp improvement or if the effect is driven by AlpacaEval's preference-based methodology rather than factual quality.</li>
    <li><strong>Claim #2 (25.6% hallucination reduction):</strong> Invalidated if reproduction studies with frontier models show &lt;5% reduction, or if the effect disappears at scale.</li>
    <li><strong>Claim #5–7 (Benchmark scores):</strong> Invalidated as benchmarks update — these are point-in-time snapshots. The claim that "no model leads all domains" is more durable than individual scores.</li>
    <li><strong>Claim #8 (15x cost):</strong> Invalidated if API pricing changes significantly or if token count estimates are materially wrong. Based on published pricing as of Feb 2026.</li>
    <li><strong>Claim #10 (Model-specific biases):</strong> Invalidated if biases are shown to be highly correlated across models (same training data → same biases), reducing the diversification benefit.</li>
  </ul>
</div>

<!-- REFERENCES -->
<div class="page" id="references">
  <h2>13. References</h2>

  <p class="reference-entry">[1] Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., Zou, J. (2024). "Mixture-of-Agents Enhances Large Language Model Capabilities." arXiv:2406.04692. Together AI, Stanford University.</p>

  <p class="reference-entry">[2] Chakrabarti, K., et al. (2025). "Neural Diversity Regularizes Hallucinations in Language Models." arXiv:2510.20690.</p>

  <p class="reference-entry">[3] LM Council. (2026). "AI Model Benchmarks February 2026." lmcouncil.ai/benchmarks. Independently-run benchmarks by Epoch AI and Scale AI.</p>

  <p class="reference-entry">[4] Arcuschin, I., Chanin, D., Garriga-Alonso, A., Camburu, O.-M. (2026). "Biases in the Blind Spot: Detecting What LLMs Fail to Mention." arXiv:2602.10117.</p>

  <p class="reference-entry">[5] API pricing: Anthropic (claude.ai/pricing), OpenAI (openai.com/pricing), Google (cloud.google.com/vertex-ai/pricing), DeepSeek (platform.deepseek.com). Accessed February 2026.</p>

  <p class="reference-entry">[6] Self-Correction Bench Authors. (2025). "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs." arXiv:2507.02778.</p>

  <p class="reference-entry">[7] Passionfruit. (2025). "GPT 5.1 vs Claude 4.5 vs Gemini 3: 2025 AI Comparison." getpassionfruit.com.</p>

  <p class="reference-entry">[8] Together AI. (2024). "Mixture of Agents Documentation." docs.together.ai/docs/mixture-of-agents.</p>

  <p class="reference-entry">[9] Ainary Research. (2026). "The Calibration Gap." AR-009.</p>

  <p class="reference-entry">[10] Nature Scientific Reports. (2025). "What social stratifications in bias blind spot can tell us about implicit social bias in both LLMs and humans." doi:10.1038/s41598-025-14875-3.</p>

  <p style="font-size: 0.8rem; color: #888; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee;"><strong>Cite as:</strong> Ainary Research (2026). <em>The Multi-Model Quality Frontier — When More Models Mean Better Research, and When They Don't.</em> AR-030.</p>

  <div class="author-section">
    <p class="author-label">About the Author</p>
    <p class="author-bio">Florian Ziesche is the founder of Ainary Ventures, where AI does 80% of the research and humans do the 20% that matters. Before Ainary, he was CEO of 36ZERO Vision and advised startups and SMEs on AI strategy and due diligence. His conviction: HUMAN × AI = LEVERAGE. This report is the proof.</p>
    <p style="font-size: 0.85rem; color: #888; margin-top: 12px;">
      <a href="https://ainaryventures.com" style="color: #888; text-decoration: none; border-bottom: 1px solid #ddd;">ainaryventures.com</a>
    </p>
  </div>
</div>

<!-- BACK COVER -->
<div class="back-cover">
  <div class="cover-brand" style="margin-bottom: 24px;">
    <span class="gold-punkt">●</span>
    <span class="brand-name">Ainary</span>
  </div>

  <p class="back-cover-services">AI Strategy · Published Research · Daily Intelligence</p>

  <p class="back-cover-cta">
    <a href="mailto:florian@ainaryventures.com" style="color: #888; text-decoration: none;">Contact</a> · <a href="mailto:florian@ainaryventures.com?subject=Feedback: AR-030" style="color: #888; text-decoration: none;">Feedback</a>
  </p>

  <p class="back-cover-contact">ainaryventures.com</p>
  <p class="back-cover-contact">florian@ainaryventures.com</p>

  <p style="font-size: 0.7rem; color: #aaa; margin-top: 48px;">© 2026 Ainary Ventures</p>
</div>

</body>
</html>
