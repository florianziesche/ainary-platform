# AR-INDEX — Ainary Report Master Index

*Generated: 2026-02-15 | Total: 37 files, 30 unique AR-IDs (+ 1 special report)*

## Version Key
- **v1/B+** = Sonnet redesign (original template, "Florian Ziesche" branding)
- **v1/A+** = Opus QA'd redesign (AR-ID branding, structured confidence, claim register)
- **v2** = Explicit v2 revision (e.g., `-v2.html`)
- **v2.2** = Late pipeline (adversarial self-review, source quality scores)

## Duplicate Pairs
Reports with both v1 and redesigned versions — the **CURRENT** version is marked ✅:

| Topic | Old (v1/B+) | Current ✅ |
|-------|------------|-----------|
| State of Agent Trust | `state-of-agent-trust-2026.html` (AR-001) | `state-of-agent-trust-2026-v2.html` (AR-001-v2) ✅ |
| Trust Maturity Model | `maturity-model-2026.html` | `trust-maturity-model-2026.html` (AR-004) ✅ |
| Financial Services | `financial-services-2026.html` | `financial-services-trust-2026.html` (AR-005) ✅ |
| Security Playbook | `security-playbook-2026.html` | `security-playbook-2026-v2.html` (AR-006) ✅ |
| Calibration Gap | `calibration-2026.html` | `calibration-gap-2026.html` (AR-009) ✅ |
| EU vs US Regulation | `eu-us-regulation-2026.html` | `eu-vs-us-regulation-2026.html` (AR-003) ✅ |
| Agent Economics | `agent-economics-2026.html` (AR-016) | `agent-economics-real-roi-2026.html` (AR-021) ✅ |

---

## Master Index

| AR-ID | Title | Core Thesis (1 sentence) | Conf. | Version | Filename |
|-------|-------|--------------------------|-------|---------|----------|
| AR-001 | State of AI Agent Trust 2026 | The $52B AI agent industry is building without a trust layer, and the cost of not fixing this is accelerating exponentially | 75% | v1/A+ | `state-of-agent-trust-2026.html` |
| AR-001-v2 | State of AI Agent Trust 2026 (v2) | Enterprises should invest in trust infrastructure now using "buy + extend" rather than building from scratch or waiting | 70% | v2 | `state-of-agent-trust-2026-v2.html` ✅ |
| AR-002 | The Trust Tax | Every enterprise AI budget carries an invisible line item — the cumulative cost of deploying agents without verification, governance, or calibration infrastructure — averaging $4.4M/company in losses | 78% | v1/A+ | `trust-tax-2026.html` |
| AR-003 | The Transatlantic Divide | The EU and US are building two incompatible regulatory frameworks for AI agents, with EU compliance costing 5–20× more, while neither framework actually defines or addresses agents | 75% | v1/A+ | `eu-vs-us-regulation-2026.html` ✅ |
| AR-003 (old) | The Transatlantic Divide | Same topic, earlier version without AR-ID branding | — | v1/B+ | `eu-us-regulation-2026.html` |
| AR-004 | The AI Agent Maturity Model | No existing AI maturity model accounts for what makes agents different — organizations think they're further along than they are because they're measuring AI-as-tool, not AI-as-actor | 72% | v1/A+ | `trust-maturity-model-2026.html` ✅ |
| AR-004 (old) | The AI Agent Maturity Model | Same topic, earlier version with "Florian Ziesche" branding | 85% | v1/B+ | `maturity-model-2026.html` |
| AR-005 | The Financial Services Trust Playbook | Financial services will deploy AI agents first AND fail first because structural forces make adoption inevitable while trust infrastructure lags behind deployment speed | 68% | v1/A+ | `financial-services-trust-2026.html` ✅ |
| AR-005 (old) | The Financial Services Playbook | Same topic, earlier version | 68% | v1/B+ | `financial-services-2026.html` |
| AR-006 | The AI Agent Security Playbook | Every defense deployed today was designed for chatbots — agents have tools, memory, and network access making the attack surface 10× larger, and every published prompt injection defense has been broken | 78% | v1/A+ | `security-playbook-2026-v2.html` ✅ |
| AR-006 (old) | The AI Agent Security Playbook | Same topic, earlier version | 78% | v1/B+ | `security-playbook-2026.html` |
| AR-007 | The Orchestration Problem | The $52B agent market is building agents without solving how they work together — 40%+ of agentic projects will be canceled by 2027 because orchestration, not capability, is the bottleneck | 72% | v1/A+ | `orchestration-2026.html` |
| AR-008 | AI Governance for Boards | Board composition is optimizing for the last crisis while AI demands fundamentally different expertise — only 22% of CEOs say their board effectively supports them on AI | 72% | v1/A+ | `governance-2026.html` |
| AR-009 | The Calibration Gap | AI agents are systematically overconfident — 84% of LLM responses show confidence exceeding accuracy, verbalized confidence is biased upward 20–30pp, and multi-agent verification amplifies rather than corrects miscalibration | 72% | v1/A+ | `calibration-gap-2026.html` ✅ |
| AR-009 (old) | The Calibration Gap | Same topic, earlier version | 72% | v1/B+ | `calibration-2026.html` |
| AR-010 | The AI Agent Failure Taxonomy | AI agents fail in six distinct compounding ways — hallucination, tool misuse, cascading failures, silent degradation, confidence miscalibration, and memory corruption — and each amplifies the others | 72% | v1/A+ | `agent-failure-taxonomy-2026.html` |
| AR-011 | The Human-in-the-Loop Illusion | Human oversight is the default answer to AI risk but fails systematically at scale — 67% of security alerts are ignored and 80–99% of clinical alarms are false positives, making HITL a liability, not a safeguard | 75% | v1/A+ | `hitl-illusion-2026.html` |
| AR-012 | The Trust Moat | Trust infrastructure isn't a cost center — it's the deepest moat in AI; companies that build calibrated, transparent agent systems compound advantages competitors cannot replicate | 75% | v1/A+ | `trust-moat-2026.html` |
| AR-013 | The Developer Trust Gap | AI trust tools exist but developers don't use them — not because they don't care about safety, but because the tools add friction without visible value; the adoption problem is a UX problem | 72% | v1/A+ | `developer-trust-gap-2026.html` |
| AR-014 | The Memory Problem | Context windows are getting larger but the memory problem isn't about size — it's about what agents remember, forget, and who decides; memory architecture is the hidden infrastructure layer determining whether agents compound intelligence or repeat mistakes | 82% | v1/A+ | `agent-memory-2026.html` |
| AR-015 | Does Knowledge Actually Compound? | Everyone claims knowledge compounds but nobody measures it — this report proposes the first quantitative framework and finds that compounding requires deliberate architecture, not just accumulation | 75% | v1/A+ | `knowledge-compounding-2026.html` |
| AR-016 | The Agent Economics Report | Actual production agent costs ($2.75/report) are radically different from vendor narratives, achieving 181× ROI — but only with systematic cost optimization and trust infrastructure | 85% | v1/A+ | `agent-economics-2026.html` |
| AR-017 | Build vs Buy vs Compose | The "build or buy" framing is obsolete — the real question is which layer to build, buy, or compose, and the trust layer is the integration point that most companies get wrong | 78% | v1/A+ | `build-buy-compose-2026.html` |
| AR-018 | The Observability Gap | AI agents fail silently and current observability tools were built for microservices, not for agents that reason and remember — the gap between what we can monitor and what we need to monitor is growing | 72% | v1/A+ | `observability-gap-2026.html` |
| AR-019 | The Agent Memory Market Map | Agent memory is becoming a distinct infrastructure category — $24M raised by Mem0 signals institutional validation, and the companies building persistent memory will control the next platform shift | 82% | v1/A+ | `agent-memory-market-map-2026.html` |
| AR-020 | Trust Scores Will Become the New Credit Scores | Within 5 years AI agents will have trust scores analogous to FICO — determining what actions they can take and creating a multi-billion dollar reputation infrastructure industry | 68% | v1/A+ | `trust-scores-credit-scores-2026.html` |
| AR-021 | AI Agent Economics — The Real ROI | Most AI agent ROI calculations are fantasy — actual deployment costs are 3–7× initial estimates when you include trust infrastructure, monitoring, and failure remediation | 82% | v2.2 | `agent-economics-real-roi-2026.html` ✅ |
| AR-022 | Most AI Governance Frameworks Are Theater | 80% of enterprise AI governance frameworks are compliance theater — the gap between what organizations document and what they actually do is so wide that governance has become a checkbox exercise | 76% | v1/A+ | `governance-theater-2026.html` |
| AR-023 | Agent Testing Is Broken | Traditional software testing assumes deterministic outputs — AI agents break this assumption fundamentally, and the testing industry hasn't caught up | 82% | v1/A+ | `agent-testing-broken-2026.html` |
| AR-024 | The Trust Transfer Problem | Trust doesn't transfer transitively — when Agent A delegates to Agent B the trust chain breaks in ways current architectures can't handle, and we're ignoring 30 years of PKI lessons | 75% | v1/A+ | `trust-transfer-problem-2026.html` |
| AR-025 | The Knowledge Compounding Flywheel | Organizations that instrument their knowledge compounding rate will outperform those that don't by 10× within 3 years — and most don't even know what to measure | 72% | v2.2 | `knowledge-compounding-flywheel-2026.html` |
| AR-026 | The Knowledge Architecture Effect | Link density — not folder structure or methodology — is the single strongest predictor of whether a knowledge system compounds; three links per note is the threshold | 62% | v2.2 | `knowledge-architecture-effect-2026.html` |
| AR-027 | The Real Cost of AI Agents in Production | The headline cost of "$2.75 per report" captures only 15–25% of real cost — honestly accounted, our system costs $70–80 per report, and enterprise budgets underestimate TCO by 40–60% | 82% | v2.2 | `real-cost-ai-agents-2026.html` |
| AR-028 | AI Governance: Framework vs. Reality | We applied ISO 42001 and NIST AI RMF clause-by-clause to our own pipeline's 10 failure modes — ISO caught 40%, NIST caught 45%, combined they caught 55%, and the 45% they both miss are the ones that actually cause disasters | 82% | v2.2 | `governance-framework-vs-reality-2026.html` |
| AR-029 | Does AI Quality Actually Compound? | Quality does not compound in our AI pipeline — efficiency does; QA scores are flat (avg 85.3) across 25 reports while production speed improved 3×, and the distinction matters more than most AI narratives acknowledge | 62% | v2.2 | `quality-compound-study-2026.html` |
| AR-030 | The Multi-Model Quality Frontier | Multi-model pipelines improve research quality primarily through error reduction not content generation — an adversarial review pass catches more errors per dollar than adding a second model | 62% | v2.2 | `multi-model-quality-frontier-2026.html` |
| — | TrustCheck: The AI Trap That Is Quietly Wiping Out Angel Investors | Fact-check of Susan Montgomery's article — core thesis on unit economics is sound but the article makes sweeping claims about angel portfolios being wiped out without a single data point | 95% | Special | `factcheck-ai-trap-angels.html` |

---

**Total: 37 report files → 30 unique AR-IDs + 1 special fact-check report**
- AR-001 through AR-015: Original research series (6 have old v1/B+ duplicates)
- AR-016 through AR-020: Tier 1 expansion
- AR-021 through AR-030: Tier 2 pipeline (v2.2 for AR-025+)
- 7 duplicate pairs (old version + current version)
- 1 special report (factcheck, no AR-ID)
