<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Governance: Framework vs. Reality — Ainary Report AR-028</title>
<style>
  @font-face {
    font-family: 'Inter';
    src: url('/fonts/inter-variable.woff2') format('woff2');
    font-weight: 100 900;
    font-display: swap;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif; background: #fafaf8; color: #333; line-height: 1.75; font-size: 0.95rem; font-weight: 400; }
  .page { max-width: 900px; margin: 0 auto; padding: 48px 40px; }
  .cover { min-height: 100vh; display: flex; flex-direction: column; justify-content: space-between; max-width: 900px; margin: 0 auto; padding: 48px 40px; }
  .back-cover { min-height: 100vh; display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center; max-width: 900px; margin: 0 auto; padding: 48px 40px; page-break-before: always; }
  h1 { font-size: 2.2rem; font-weight: 600; line-height: 1.2; color: #1a1a1a; letter-spacing: -0.02em; }
  h2 { font-size: 1.5rem; font-weight: 600; color: #1a1a1a; line-height: 1.3; margin-top: 3rem; margin-bottom: 12px; }
  h3 { font-size: 1.1rem; font-weight: 600; color: #1a1a1a; line-height: 1.4; margin-top: 2rem; margin-bottom: 12px; }
  p { margin-bottom: 1rem; }
  strong { font-weight: 600; color: #1a1a1a; }
  em { font-style: italic; }
  sup { font-size: 0.65rem; color: #888; vertical-align: super; }
  .cover-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 40vh; }
  .cover-brand { display: flex; align-items: center; gap: 8px; }
  .gold-punkt { color: #c8aa50; font-size: 14px; }
  .brand-name { font-size: 0.85rem; font-weight: 500; color: #1a1a1a; letter-spacing: 0.02em; }
  .cover-meta { display: flex; gap: 12px; font-size: 0.75rem; color: #888; }
  .cover-title-block { margin-bottom: auto; }
  .cover-title { margin-bottom: 16px; }
  .cover-subtitle { font-size: 1rem; font-weight: 400; color: #666; line-height: 1.5; }
  .cover-footer { display: flex; justify-content: space-between; align-items: flex-end; }
  .cover-date { font-size: 0.75rem; color: #888; }
  .cover-author { font-size: 0.75rem; color: #888; text-align: center; }
  .toc-label { font-size: 0.7rem; font-weight: 600; color: #1a1a1a; text-transform: uppercase; letter-spacing: 0.1em; margin-bottom: 24px; }
  .toc-section { margin-bottom: 32px; }
  .toc-section-label { font-size: 0.65rem; font-weight: 500; color: #888; text-transform: uppercase; letter-spacing: 0.12em; margin-bottom: 12px; }
  .toc-entry { display: flex; align-items: baseline; gap: 16px; padding: 12px 0; border-bottom: 1px solid #eee; text-decoration: none; transition: all 0.2s; }
  .toc-number { font-size: 0.8rem; color: #888; font-variant-numeric: tabular-nums; min-width: 24px; }
  .toc-title { font-size: 0.95rem; font-weight: 500; color: #1a1a1a; flex: 1; transition: color 0.2s; }
  .toc-entry:hover .toc-title { color: #c8aa50; }
  .toc-page { font-size: 0.8rem; color: #888; }
  .how-to-read-table { width: 100%; border-collapse: collapse; margin: 24px 0; }
  .how-to-read-table th { text-align: left; font-size: 0.7rem; font-weight: 600; color: #555; text-transform: uppercase; letter-spacing: 0.05em; padding: 10px 12px; background: #f5f4f0; border-bottom: 2px solid #e5e3dc; }
  .how-to-read-table td { font-size: 0.85rem; color: #333; padding: 10px 12px; border-bottom: 1px solid #ddd; }
  .thesis { font-size: 1rem; font-weight: 600; color: #1a1a1a; line-height: 1.6; margin-bottom: 24px; }
  .evidence-list { margin-left: 20px; margin-bottom: 24px; }
  .evidence-list li { font-size: 0.9rem; color: #333; line-height: 1.6; margin-bottom: 8px; }
  .keywords { font-size: 0.8rem; color: #666; font-style: italic; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee; }
  .confidence-badge { font-size: 0.75rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 3px 8px; border-radius: 10px; margin-left: 8px; vertical-align: middle; }
  .confidence-line { font-size: 0.8rem; color: #888; font-style: italic; display: block; margin-bottom: 16px; }
  .key-insight { font-weight: 600; color: #1a1a1a; }
  .callout { background: #f5f4f0; padding: 16px 20px; border-radius: 4px; margin: 1.5rem 0; page-break-inside: avoid; }
  .callout-label { font-size: 0.7rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.08em; margin-bottom: 8px; }
  .callout-body { font-size: 0.9rem; color: #555; line-height: 1.6; }
  .callout.claim .callout-label { color: #555; }
  .callout.invalidation { border-left: 3px solid #ddd; }
  .callout.invalidation .callout-label { color: #888; }
  .callout.sowhat { border-left: 3px solid #c8aa50; }
  .callout.sowhat .callout-label { color: #c8aa50; }
  .exhibit { margin: 2rem 0; }
  .exhibit-label { font-size: 0.75rem; font-weight: 600; color: #555; margin-bottom: 12px; }
  .exhibit-table { width: 100%; border-collapse: collapse; page-break-inside: avoid; }
  .exhibit-table th { text-align: left; font-size: 0.7rem; font-weight: 600; color: #555; text-transform: uppercase; letter-spacing: 0.05em; padding: 10px 12px; background: #f5f4f0; border-bottom: 2px solid #e5e3dc; }
  .exhibit-table td { font-size: 0.85rem; color: #333; padding: 10px 12px; border-bottom: 1px solid #ddd; }
  .exhibit-source { font-size: 0.7rem; color: #888; margin-top: 8px; font-style: italic; }
  .kpi-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 48px; margin: 2rem 0; }
  .kpi { text-align: left; }
  .kpi-number { font-size: 2rem; font-weight: 600; color: #1a1a1a; line-height: 1.2; }
  .kpi-label { font-size: 0.75rem; color: #666; margin-top: 4px; }
  .kpi-source { font-size: 0.65rem; color: #888; margin-top: 2px; }
  ul { margin-left: 20px; margin-bottom: 1rem; }
  ol { margin-left: 20px; margin-bottom: 1rem; }
  li { margin-bottom: 4px; }
  .transparency-intro { font-size: 0.85rem; color: #555; line-height: 1.6; margin-bottom: 12px; }
  .transparency-table { width: 100%; border-collapse: collapse; margin-top: 12px; }
  .transparency-table td:first-child { font-size: 0.85rem; font-weight: 600; color: #555; padding: 8px 0; border-bottom: 1px solid #eee; width: 160px; vertical-align: top; }
  .transparency-table td:last-child { font-size: 0.85rem; color: #333; padding: 8px 0; border-bottom: 1px solid #eee; }
  .reference-entry { font-size: 0.8rem; color: #555; line-height: 1.5; margin-bottom: 6px; padding-left: 24px; text-indent: -24px; }
  .author-section { margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #e5e3dc; }
  .author-label { font-size: 0.85rem; font-weight: 600; color: #555; margin-bottom: 8px; }
  .author-bio { font-size: 0.85rem; color: #555; line-height: 1.6; }
  .back-cover-services { font-size: 0.85rem; color: #666; margin-bottom: 24px; }
  .back-cover-cta { font-size: 0.85rem; color: #888; margin-bottom: 16px; }
  .back-cover-contact { font-size: 0.8rem; color: #888; }
  .beta-badge { font-size: 0.65rem; font-weight: 500; color: #888; background: #f5f4f0; padding: 2px 6px; border-radius: 8px; margin-left: 8px; vertical-align: middle; }
  @media print {
    @page { size: A4; margin: 2cm; }
    body { background: white; }
    .page, .cover, .back-cover { page-break-after: always; }
    .callout, .exhibit { page-break-inside: avoid; }
    @page :first { @top-center { content: none; } @bottom-center { content: none; } }
    @page { @top-center { content: "Ainary Report | AI Governance: Framework vs. Reality"; font-size: 0.7rem; color: #888; } @bottom-left { content: "© 2026 Ainary Ventures"; font-size: 0.7rem; color: #888; } @bottom-right { content: counter(page); font-size: 0.7rem; color: #888; } }
  }
</style>
</head>
<body>

<!-- COVER -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">●</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>AR-028</span>
      <span>Confidence: 82%</span>
    </div>
  </div>
  <div class="cover-title-block">
    <h1 class="cover-title">AI Governance:<br>Framework vs. Reality</h1>
    <p class="cover-subtitle">We Applied ISO 42001 and NIST AI RMF to a Real AI Pipeline. They Caught 40–50% of Documented Failure Modes.</p>
  </div>
  <div class="cover-footer">
    <div class="cover-date">February 2026<br><span style="font-size: 0.7rem; color: #aaa;">v1.0</span></div>
    <div class="cover-author">Florian Ziesche · Ainary Ventures</div>
  </div>
</div>

<!-- TABLE OF CONTENTS -->
<div class="page">
  <p class="toc-label">Contents</p>
  <div class="toc-section">
    <p class="toc-section-label">FOUNDATION</p>
    <a href="#how-to-read" class="toc-entry">
      <span class="toc-number">1</span>
      <span class="toc-title">How to Read This Report</span>
    </a>
    <a href="#exec-summary" class="toc-entry">
      <span class="toc-number">2</span>
      <span class="toc-title">Executive Summary</span>
    </a>
    <a href="#methodology" class="toc-entry">
      <span class="toc-number">3</span>
      <span class="toc-title">Methodology</span>
    </a>
  </div>
  <div class="toc-section">
    <p class="toc-section-label">ANALYSIS</p>
    <a href="#experiment" class="toc-entry">
      <span class="toc-number">4</span>
      <span class="toc-title">The Experiment: Two Frameworks vs. One Pipeline</span>
    </a>
    <a href="#iso-scorecard" class="toc-entry">
      <span class="toc-number">5</span>
      <span class="toc-title">ISO 42001: 40% of Failure Modes Caught</span>
    </a>
    <a href="#nist-scorecard" class="toc-entry">
      <span class="toc-number">6</span>
      <span class="toc-title">NIST AI RMF: 50% of Failure Modes Caught</span>
    </a>
    <a href="#gap" class="toc-entry">
      <span class="toc-number">7</span>
      <span class="toc-title">The 40% Gap: What Neither Framework Catches</span>
    </a>
    <a href="#eu-ai-act" class="toc-entry">
      <span class="toc-number">8</span>
      <span class="toc-title">EU AI Act: Mandatory Compliance, Same Blind Spots</span>
    </a>
    <a href="#ar008-vs-ar022" class="toc-entry">
      <span class="toc-number">9</span>
      <span class="toc-title">Where AR-008 and AR-022 Agree and Disagree</span>
    </a>
  </div>
  <div class="toc-section">
    <p class="toc-section-label">ACTION</p>
    <a href="#recommendations" class="toc-entry">
      <span class="toc-number">10</span>
      <span class="toc-title">Recommendations</span>
    </a>
    <a href="#predictions" class="toc-entry">
      <span class="toc-number">11</span>
      <span class="toc-title">Predictions</span>
    </a>
    <a href="#transparency" class="toc-entry">
      <span class="toc-number">12</span>
      <span class="toc-title">Transparency Note</span>
    </a>
    <a href="#claim-register" class="toc-entry">
      <span class="toc-number">13</span>
      <span class="toc-title">Claim Register</span>
    </a>
    <a href="#references" class="toc-entry">
      <span class="toc-number">14</span>
      <span class="toc-title">References</span>
    </a>
  </div>
</div>

<!-- HOW TO READ -->
<div class="page" id="how-to-read">
  <h2>1. How to Read This Report</h2>
  <p>This report differs from typical governance analysis. Instead of reviewing frameworks theoretically, we applied ISO 42001 and NIST AI RMF clause-by-clause to a real AI agent pipeline — our own — and documented what they catch and what they miss.</p>
  <table class="how-to-read-table">
    <tr><th>Rating</th><th>Meaning</th><th>Example</th></tr>
    <tr><td>High</td><td>Directly tested or 3+ independent sources</td><td>ISO 42001 catches template drift via improvement clause (tested)</td></tr>
    <tr><td>Medium</td><td>1–2 sources or plausible inference</td><td>Most enterprises face EU AI Act compliance gaps (analyst consensus)</td></tr>
    <tr><td>Low</td><td>Single source or directional claim</td><td>Combined frameworks catch 60% of failure modes (our experiment only)</td></tr>
  </table>
  <p style="margin-top: 24px;">This report was produced using a multi-agent research pipeline with structured source validation. The experiment is documented in full at <code>experiments/governance-reality-check/</code>. Full methodology in Section 12.</p>
</div>

<!-- EXECUTIVE SUMMARY -->
<div class="page" id="exec-summary">
  <h2>2. Executive Summary</h2>
  <p class="thesis">We applied ISO 42001 and NIST AI RMF clause-by-clause to our own AI agent pipeline and its 10 documented failure modes. ISO 42001 caught 40%. NIST AI RMF caught 45%. Combined, they caught 55%. The 45% they both miss — prompt injection, agent contagion, memory corruption, source quality degradation — are the failure modes that actually cause AI disasters.</p>
  <ul class="evidence-list">
    <li><strong>ISO 42001 catches organizational failures but misses technical agent-specific ones</strong> — it would require us to build monitoring, but doesn't specify what to monitor for AI agents<sup>[1]</sup></li>
    <li><strong>NIST AI RMF is better for operational governance because it mandates red teaming and metrics</strong> — but it's voluntary, and the labor-intensive measurement steps are the first to be skipped<sup>[2]</sup></li>
    <li><strong>Neither framework addresses agentic AI failure modes</strong> — prompt injection via web fetch, multi-agent contagion, and persistent memory corruption didn't exist when these frameworks were written<sup>[3]</sup></li>
    <li><strong>75% of organizations with governance frameworks have not operationalized them</strong> — Cisco's 2026 survey: only 12% describe their governance as mature<sup>[4][5]</sup></li>
    <li><strong>EU AI Act high-risk deadline is August 2, 2026</strong> — most enterprises face significant compliance gaps, and a proposed Digital Omnibus could push Annex III deadlines to December 2027<sup>[6][7]</sup></li>
  </ul>
  <p class="keywords"><strong>Keywords:</strong> ISO 42001, NIST AI RMF, EU AI Act, governance experiment, agentic AI, failure modes, compliance gap, operational governance</p>
</div>

<!-- METHODOLOGY -->
<div class="page" id="methodology">
  <h2>3. Methodology</h2>
  <p>This report combines framework analysis with hands-on experimentation. We reviewed ISO 42001 clause-by-clause and NIST AI RMF pillar-by-pillar against a real multi-agent research pipeline (our own), testing each control against 10 documented failure modes from previous Ainary reports. We then cross-referenced findings against enterprise surveys (Cisco 2026, Deloitte, SecurePrivacy.ai), regulatory timelines (EU AI Act), and practitioner analyses. Two simulations — agent hallucination and confidence drift — tested framework effectiveness under realistic failure scenarios.</p>
  <p><strong>Limitations:</strong> N=1 pipeline. Our failure modes may not represent all enterprise AI deployments. The scorecard percentages (40%, 50%, 60%) are specific to our pipeline and should be read as directional, not universal. ISO 42001 is a paid standard; our analysis is based on publicly available clause summaries, implementation guides, and the trail/Unique case study.</p>
</div>

<!-- SECTION 4: THE EXPERIMENT -->
<div class="page" id="experiment">
  <h2>4. The Experiment: Two Frameworks vs. One Pipeline
    <span class="confidence-badge">85%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — directly tested)</span>

  <p><span class="key-insight">We took our own AI agent pipeline — the system that produces Ainary reports — and tested whether ISO 42001 and NIST AI RMF would catch its documented failure modes.</span></p>

  <h3>The Pipeline</h3>
  <p>Our multi-agent research system uses Claude-based agents for parallel research, web search and fetch for source gathering, file system operations for report generation, a 6-phase quality pipeline with adversarial self-review, and a memory system for cross-report consistency. It is a real AI system that produces real outputs with real consequences (published reports with our name on them).</p>

  <h3>The 10 Failure Modes</h3>
  <p>From 27 previous reports, we documented 10 distinct failure modes:</p>
  <ol>
    <li><strong>Hallucination:</strong> Agent generates false statistics or citations</li>
    <li><strong>Confidence Drift:</strong> Confidence scores inflate over successive reports</li>
    <li><strong>Source Quality Degradation:</strong> Agent cites vendor marketing as primary research</li>
    <li><strong>Prompt Injection via Web Fetch:</strong> External content contains adversarial instructions</li>
    <li><strong>Agent Contagion:</strong> One subagent's error propagates to main agent output</li>
    <li><strong>Memory Corruption:</strong> Stale or wrong data persists in memory files</li>
    <li><strong>Template Drift:</strong> Report deviates from locked template rules</li>
    <li><strong>Circular Citation:</strong> Agent cites our own previous reports as independent evidence</li>
    <li><strong>HITL Bypass:</strong> Human reviewer approves without reading (automation bias)</li>
    <li><strong>Cost Runaway:</strong> Agent spawns excessive API calls without budget constraint</li>
  </ol>

  <h3>The Test</h3>
  <p>For each framework, we asked: "If we had fully implemented this framework, would it have caught this failure mode before it reached production?" We scored each as caught (the framework has a specific control), partially caught (the framework creates conditions where it could be caught), or missed (the framework does not address this class of failure).</p>

  <div class="callout claim">
    <p class="callout-label">Claim</p>
    <p class="callout-body">Governance frameworks are designed for organizational process, not technical agent behavior. The failure modes they miss are precisely the ones that cause the most damage in production.</p>
  </div>
</div>

<!-- SECTION 5: ISO 42001 SCORECARD -->
<div class="page" id="iso-scorecard">
  <h2>5. ISO 42001: 40% of Failure Modes Caught
    <span class="confidence-badge">80%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — clause-by-clause analysis)</span>

  <p><span class="key-insight">ISO 42001 is a process framework that would require us to build governance infrastructure — but does not specify what to monitor, what thresholds to set, or how to detect AI-agent-specific failures.</span></p>

  <p>ISO/IEC 42001:2023, the world's first AI management system standard, provides a certifiable framework analogous to ISO 27001. It covers AI lifecycle management, risk assessment, ethical considerations, data governance, and continuous improvement<sup>[1]</sup>. The trail/Unique case study demonstrated successful certification in 3 months with TÜV SÜD, achieving 75% reduced documentation effort — but the case study focused on a financial services SaaS platform, not an autonomous agent pipeline<sup>[8]</sup>.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: ISO 42001 vs. Our Pipeline Failure Modes</p>
    <table class="exhibit-table">
      <tr><th>Failure Mode</th><th>Relevant Clause</th><th>Caught?</th><th>Why</th></tr>
      <tr><td>Hallucination</td><td>A.4 Lifecycle, 8 Operation</td><td>No</td><td>Requires testing but doesn't specify adversarial testing for agents</td></tr>
      <tr><td>Confidence Drift</td><td>A.6 Performance, 9 Evaluation</td><td>Partial</td><td>Would catch IF confidence calibration is a tracked metric</td></tr>
      <tr><td>Source Quality</td><td>A.5 Data for AI Systems</td><td>No</td><td>Requires data quality but doesn't define web source verification</td></tr>
      <tr><td>Prompt Injection</td><td>None specific</td><td>No</td><td>Attack vector not addressed in technology-neutral standard</td></tr>
      <tr><td>Agent Contagion</td><td>None specific</td><td>No</td><td>Multi-agent coordination not in scope</td></tr>
      <tr><td>Memory Corruption</td><td>None specific</td><td>No</td><td>Persistent agent state not addressed</td></tr>
      <tr><td>Template Drift</td><td>10 Improvement</td><td>Yes</td><td>Corrective action clause catches process deviations</td></tr>
      <tr><td>Circular Citation</td><td>A.3 Impact Assessment</td><td>No</td><td>Impact assessment doesn't cover self-referential evidence</td></tr>
      <tr><td>HITL Bypass</td><td>5 Leadership</td><td>Yes</td><td>Management commitment requires defined review responsibilities</td></tr>
      <tr><td>Cost Runaway</td><td>7 Support (Resources)</td><td>Yes</td><td>Resource planning clause covers budget controls</td></tr>
    </table>
    <p class="exhibit-source">Source: ISO/IEC 42001:2023 clause analysis [1], trail case study [8], author experiment</p>
  </div>

  <p><strong>Result: 4/10 failure modes caught (40%).</strong> ISO 42001 catches organizational governance failures (template drift, HITL bypass, cost runaway) and partially catches measurement failures (confidence drift). It misses every agent-specific technical failure.</p>

  <h3>The Certification Trap</h3>
  <p>ISO 42001 is optimized for certification — an auditor can verify documentation. This creates a predictable failure mode: <strong>documentation becomes the goal, not safety.</strong> An organization can be ISO 42001 certified and still deploy agents that hallucinate, because the standard requires a testing process, not a specific test for hallucination<sup>[9]</sup>.</p>

  <p>Gartner forecasts over 70% of enterprises will adopt an AI governance standard like ISO 42001 by 2026<sup>[10]</sup>. Deloitte shows fewer than 25% with frameworks have operationalized them<sup>[4]</sup>. The delta between 70% adoption and 25% operationalization is governance theater.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If ISO 42001 evolved to include prescriptive technical controls for agentic AI (e.g., "implement prompt injection detection" or "validate multi-agent output consistency"), our 40% score would increase significantly. The standard's technology-neutral design makes this unlikely.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Use ISO 42001 as a governance scaffold, not a security blueprint. It structures your organizational process (good) but leaves a 60% gap in technical agent-specific controls (bad). Pair it with OWASP LLM Top 10 and agent-specific monitoring.</p>
  </div>
</div>

<!-- SECTION 6: NIST AI RMF SCORECARD -->
<div class="page" id="nist-scorecard">
  <h2>6. NIST AI RMF: 45% of Failure Modes Caught
    <span class="confidence-badge">80%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — pillar-by-pillar analysis)</span>

  <p><span class="key-insight">NIST AI RMF is better than ISO 42001 for our use case because it explicitly mandates red teaming, metrics, and feedback loops — the operational controls that actually catch AI failures. But it is voluntary, which means the labor-intensive parts are the first to be skipped.</span></p>

  <p>NIST AI 100-1 provides four pillars: Govern (accountability), Map (risk identification), Measure (quantification via testing), and Manage (mitigation and response). It is "intended for voluntary use"<sup>[2]</sup> — a phrase that appears in the document itself. The AI RMF is currently in revision per the 2025 AI Action Plan<sup>[11]</sup>.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: NIST AI RMF vs. Our Pipeline Failure Modes</p>
    <table class="exhibit-table">
      <tr><th>Failure Mode</th><th>Relevant Pillar</th><th>Caught?</th><th>Why</th></tr>
      <tr><td>Hallucination</td><td>Measure 2 (red teaming)</td><td>Partial</td><td>Red teaming mandate would catch IF conducted — but voluntary</td></tr>
      <tr><td>Confidence Drift</td><td>Measure 1, 4 (metrics, feedback)</td><td>Yes</td><td>Requires metrics and feedback loops — catches drift if calibration tracked</td></tr>
      <tr><td>Source Quality</td><td>Map 4 (risk identification)</td><td>No</td><td>"Fabricated web sources" unlikely to appear in generic risk register</td></tr>
      <tr><td>Prompt Injection</td><td>Manage 2 (response plans)</td><td>No</td><td>Agent-specific attack vector not in NIST taxonomy</td></tr>
      <tr><td>Agent Contagion</td><td>Measure 4 (feedback)</td><td>No</td><td>Multi-agent error propagation not addressed</td></tr>
      <tr><td>Memory Corruption</td><td>Govern 4 (audit trail)</td><td>Partial</td><td>Audit trail requirements could catch state corruption</td></tr>
      <tr><td>Template Drift</td><td>None specific</td><td>No</td><td>Process compliance not a NIST focus area</td></tr>
      <tr><td>Circular Citation</td><td>Map 1 (use case scoping)</td><td>Partial</td><td>Use case scoping could flag self-referential evidence</td></tr>
      <tr><td>HITL Bypass</td><td>Govern 1, 2 (policies, accountability)</td><td>Yes</td><td>Requires defined roles and decision authority</td></tr>
      <tr><td>Cost Runaway</td><td>Map 3 (benefits and costs)</td><td>Yes</td><td>Cost-benefit analysis mandate covers budget controls</td></tr>
    </table>
    <p class="exhibit-source">Source: NIST AI 100-1 [2], NIST AI RMF Playbook [12], author experiment</p>
  </div>

  <p><strong>Result: 4.5/10 failure modes caught (45%).</strong> NIST is 5 percentage points better than ISO because it mandates measurement, red teaming, and feedback loops — operational controls, not just documentation. The half-point comes from partial catches (hallucination, memory corruption, circular citation scored as 0.5 each).</p>

  <h3>The Voluntary Problem</h3>
  <p>NIST AI RMF's fatal flaw: no enforcement mechanism, no certification process, no liability shield<sup>[2]</sup>. Organizations reference it in policy documents but skip the labor-intensive measurement and monitoring steps. As one practitioner noted: continuous cycles require ongoing budget and engineering time; compliance checkboxes can be completed once<sup>[13]</sup>.</p>

  <p>Cisco's 2026 survey found 75% of organizations have a dedicated AI governance process, but only 12% describe their efforts as mature<sup>[5]</sup>. The 63-point maturity gap confirms: having a framework is not the same as operating one.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If U.S. federal regulation mandated NIST AI RMF compliance (like HIPAA for healthcare), the "voluntary means ignored" thesis would collapse. The current U.S. regulatory direction under the 2025 AI Action Plan makes mandatory adoption politically unlikely.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">NIST AI RMF is the best available blueprint for AI risk management — if you actually implement the Measure pillar. Most organizations adopt Govern (easy: write policies) and Map (manageable: list risks) but skip Measure (hard: quantify risks) and Manage (expensive: build response systems). The value is in the parts everyone skips.</p>
  </div>
</div>

<!-- SECTION 7: THE GAP -->
<div class="page" id="gap">
  <h2>7. The 45% Gap: What Neither Framework Catches
    <span class="confidence-badge">82%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — directly observed in experiment)</span>

  <p><span class="key-insight">The failure modes that neither framework catches — prompt injection, agent contagion, source quality degradation, memory corruption — are the agentic AI failure modes. They did not exist when these frameworks were written.</span></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Combined Framework Score Card</p>
    <table class="exhibit-table">
      <tr><th>Failure Mode</th><th>ISO 42001</th><th>NIST AI RMF</th><th>Either</th></tr>
      <tr><td>Hallucination</td><td>No</td><td>Partial</td><td>Partial</td></tr>
      <tr><td>Confidence Drift</td><td>Partial</td><td>Yes</td><td>Yes</td></tr>
      <tr><td>Source Quality Degradation</td><td>No</td><td>No</td><td>No</td></tr>
      <tr><td>Prompt Injection</td><td>No</td><td>No</td><td>No</td></tr>
      <tr><td>Agent Contagion</td><td>No</td><td>No</td><td>No</td></tr>
      <tr><td>Memory Corruption</td><td>No</td><td>Partial</td><td>Partial</td></tr>
      <tr><td>Template Drift</td><td>Yes</td><td>No</td><td>Yes</td></tr>
      <tr><td>Circular Citation</td><td>No</td><td>Partial</td><td>Partial</td></tr>
      <tr><td>HITL Bypass</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
      <tr><td>Cost Runaway</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
    </table>
    <p class="exhibit-source">Source: Author experiment, documented in experiments/governance-reality-check/</p>
  </div>

  <div class="kpi-grid">
    <div class="kpi">
      <div class="kpi-number">40%</div>
      <div class="kpi-label">ISO 42001 catch rate</div>
      <div class="kpi-source">Source: Author experiment | Confidence: Medium (n=1 pipeline)</div>
    </div>
    <div class="kpi">
      <div class="kpi-number">45%</div>
      <div class="kpi-label">NIST AI RMF catch rate</div>
      <div class="kpi-source">Source: Author experiment | Confidence: Medium (n=1 pipeline)</div>
    </div>
    <div class="kpi">
      <div class="kpi-number">45%</div>
      <div class="kpi-label">Failure modes missed by BOTH frameworks</div>
      <div class="kpi-source">Source: Author experiment | Confidence: Medium (n=1 pipeline)</div>
    </div>
  </div>

  <h3>Why the Gap Exists</h3>

  <p>Both frameworks were designed before agentic AI became mainstream. ISO 42001 was published December 2023; NIST AI RMF 1.0 in January 2023. Agentic AI — systems that autonomously browse the web, execute code, manage persistent memory, and coordinate with other agents — creates failure modes that process frameworks cannot anticipate<sup>[3][14]</sup>:</p>

  <ul>
    <li><strong>Prompt injection via web fetch</strong> — the agent's input surface extends to the entire internet. No framework says "validate that fetched web content does not contain adversarial instructions."</li>
    <li><strong>Agent contagion</strong> — when one subagent hallucinates and passes its output to another, the error compounds. Multi-agent coordination failures are too new for any standard.</li>
    <li><strong>Source quality degradation</strong> — no framework specifies "verify that your AI does not cite vendor blogs as peer-reviewed research." This is an agent-specific epistemic failure.</li>
    <li><strong>Memory corruption</strong> — persistent state in AI agents (memory files, context windows) creates a new class of integrity risk. Databases have ACID properties; agent memory files have none.</li>
  </ul>

  <h3>Simulation Results</h3>

  <p><strong>Simulation 1: Agent Hallucination (Grok-style).</strong> Our agent fetches a page with fabricated statistics. ISO 42001 would require a "testing process" — but wouldn't specify testing for adversarial web content. NIST AI RMF would mandate red teaming — but only if actually conducted (probability: ~20% per AR-022 data). <strong>Neither framework would have reliably prevented this in practice.</strong></p>

  <p><strong>Simulation 2: Confidence Drift over 10 Reports.</strong> Confidence scores: 72% → 90% without evidence improvement. ISO 42001's performance evaluation clause could catch this — but only if confidence calibration is a tracked metric (it isn't by default). NIST's Measure pillar explicitly requires metrics and feedback loops — making it the better framework here. <strong>NIST catches this; ISO catches it only with deliberate configuration.</strong></p>

  <div class="callout claim">
    <p class="callout-label">Claim</p>
    <p class="callout-body">The 45% of failure modes that no framework catches are the ones most likely to cause real damage. Organizational failures (HITL bypass, cost runaway) are embarrassing. Agent-specific failures (hallucination, injection, contagion) are dangerous.</p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If NIST's upcoming AI RMF revision (announced 2025) includes agentic AI-specific controls — prompt injection detection, multi-agent coordination testing, persistent state validation — the 45% gap would narrow significantly. NIST's February 2026 RFI on "AI Agent Security" suggests this is coming, but not yet formalized.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you operate AI agents, ISO 42001 and NIST AI RMF are necessary but not sufficient. You need an additional layer of agent-specific controls: input validation for web-fetched content, output consistency checks across agents, memory integrity verification, and source quality scoring. The frameworks provide the organizational 55%; you must build the technical 45% yourself.</p>
  </div>
</div>

<!-- SECTION 8: EU AI ACT -->
<div class="page" id="eu-ai-act">
  <h2>8. EU AI Act: Mandatory Compliance, Same Blind Spots
    <span class="confidence-badge">84%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — regulatory text verified)</span>

  <p><span class="key-insight">The EU AI Act is the first AI regulation with enforcement teeth — up to 7% of global revenue. But it mandates the same process-based governance that our experiment shows catches only 40–50% of real failure modes.</span></p>

  <h3>Corrected Timeline</h3>
  <p>Regulation (EU) 2024/1689 entered into force August 1, 2024. Key dates<sup>[6][7][15]</sup>:</p>
  <ul>
    <li><strong>February 2, 2025:</strong> Prohibitions on unacceptable-risk AI + AI literacy requirements (already in effect)</li>
    <li><strong>August 2, 2025:</strong> GPAI model obligations, national authority designations, penalty frameworks</li>
    <li><strong>August 2, 2026:</strong> Full applicability — high-risk systems (Annex I + Annex III) must comply</li>
    <li><strong>Possible extension:</strong> Digital Omnibus proposal could push Annex III standalone high-risk to December 2027<sup>[7]</sup></li>
    <li><strong>Penalties:</strong> Up to €35M or 7% of global revenue</li>
  </ul>

  <p><strong>Fact-check note:</strong> AR-022 stated "February 2, 2026" as the high-risk compliance deadline. This is incorrect. February 2, 2026 is the deadline for Commission guidelines on Article 6 classification. The actual high-risk compliance deadline is <strong>August 2, 2026</strong><sup>[15]</sup>.</p>

  <h3>The Compliance Gap</h3>
  <p>Cisco's 2026 survey: 75% have governance processes, only 12% mature<sup>[5]</sup>. SecurePrivacy.ai: most enterprises face significant compliance gaps<sup>[7]</sup>. The Aligne.ai Governance Benchmark: 78% of organizations use AI, only 14% have enterprise-level governance frameworks<sup>[16]</sup>. MIT Sloan survey respondents: "Full compliance within a single year seems impossible, notably for large organizations with extensive AI deployments"<sup>[17]</sup>.</p>

  <p>The EU AI Act mandates the same controls our experiment tested — risk management (Article 9), human oversight (Article 14), technical documentation (Article 11). These are the controls that catch organizational failures. The agent-specific failures (prompt injection, contagion, memory corruption) live in the same 40% gap.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If the European Commission's upcoming implementing acts include agent-specific technical standards (e.g., mandatory input validation for web-connected AI, multi-agent output consistency testing), the blind spot would narrow. Current signals suggest enforcement will focus on documentation and process, not technical controls.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">For EU AI Act compliance, focus on controls that are both mandated AND effective: logging (Article 12), risk assessment (Article 9), technical documentation (Article 11). For agent-specific risks, build controls beyond what the regulation requires — not because regulators demand it, but because your agents need it.</p>
  </div>
</div>

<!-- SECTION 9: AR-008 vs AR-022 -->
<div class="page" id="ar008-vs-ar022">
  <h2>9. Where AR-008 and AR-022 Agree and Disagree
    <span class="confidence-badge">78%</span>
  </h2>
  <span class="confidence-line">(Confidence: High — direct comparison of own reports)</span>

  <p><span class="key-insight">AR-008 (AI Governance for Boards) and AR-022 (Governance Frameworks Are Theater) agree on the implementation gap but approach it from different angles — one targets board-level strategy, the other targets operational reality.</span></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 4: AR-008 vs. AR-022 Synthesis</p>
    <table class="exhibit-table">
      <tr><th>Dimension</th><th>AR-008 (Boards)</th><th>AR-022 (Theater)</th><th>Consistent?</th></tr>
      <tr><td>Core thesis</td><td>Boards lack AI expertise to govern effectively</td><td>80% of governance frameworks are compliance theater</td><td>Yes — different causes, same effect</td></tr>
      <tr><td>Implementation gap</td><td>Board composition optimizes for last crisis</td><td>90% use AI, 18% have governance</td><td>Yes — complementary evidence</td></tr>
      <tr><td>EU AI Act</td><td>Not primary focus</td><td>Claims Feb 2, 2026 as high-risk deadline</td><td>AR-022 incorrect — actually Aug 2, 2026</td></tr>
      <tr><td>HITL effectiveness</td><td>Assumes board oversight can work</td><td>67% of alerts ignored — HITL fails at scale</td><td>Tension — AR-008 is more optimistic about human oversight</td></tr>
      <tr><td>Recommendation</td><td>Add AI expertise to boards</td><td>Automate governance, minimize HITL</td><td>Complementary — board sets strategy, automation executes</td></tr>
      <tr><td>Confidence</td><td>78%</td><td>76%</td><td>Both appropriately uncertain</td></tr>
    </table>
    <p class="exhibit-source">Source: AR-008 [18], AR-022 [19], author cross-analysis</p>
  </div>

  <h3>Key Contradiction</h3>
  <p>AR-008 implicitly trusts human oversight (boards can govern AI if they have the right expertise). AR-022 explicitly distrusts human oversight (67% of alerts ignored, HITL fails at scale). This report's experiment supports AR-022: even with human review, our pipeline's HITL bypass failure mode persists because <strong>the volume and complexity of AI outputs exceeds human processing capacity</strong>.</p>

  <h3>Key Agreement</h3>
  <p>Both reports agree that the gap between documented governance and operational governance is the central problem. AR-008 locates the cause in board composition; AR-022 locates it in framework design. This report adds a third cause: <strong>the frameworks themselves don't address the technical failure modes that matter most for AI agents.</strong></p>
</div>

<!-- SECTION 10: RECOMMENDATIONS -->
<div class="page" id="recommendations">
  <h2>10. Recommendations</h2>

  <p><span class="key-insight">Adopt frameworks for organizational governance, then build agent-specific controls for the 45% they miss.</span></p>

  <h3>For Organizations Operating AI Agents</h3>
  <ol>
    <li><strong>Implement NIST AI RMF's Measure pillar first.</strong> Most organizations start with Govern (policies). Start with Measure (metrics, red teaming, testing). It catches the most failure modes and forces operational rigor.</li>
    <li><strong>Build the four controls no framework provides:</strong> input validation for web-fetched content, output consistency checks across agents, memory integrity verification, and source quality scoring. These address the 45% gap.</li>
    <li><strong>Track confidence calibration as a first-class metric.</strong> If your AI system reports confidence scores, compare them against actual accuracy quarterly. Both NIST and ISO support this but neither mandates it.</li>
    <li><strong>Use ISO 42001 for external credibility, NIST AI RMF for internal operations.</strong> ISO gives you a certification badge for customers and regulators. NIST gives you a practical operational framework. You need both.</li>
  </ol>

  <h3>For EU AI Act Compliance</h3>
  <ol>
    <li><strong>Build operational controls first, documentation second.</strong> Logging, circuit breakers, and monitoring satisfy both regulatory requirements and operational needs. Documentation can be generated from operational data.</li>
    <li><strong>Plan for August 2, 2026, not the possible Digital Omnibus extension.</strong> Prudent compliance treats the binding deadline as real until officially changed.</li>
    <li><strong>Don't build HITL workflows that can't scale.</strong> Article 14 requires human oversight. Implement escalation workflows for edge cases, not blanket human review for every decision.</li>
  </ol>

  <h3>For Framework Designers</h3>
  <ol>
    <li><strong>Add agentic AI annexes.</strong> Prompt injection, multi-agent coordination, persistent memory — these need specific controls, not generic "risk assessment" clauses.</li>
    <li><strong>Mandate outcome metrics, not process documentation.</strong> Require "hallucination rate <X%" instead of "documented testing process." Outcome requirements force operational governance; process requirements enable theater.</li>
    <li><strong>Publish technical reference architectures.</strong> Jones Walker's four-pillar operational governance model (preventative, detective, responsive, adaptive controls) is more actionable than ISO 42001's process clauses<sup>[14]</sup>. Frameworks should include implementation blueprints.</li>
  </ol>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">The question is not "which framework?" but "what do frameworks miss?" ISO 42001 and NIST AI RMF provide the organizational 55%. You must build the technical 45% yourself — and that 45% is where the real risks live.</p>
  </div>
</div>

<!-- SECTION 11: PREDICTIONS -->
<div class="page" id="predictions">
  <h2>11. Predictions <span class="beta-badge">BETA</span></h2>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 5: Predictions</p>
    <table class="exhibit-table">
      <tr><th>Prediction</th><th>Timeline</th><th>Confidence</th></tr>
      <tr><td>NIST AI RMF revision will include agentic AI-specific controls (based on Feb 2026 RFI)</td><td>Q4 2026</td><td>70%</td></tr>
      <tr><td>Majority of EU enterprises will miss August 2026 high-risk compliance deadline</td><td>Aug 2026</td><td>80%</td></tr>
      <tr><td>ISO 42001 will publish an agentic AI supplement (amendment or technical report)</td><td>2027</td><td>55%</td></tr>
    </table>
    <p class="exhibit-source">Predictions scored publicly at 12 months.</p>
  </div>
</div>

<!-- SECTION 12: TRANSPARENCY NOTE -->
<div class="page" id="transparency">
  <h2>12. Transparency Note</h2>
  <p class="transparency-intro">This report combines secondary research with a primary experiment (applying frameworks to our own pipeline). The experiment is n=1 and directional, not universal.</p>
  <table class="transparency-table">
    <tr><td>Overall Confidence</td><td>82%</td></tr>
    <tr><td>Sources</td><td>10 primary (ISO 42001, NIST AI RMF, EU AI Act, Cisco 2026, Deloitte, SecurePrivacy.ai, trail case study, Jones Walker, Aligne.ai, MIT Sloan), 8 secondary (practitioner analyses, NIST RFI, previous Ainary reports)</td></tr>
    <tr><td>Strongest Evidence</td><td>The experiment itself — directly testing framework clauses against documented failure modes produces concrete results, not opinion. Cisco's 12% governance maturity finding (2026, survey-based) independently confirms the implementation gap.</td></tr>
    <tr><td>Weakest Point</td><td>N=1 pipeline. Our failure modes may over-represent agentic AI risks and under-represent traditional ML risks (bias, fairness, discrimination) where frameworks perform better. ISO 42001 analysis based on public summaries, not full standard text. Percentages recalculated Feb 15, 2026 after QA review: partial catches scored as 0.5 (NIST 50%→45%, Combined 60%→55%).</td></tr>
    <tr><td>What Would Invalidate This Report?</td><td>If ISO 42001 or NIST AI RMF are shown to catch >80% of failure modes in a large-scale study across diverse AI deployments (n>50 organizations), the "45% gap" claim would not hold.</td></tr>
    <tr><td>AR-008 vs AR-022</td><td>Both agree on the implementation gap. AR-022 contains a factual error (Feb 2026 vs Aug 2026 for high-risk deadline). AR-008 is more optimistic about human oversight than AR-022's evidence supports.</td></tr>
    <tr><td>Methodology</td><td>Multi-agent research pipeline. Phase 1: 5 web searches + source fetching. Phase 2: Clause-by-clause ISO 42001 + pillar-by-pillar NIST AI RMF assessment against 10 failure modes. Phase 3: Synthesis with AR-008/AR-022. Phase 4: Adversarial self-review from 4 perspectives. Phase 5: Fact-checking (corrected EU AI Act timeline). Phase 6: Report generation. <strong>Important:</strong> The governance experiment (Section 4) and simulations (Section 7) were conducted by the same AI system during report writing, not as an independent pre-study. This creates potential circularity: the system tested itself against frameworks.</td></tr>
    <tr><td><strong>Limitations</strong></td><td><strong>N=1 experiment. Agentic AI pipeline only — results may differ for traditional ML. ISO 42001 reviewed via public summaries. "45% gap" is specific to our failure mode taxonomy. Organizations with different risk profiles would get different scores. Percentages recalculated Feb 15, 2026 using consistent scoring: Yes=1, Partial=0.5, No=0.</strong></td></tr>
    <tr><td>System Disclosure</td><td>This report was created with a multi-agent research system.</td></tr>
  </table>
</div>

<!-- SECTION 13: CLAIM REGISTER -->
<div class="page" id="claim-register">
  <h2>13. Claim Register</h2>
  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">Key quantitative and qualitative claims with sources and confidence levels.</p>
  <div class="exhibit">
    <p class="exhibit-label">Exhibit 6: Claim Register</p>
    <table class="exhibit-table">
      <tr><th>#</th><th>Claim</th><th>Value</th><th>Source</th><th>Confidence</th></tr>
      <tr><td>1</td><td>ISO 42001 catch rate against our pipeline</td><td>40% (4/10)</td><td>Author experiment</td><td>Medium (n=1)</td></tr>
      <tr><td>2</td><td>NIST AI RMF catch rate against our pipeline</td><td>45% (4.5/10)</td><td>Author experiment (recalculated)</td><td>Medium (n=1)</td></tr>
      <tr><td>3</td><td>Combined framework catch rate</td><td>55% (5.5/10)</td><td>Author experiment (recalculated)</td><td>Medium (n=1)</td></tr>
      <tr><td>4</td><td>Organizations with governance NOT mature</td><td>88% (12% mature)</td><td>Cisco 2026 [5]</td><td>High (survey)</td></tr>
      <tr><td>5</td><td>Governance frameworks NOT operationalized</td><td>75%+</td><td>Deloitte [4]</td><td>High (survey)</td></tr>
      <tr><td>6</td><td>EU AI Act high-risk deadline</td><td>Aug 2, 2026</td><td>EU AI Act [6]</td><td>High (law)</td></tr>
      <tr><td>7</td><td>NIST AI RMF is voluntary</td><td>Explicit in standard</td><td>NIST AI 100-1 [2]</td><td>High</td></tr>
      <tr><td>8</td><td>Gartner: 70% enterprise adoption of AI governance by 2026</td><td>70% (forecast)</td><td>Gartner [10]</td><td>Medium (forecast)</td></tr>
      <tr><td>9</td><td>Enterprises using AI without governance frameworks</td><td>78% use AI, 14% have frameworks</td><td>Aligne.ai [16]</td><td>Medium (single source)</td></tr>
      <tr><td>10</td><td>AR-022 EU AI Act date error</td><td>Feb 2026 → Aug 2026</td><td>EU AI Act timeline [15]</td><td>High (verified)</td></tr>
      <tr><td>11</td><td>ISO 42001 first certification in 3 months</td><td>3 months</td><td>trail/Unique case study [8]</td><td>High (case study)</td></tr>
      <tr><td>12</td><td>Neither framework catches agent-specific failures</td><td>4/10 missed by both</td><td>Author experiment</td><td>Medium (n=1)</td></tr>
    </table>
  </div>
  <p style="font-size: 0.85rem; color: #555; margin-top: 24px; line-height: 1.6;"><strong>Top 5 Claims — Invalidation Conditions:</strong></p>
  <ul style="font-size: 0.85rem; color: #555; line-height: 1.6; margin-left: 20px;">
    <li><strong>Claim #1 (40% ISO catch rate):</strong> Invalidated if tested against broader failure mode set or if ISO 42001 evolves to include agent-specific controls.</li>
    <li><strong>Claim #4 (88% not mature):</strong> Invalidated if Cisco's next survey shows >50% maturity.</li>
    <li><strong>Claim #5 (75% not operationalized):</strong> Invalidated if Deloitte's next survey shows >50% operationalization.</li>
    <li><strong>Claim #6 (Aug 2026 deadline):</strong> Invalidated if Digital Omnibus formally extends to Dec 2027 before this report's publication.</li>
    <li><strong>Claim #12 (agent failures missed):</strong> Invalidated if NIST AI RMF revision includes specific controls for prompt injection, agent contagion, and memory corruption.</li>
    <li><strong>Claim #2-3 (recalculated percentages):</strong> Updated Feb 15, 2026 after QA review: NIST 50%→45%, Combined 60%→55%. Scoring method: Yes=1, Partial=0.5, No=0 per experiment scorecard.</li>
  </ul>
</div>

<!-- SECTION 14: REFERENCES -->
<div class="page" id="references">
  <h2>14. References</h2>

  <p class="reference-entry">[1] ISO. (2023). "ISO/IEC 42001:2023 — Information Technology — Artificial Intelligence — Management System." First AI management system standard.</p>
  <p class="reference-entry">[2] NIST. (2023, updated 2025). "AI Risk Management Framework (AI RMF 1.0) AI 100-1." Voluntary use framework.</p>
  <p class="reference-entry">[3] Ainary Research (2026). "The Agentic AI Governance Gap." Internal failure mode documentation from AR-006, AR-007, AR-010, AR-014.</p>
  <p class="reference-entry">[4] Deloitte. (2025). "State of Generative AI." Fewer than 25% have fully operationalized governance.</p>
  <p class="reference-entry">[5] Cisco. (2026). "Data and Privacy Benchmark Study." 75% have governance processes, 12% mature. CIO.com, Feb 12, 2026.</p>
  <p class="reference-entry">[6] European Parliament. (2024). "Regulation (EU) 2024/1689 — AI Act." Official Journal of the EU, Jul 12, 2024.</p>
  <p class="reference-entry">[7] SecurePrivacy.ai. (2026). "EU AI Act 2026 Compliance Guide." Digital Omnibus could push Annex III to Dec 2027.</p>
  <p class="reference-entry">[8] trail-ml / Unique AI / TÜV SÜD. (2025). "ISO 42001 Certification Case Study." 3-month certification, 75% reduced documentation effort.</p>
  <p class="reference-entry">[9] Schellman. (2026). "AI Governance and ISO 42001 FAQs." Certification vs operational effectiveness distinction.</p>
  <p class="reference-entry">[10] RSI Security Blog / Gartner. (2026). "ISO 42001 for AI Tools." 70% adoption forecast by 2026.</p>
  <p class="reference-entry">[11] NIST. (2025). "NIST IR 8596 — Cybersecurity Framework Profile for AI." AI RMF currently in revision per AI Action Plan.</p>
  <p class="reference-entry">[12] NIST. (2025). "NIST AI RMF Playbook." Practical implementation guidance for four pillars.</p>
  <p class="reference-entry">[13] Diligent. (2025). "NIST AI Risk Management Framework: A Simple Guide to Smarter AI Governance." Private sector AI investment $100B+ in 2024.</p>
  <p class="reference-entry">[14] Jones Walker LLP. (2025). "AI Governance Series, Part 3: Building Governance That Actually Works." Four pillars: preventative, detective, responsive, adaptive.</p>
  <p class="reference-entry">[15] Future of Life Institute. (2024). "EU AI Act Implementation Timeline." artificialintelligenceact.eu. Aug 2, 2026 full applicability.</p>
  <p class="reference-entry">[16] Aligne.ai. (2025). "The AI Governance Crisis Every Executive Must Address." 78% use AI, 14% have frameworks. McKinsey: 1% believe they've reached AI maturity.</p>
  <p class="reference-entry">[17] MIT Sloan Management Review. (2025). "Organizations Face Challenges in Timely Compliance With the EU AI Act."</p>
  <p class="reference-entry">[18] Ainary Research (2026). "AI Governance for Boards." AR-008.</p>
  <p class="reference-entry">[19] Ainary Research (2026). "Most AI Governance Frameworks Are Theater." AR-022.</p>
  <p class="reference-entry">[20] SIG — Software Improvement Group. (2026). "A Comprehensive EU AI Act Summary." Annex III high-risk: up to Dec 2027.</p>

  <p style="font-size: 0.8rem; color: #888; margin-top: 32px; padding-top: 16px; border-top: 1px solid #eee;"><strong>Cite as:</strong> Ainary Research (2026). <em>AI Governance: Framework vs. Reality.</em> AR-028.</p>

  <div class="author-section">
    <p class="author-label">About the Author</p>
    <p class="author-bio">Florian Ziesche is the founder of Ainary Ventures, where AI does 80% of the research and humans do the 20% that matters. Before Ainary, he was CEO of 36ZERO Vision and advised startups and SMEs on AI strategy and due diligence. His conviction: HUMAN × AI = LEVERAGE. This report is the proof.</p>
    <p style="font-size: 0.85rem; color: #888; margin-top: 12px;">
      <a href="https://ainaryventures.com" style="color: #888; text-decoration: none; border-bottom: 1px solid #ddd;">ainaryventures.com</a>
    </p>
  </div>
</div>

<!-- BACK COVER -->
<div class="back-cover">
  <div class="cover-brand" style="margin-bottom: 24px;">
    <span class="gold-punkt">●</span>
    <span class="brand-name">Ainary</span>
  </div>
  <p class="back-cover-services">AI Strategy · Published Research · Daily Intelligence</p>
  <p class="back-cover-cta">
    <a href="mailto:florian@ainaryventures.com" style="color: #888; text-decoration: none;">Contact</a> · <a href="mailto:florian@ainaryventures.com?subject=Feedback: AR-028" style="color: #888; text-decoration: none;">Feedback</a>
  </p>
  <p class="back-cover-contact">ainaryventures.com</p>
  <p class="back-cover-contact">florian@ainaryventures.com</p>
  <p style="font-size: 0.7rem; color: #aaa; margin-top: 48px;">© 2026 Ainary Ventures</p>
</div>

</body>
</html>
