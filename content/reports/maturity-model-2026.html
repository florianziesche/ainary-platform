<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The AI Agent Maturity Model — Florian Ziesche</title>
    <style>
        @font-face {
            font-family: 'Inter';
            src: url('/fonts/inter-variable.woff2') format('woff2');
            font-weight: 100 600;
            font-display: swap;
        }
        :root {
            --bg: #fafaf8;
            --text: #1a1a1a;
            --text-secondary: #555;
            --text-muted: #888;
            --gold: #c8aa50;
            --border: #e5e3dc;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Inter', system-ui, -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.75;
            font-size: 16px;
            -webkit-font-smoothing: antialiased;
        }
        .wrapper { max-width: 900px; margin: 0 auto; padding: 0 2rem; }

        /* Cover */
        .cover {
            text-align: center;
            padding: 5rem 2rem 4rem;
            border-bottom: 1px solid var(--border);
        }
        .cover-label {
            font-size: 0.75rem;
            font-weight: 600;
            letter-spacing: 3px;
            text-transform: uppercase;
            color: var(--text-muted);
            margin-bottom: 1.5rem;
        }
        .cover h1 {
            font-size: 2.4rem;
            font-weight: 600;
            line-height: 1.15;
            letter-spacing: -0.02em;
            margin-bottom: 1rem;
        }
        .cover .subtitle {
            font-size: 1.05rem;
            color: var(--text-secondary);
            max-width: 680px;
            margin: 0 auto 2rem;
            font-style: italic;
        }
        .cover .author {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        .cover .author strong { color: var(--text); font-weight: 500; }
        .cover .date {
            font-size: 0.8rem;
            color: var(--text-muted);
            margin-top: 0.3rem;
        }

        /* TOC */
        .toc {
            padding: 2.5rem 0;
            border-bottom: 1px solid var(--border);
        }
        .toc h2 {
            font-size: 0.75rem;
            font-weight: 600;
            letter-spacing: 2px;
            text-transform: uppercase;
            color: var(--text-muted);
            margin-bottom: 1rem;
        }
        .toc ol {
            list-style: none;
            counter-reset: toc;
        }
        .toc ol li {
            counter-increment: toc;
            margin-bottom: 0.4rem;
        }
        .toc ol li a {
            color: var(--text);
            text-decoration: none;
            font-size: 0.95rem;
            display: flex;
            align-items: center;
            gap: 0.6rem;
            transition: color 0.15s;
        }
        .toc ol li a:hover { color: var(--gold); }
        .toc ol li a::before {
            content: counter(toc, decimal-leading-zero);
            font-size: 0.75rem;
            font-weight: 600;
            color: var(--text-muted);
            min-width: 1.5rem;
        }

        /* Sections */
        .section {
            padding: 3rem 0;
            border-bottom: 1px solid var(--border);
        }
        .section-header {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            margin-bottom: 1.5rem;
        }
        .section-icon {
            width: 24px;
            height: 24px;
            color: var(--text-muted);
            flex-shrink: 0;
        }
        .section h2 {
            font-size: 1.4rem;
            font-weight: 600;
            line-height: 1.3;
        }
        .confidence {
            font-size: 0.8rem;
            color: var(--text-muted);
            font-weight: 400;
            margin-left: 0.5rem;
        }
        .section h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 2rem 0 0.75rem;
        }
        .section p { margin-bottom: 1rem; }
        .section strong { font-weight: 600; }

        /* So What + Invalidate = italic indented text */
        .so-what, .invalidate {
            margin: 1.5rem 0;
            padding-left: 1.5rem;
            font-style: italic;
            color: var(--text-secondary);
        }
        .so-what strong, .invalidate strong {
            font-style: normal;
            color: var(--text);
        }

        /* Footnote refs */
        .fn {
            color: var(--text-muted);
            font-size: 0.7em;
            vertical-align: super;
            text-decoration: none;
            line-height: 0;
        }
        .fn:hover { color: var(--gold); }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.88rem;
        }
        th {
            text-align: left;
            font-weight: 600;
            padding: 0.6rem 0.8rem;
            border-bottom: 2px solid var(--text);
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-secondary);
        }
        td {
            padding: 0.6rem 0.8rem;
            border-bottom: 1px solid var(--border);
            vertical-align: top;
        }
        tr:last-child td { border-bottom: none; }

        /* Lists */
        ul, ol { padding-left: 1.5rem; margin: 0.75rem 0; }
        li { margin-bottom: 0.3rem; }

        /* Links */
        a { color: var(--text); }

        /* Gold accent — CTA only */
        .cta {
            text-align: center;
            padding: 2rem 0;
            margin: 2rem 0;
        }
        .cta a {
            color: var(--gold);
            font-weight: 600;
            text-decoration: none;
            font-size: 1rem;
        }
        .cta a:hover { text-decoration: underline; }

        /* Key stat gold */
        .gold-stat { color: var(--gold); font-weight: 600; }

        /* Methodology small */
        .methodology {
            font-size: 0.9rem;
            color: var(--text-secondary);
            line-height: 1.7;
        }

        /* Self-assessment scoring */
        .scoring p {
            margin-bottom: 0.5rem;
        }

        /* Footer */
        .footer {
            padding: 3rem 0;
            text-align: center;
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        .footer a { color: var(--gold); text-decoration: none; }
        .footer a:hover { text-decoration: underline; }
        .footer .copyright { margin-top: 1rem; }

        /* Print */
        @media print {
            body { font-size: 10.5pt; background: #fff; color: #000; }
            .wrapper { max-width: 100%; padding: 0; }
            .cover { padding: 3rem 0 2rem; }
            .section { break-inside: avoid; }
            .toc { break-after: page; }
            a { color: #000; }
            .fn { color: #666; }
            .so-what, .invalidate { color: #333; }
            .gold-stat { color: #000; }
            .cta a { color: #000; }
            .footer a { color: #000; }
            @page { margin: 2cm; }
        }
    </style>
</head>
<body>

<div class="wrapper">

    <!-- COVER -->
    <div class="cover">
        <div class="cover-label">Research Report</div>
        <h1>The AI Agent Maturity Model</h1>
        <p class="subtitle">A Framework for Measuring How Ready Your Organization Actually Is</p>
        <p class="author"><strong>Florian Ziesche</strong> — Ainary Ventures</p>
        <p class="date">February 2026</p>
    </div>

    <!-- TOC -->
    <nav class="toc">
        <h2>Contents</h2>
        <ol>
            <li><a href="#exec-summary">Executive Summary</a></li>
            <li><a href="#methodology">Methodology</a></li>
            <li><a href="#s1">The Maturity Illusion</a></li>
            <li><a href="#s2">Why Existing Models Fail for Agents</a></li>
            <li><a href="#s3">The AGENT Framework — 5 Dimensions</a></li>
            <li><a href="#s4">The 5 Levels — From Playground to Organism</a></li>
            <li><a href="#s5">The 5-Minute Self-Assessment</a></li>
            <li><a href="#s6">The Level 1 → Level 3 Playbook</a></li>
            <li><a href="#s7">Why Level 3 Is the Real Goal for 2026</a></li>
            <li><a href="#s8">Predictions</a></li>
            <li><a href="#claim-register">Claim Register</a></li>
            <li><a href="#references">References</a></li>
        </ol>
    </nav>

    <!-- EXECUTIVE SUMMARY -->
    <div class="section" id="exec-summary">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/></svg>
            <h2>Executive Summary</h2>
        </div>

        <p>Read this in 30 seconds. Decide if the next 20 minutes are worth your time.</p>

        <p><strong>No existing AI maturity model accounts for what makes agents different from traditional AI.</strong> Gartner, McKinsey, Deloitte, Microsoft, and IBM all measure AI-as-tool. Agents aren't tools — they're actors that make decisions on your behalf. Every major framework has a blind spot the size of the entire agentic paradigm.</p>

        <p><strong>62% of enterprises experiment with AI agents, but fewer than 10% deploy them enterprise-wide, and only 6% see meaningful EBIT impact.</strong><a class="fn" href="#ref1">[1]</a><a class="fn" href="#ref2">[2]</a> The gap between experimentation and production is where most organizations live — and die.</p>

        <p><strong>The AGENT framework introduces 5 measurable dimensions — Autonomy, Governance, Error Handling, Networked Trust, and Team Integration — across 5 maturity levels.</strong> It's designed so a CTO can place their organization in under 5 minutes using 10 binary questions.</p>

        <p><strong>Level 3 ("Calibrated") is the survival threshold for 2026.</strong> EU AI Act enforcement begins August 2026. Organizations below Level 3 face regulatory exposure, and the compliance cost is $2-5M — but the cost of a single uncalibrated agent catastrophe will exceed <span class="gold-stat">$100M</span>.<a class="fn" href="#ref13">[13]</a></p>

        <p><strong>The model is a hypothesis, not gospel.</strong> It's built on patterns from CMMI and DORA applied to agent-specific research across 22 sources. It has not been empirically validated across enterprises. Use it as a starting diagnostic, not a certification.</p>
    </div>

    <!-- METHODOLOGY -->
    <div class="section" id="methodology">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><path d="M12 6v6l4 2"/></svg>
            <h2>Methodology</h2>
        </div>

        <p class="methodology">This framework synthesizes two categories of input: (1) a systematic review of 6 existing AI maturity models (Gartner, McKinsey, Deloitte, Microsoft, Google Cloud, IBM) to identify what they measure and what they miss, and (2) 15 research briefs on agent-specific phenomena — overconfidence calibration, adversarial attacks on multi-agent systems, memory poisoning, human-in-the-loop failure modes, non-human identity management, and regulatory convergence — totaling 22 primary and secondary sources. The maturity model structure draws on design principles from two proven precedents: CMMI (Carnegie Mellon, 1987–present) and DORA (Google, 2014–present), specifically their emphasis on outcome-based measurement, prescriptive levels, and self-assessability. All quantitative claims carry explicit source attribution and confidence ratings (High/Medium/Low). The model itself is a proposed framework — not an empirically validated assessment tool. It should be treated as a structured hypothesis about what agent readiness looks like, to be tested against real organizational data.</p>
    </div>

    <!-- SECTION 1 -->
    <div class="section" id="s1">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M10.29 3.86L1.82 18a2 2 0 0 0 1.71 3h16.94a2 2 0 0 0 1.71-3L13.71 3.86a2 2 0 0 0-3.42 0z"/><line x1="12" y1="9" x2="12" y2="13"/><line x1="12" y1="17" x2="12.01" y2="17"/></svg>
            <h2>Section 1: The Maturity Illusion <span class="confidence">(Confidence: High)</span></h2>
        </div>

        <p>Here's the picture: 62% of enterprises are experimenting with AI agents.<a class="fn" href="#ref2">[2]</a> Gartner projects 40% of enterprise applications will incorporate agentic AI by end of 2026.<a class="fn" href="#ref18">[18]</a> The agent market is forecast to grow from $7.8B to $52B by 2030 — a 45.8% CAGR.<a class="fn" href="#ref21">[21]</a></p>

        <p>Now here's the other picture: fewer than 10% of those experimenting organizations have deployed agents enterprise-wide.<a class="fn" href="#ref2">[2]</a> Only <strong>6%</strong> of enterprises qualify as "AI High Performers" with measurable EBIT impact, according to McKinsey's survey of 1,993 organizations.<a class="fn" href="#ref1">[1]</a> Only 54% of AI projects make it from pilot to production.<a class="fn" href="#ref4">[4]</a> And Gartner predicts more than 40% of agentic AI projects will be abandoned by 2027.<a class="fn" href="#ref3">[3]</a></p>

        <p><strong>Evidence:</strong> These numbers come from large-sample surveys (McKinsey n=1,993, Gartner enterprise data). The 6% figure is particularly robust — McKinsey defines "High Performer" as organizations attributing ≥5% of EBIT to AI, which is a measurable threshold, not self-assessment.<a class="fn" href="#ref1">[1]</a></p>

        <p><strong>Interpretation:</strong> The gap between experimentation rates (62%) and production deployment (&lt;10%) suggests a structural problem, not a timing problem. Organizations aren't slowly moving up a maturity curve — they're stuck.</p>

        <p>I believe the core issue is that every existing AI maturity model measures the wrong thing. They ask: "How well do you USE AI?" The right question for agents is: "How well do you GOVERN actors that make decisions on your behalf?"</p>

        <p>That distinction — tool versus actor — is why organizations think they're further along than they are. If you measure yourself against a tool-use framework, having ChatGPT Enterprise and a few LangChain workflows puts you at Level 3. If you measure yourself against an actor-governance framework, those same deployments are Level 1.</p>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If the 62% experimentation rate includes organizations with robust governance frameworks that simply haven't scaled yet (i.e., the bottleneck is business case, not maturity), then the "stuck at Level 1" thesis overstates the problem. I don't see evidence of this — McKinsey's data shows high performers are differentiated by workflow redesign (55% vs 20%), not by governance maturity — but it's possible.<a class="fn" href="#ref1">[1]</a>
        </div>

        <div class="so-what">
            <strong>So What?</strong> If you're a CTO reading this, the question isn't whether you're "doing AI agents." It's whether you could answer, right now: How many agents does your organization run? What was their error rate last month? What happens when one fails? If you can't answer those questions, you're at Level 1 — regardless of your AI budget.
        </div>
    </div>

    <!-- SECTION 2 -->
    <div class="section" id="s2">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M21 21l-6-6m2-5a7 7 0 1 1-14 0 7 7 0 0 1 14 0z"/></svg>
            <h2>Section 2: Why Existing Models Fail for Agents <span class="confidence">(Confidence: High)</span></h2>
        </div>

        <p>I reviewed 6 major AI maturity models. Here's what each measures — and what each misses.</p>

        <p><strong>Gartner AI Maturity Model</strong> uses 4 levels (Awareness → Active → Operational → Transformational) focused on organizational readiness, governance, and strategy alignment. It's the most comprehensive for traditional AI. But it contains zero dimensions for agent autonomy, inter-agent trust, memory architecture, or orchestration patterns.<a class="fn" href="#ref14">[14]</a></p>

        <p><strong>McKinsey</strong> doesn't publish a formal maturity model, but their annual State of AI report effectively creates a 3-tier classification: Experimenters → Scalers → High Performers. The insight is valuable — high performers redesign workflows, not just deploy tools. But "agents" as a distinct capability category? Absent.<a class="fn" href="#ref1">[1]</a></p>

        <p><strong>Deloitte's "State of AI in the Enterprise"</strong> (7th edition, 2024) uses Pathseekers → Explorers → Practitioners → Seasoned. Most enterprises cluster at "Explorers." Focus: ROI measurement, organizational change, talent gaps. Agent-specific dimensions: none.<a class="fn" href="#ref19">[19]</a></p>

        <p><strong>Microsoft AI Maturity Assessment</strong> has 5 levels (Foundational → Approaching → Aspirational → Mature → Transformational), focused on cloud infrastructure, data platform, and organizational culture. No scoring for orchestration, trust, memory, or autonomy.<a class="fn" href="#ref20">[20]</a></p>

        <p><strong>Google Cloud AI Maturity Scale</strong> uses 3 phases (Tactical → Strategic → Transformational). Google has explicitly named "Agent Trust" as a key 2026 theme — but hasn't updated their maturity framework to include it.<a class="fn" href="#ref26">[26]</a></p>

        <p><strong>IBM AI Ladder</strong> has 4 rungs (Collect → Organize → Analyze → Infuse). This is a data maturity model wearing AI clothes. Agent paradigm: entirely absent.<a class="fn" href="#ref22">[22]</a></p>

        <h3>The Common Blind Spot</h3>

        <p>Every model above shares the same assumption: AI is a capability you add to existing workflows. Agents break this assumption. An agent isn't a better Excel formula — it's a new employee who never sleeps, has perfect recall of whatever you put in its memory (verified or not), and makes decisions at machine speed with no natural pause for judgment.</p>

        <p>The right comparison isn't IT maturity. It's closer to HR maturity — how well do you onboard, credential, monitor, evaluate, and (when necessary) terminate autonomous actors?</p>

        <h3>What CMMI and DORA Got Right</h3>

        <p><strong>CMMI</strong> (Capability Maturity Model Integration), developed at Carnegie Mellon starting in 1987, succeeded because it was prescriptive — each level has specific process areas with concrete goals and practices — and because adoption was enforced through U.S. government contracts.<a class="fn" href="#ref16">[16]</a></p>

        <p><strong>DORA</strong> (DevOps Research and Assessment), now part of Google, succeeded because it measured outcomes, not inputs. Four numbers. Four tiers (Low → Elite). Annual benchmarking reports create competitive pressure.<a class="fn" href="#ref17">[17]</a></p>

        <p>The lesson: a maturity model works when it's prescriptive (CMMI) and outcome-based (DORA). Most AI maturity models are neither — they're descriptive and input-based. "Do you have a data strategy?" is an input question. "What's your agent error rate?" is an outcome question.</p>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If Gartner or McKinsey publishes an agent-specific maturity model before this framework gains traction, the novelty claim weakens. As of February 2026, none has. But the consulting industry moves fast when there's enterprise demand.
        </div>

        <div class="so-what">
            <strong>So What?</strong> If you're benchmarking your organization's AI maturity against any of these 6 frameworks, you're measuring the wrong thing. You're measuring how well you use AI as a tool. You need to measure how well you govern AI as an actor. That's what the AGENT framework does.
        </div>
    </div>

    <!-- SECTION 3 -->
    <div class="section" id="s3">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="3" width="18" height="18" rx="2" ry="2"/><line x1="3" y1="9" x2="21" y2="9"/><line x1="9" y1="3" x2="9" y2="21"/></svg>
            <h2>Section 3: The AGENT Framework — 5 Dimensions <span class="confidence">(Confidence: Medium)</span></h2>
        </div>

        <p>The 5 dimensions of the AGENT framework aren't arbitrary. Each maps to a documented failure mode in agentic AI systems.</p>

        <h3>A — Autonomy</h3>
        <p><em>What it measures:</em> How independently do your agents operate? What decisions can they make without human approval? How are autonomy boundaries defined and enforced?</p>
        <p><em>Why it matters:</em> The spectrum from "chatbot with tools" to "autonomous actor" is where most organizations lose track. Without explicit autonomy boundaries, agents either do too little (expensive human bottleneck) or too much (uncontrolled risk).</p>

        <h3>G — Governance</h3>
        <p><em>What it measures:</em> Policies, audit trails, compliance posture, agent lifecycle management. Can you explain to a regulator what your agents did last Tuesday?</p>
        <p><em>Why it matters:</em> EU AI Act enforcement begins August 2026. Maximum penalties: €35M or 7% of global revenue.<a class="fn" href="#ref13">[13]</a> Only 10% of organizations have a non-human identity strategy.<a class="fn" href="#ref14">[14]</a></p>

        <h3>E — Error Handling</h3>
        <p><em>What it measures:</em> Confidence calibration, failure recovery, graceful degradation. Do your agents know what they don't know?</p>
        <p><em>Why it matters:</em> <strong>84%</strong> of LLMs are overconfident across 351 tested scenarios.<a class="fn" href="#ref5">[5]</a> Verbalized confidence expressions (VCE) are systematically biased.<a class="fn" href="#ref6">[6]</a> If your error handling relies on agents self-reporting uncertainty, it doesn't work.</p>

        <h3>N — Networked Trust</h3>
        <p><em>What it measures:</em> Inter-agent trust, identity management, orchestration integrity. When Agent A delegates to Agent B, does anyone verify the handoff?</p>
        <p><em>Why it matters:</em> Multi-agent system hijacking succeeds 45-64% of the time in research settings.<a class="fn" href="#ref9">[9]</a> Memory injection attacks succeed at rates above 95%.<a class="fn" href="#ref10">[10]</a> All 12 tested prompt injection defenses have been broken.<a class="fn" href="#ref11">[11]</a></p>

        <h3>T — Team Integration</h3>
        <p><em>What it measures:</em> Human-agent collaboration design. Is human-in-the-loop effective, or is it theater?</p>
        <p><em>Why it matters:</em> 67% of security alerts are already ignored due to alert fatigue.<a class="fn" href="#ref8">[8]</a> In healthcare, false positive rates range from 80-99%.<a class="fn" href="#ref23">[23]</a> Adding "a human reviews it" doesn't make it safe — it makes it slow AND unsafe if the human is overwhelmed.</p>

        <h3>How the Dimensions Interact</h3>
        <p>These 5 dimensions aren't independent. Weak Governance undermines Networked Trust — you can't establish inter-agent trust without identity management. Poor Error Handling makes Team Integration impossible — humans can't meaningfully review outputs if they don't know the agent's confidence level. Low Autonomy can mask problems in every other dimension — if agents don't do much, you never discover your governance, error handling, trust, and collaboration gaps.</p>

        <p>This is why the model is progressive. You can't skip levels. Level 3's calibration requirements depend on Level 2's observability infrastructure, which depends on Level 1's basic awareness of what agents exist.</p>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If agent-specific failures turn out to be manageable through traditional IT governance frameworks (i.e., existing SOC/SIEM/IAM tools handle agent monitoring adequately), then agent-specific dimensions add complexity without value. Early evidence suggests otherwise — 23% of IT professionals report agent credential leaks through existing systems<a class="fn" href="#ref15">[15]</a> — but the data is thin.
        </div>

        <div class="so-what">
            <strong>So What?</strong> These 5 dimensions are a diagnostic lens, not a compliance checklist. Use them to identify which dimension is your weakest — that's where your next agent failure will come from.
        </div>
    </div>

    <!-- SECTION 4 -->
    <div class="section" id="s4">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><polyline points="22 12 18 12 15 21 9 3 6 12 2 12"/></svg>
            <h2>Section 4: The 5 Levels — From Playground to Organism <span class="confidence">(Confidence: Medium-High)</span></h2>
        </div>

        <h3>The AGENT Maturity Matrix</h3>

        <table>
            <thead>
                <tr>
                    <th>Level</th>
                    <th>Autonomy (A)</th>
                    <th>Governance (G)</th>
                    <th>Error Handling (E)</th>
                    <th>Networked Trust (N)</th>
                    <th>Team Integration (T)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>1: Ad Hoc</strong></td>
                    <td>Individual use, no boundaries</td>
                    <td>No policies</td>
                    <td>No tracking</td>
                    <td>No inter-agent awareness</td>
                    <td>No HITL design</td>
                </tr>
                <tr>
                    <td><strong>2: Managed</strong></td>
                    <td>Defined use cases</td>
                    <td>Agent inventory, basic guardrails</td>
                    <td>Incident tracking</td>
                    <td>Agent catalog</td>
                    <td>Basic escalation</td>
                </tr>
                <tr>
                    <td><strong>3: Calibrated</strong></td>
                    <td>Confidence-gated autonomy</td>
                    <td>Identity management, audit trails</td>
                    <td>Calibrated confidence, SLAs</td>
                    <td>Credential governance</td>
                    <td>Designed HITL with severity tiers</td>
                </tr>
                <tr>
                    <td><strong>4: Orchestrated</strong></td>
                    <td>Delegation rules, autonomy SLAs per agent class</td>
                    <td>Cross-agent audit, policy-as-code</td>
                    <td>Automated degradation, MTTR &lt;4h</td>
                    <td>Inter-agent trust scoring, anomaly detection MTTR &lt;2h</td>
                    <td>Escalation chains with measured response times</td>
                </tr>
                <tr>
                    <td><strong>5: Autonomous</strong></td>
                    <td>Self-adjusting boundaries, drift detection &lt;5%</td>
                    <td>Automated compliance, continuous audit</td>
                    <td>Self-calibration, ECE &lt;1%</td>
                    <td>Self-healing trust, &lt;15min recovery</td>
                    <td>Strategic oversight only, &lt;5% human intervention</td>
                </tr>
            </tbody>
        </table>

        <!-- Level 1 -->
        <h3>Level 1: Ad Hoc — "The Playground"</h3>

        <p>Individual employees experiment with AI agents. No organizational strategy exists. Agents are glorified chatbots with tool access. This is where the vast majority of organizations are — including many that believe otherwise.</p>

        <p><strong>Measurable Criteria:</strong></p>
        <ol>
            <li>Agents used by individuals, not teams — no shared configuration</li>
            <li>No version control on agent prompts or configurations</li>
            <li>No observability — you cannot determine what agents did yesterday</li>
            <li>No defined escalation path from agent to human</li>
            <li>Agent credentials are personal API keys</li>
        </ol>

        <p><strong>Typical Mistakes:</strong></p>
        <ul>
            <li>Treating ChatGPT Enterprise or Claude with tool use as "having an agent strategy"</li>
            <li>Demo-driven adoption: "Look what it can do!" without "What happens when it fails?"</li>
            <li>Logging agent-generated errors as human errors — the agent becomes invisible in your incident data</li>
        </ul>

        <p><strong>Example:</strong> A development team uses Cursor + Claude for code generation. Each developer has their own system prompts. Nobody tracks hallucination rates or measures code review rejection rates for agent-generated code. When an agent-generated bug hits production, it's logged as a developer bug. The organization has no idea how much of its codebase was agent-generated, what the quality differential is, or how to improve it.</p>

        <p><strong>What's Missing to Reach Level 2:</strong> Centralized visibility. You need to know what agents exist, what they're doing, and how often they fail — before you can govern any of it.</p>

        <!-- Level 2 -->
        <h3>Level 2: Managed — "The Dashboard"</h3>

        <p>The organization has visibility into agent activities. Basic guardrails exist. Agents are tracked, but trust is binary — on or off. There's a dashboard. Someone occasionally looks at it.</p>

        <p><strong>Measurable Criteria:</strong></p>
        <ol>
            <li>Centralized agent observability (logs, traces, cost tracking)</li>
            <li>Defined use cases: documented list of what agents MAY do</li>
            <li>Basic input/output validation (guardrails)</li>
            <li>Agent inventory: you know how many agents run in your organization</li>
            <li>Incident tracking for agent failures — separate from human error tracking</li>
        </ol>

        <p><strong>Typical Mistakes:</strong></p>
        <ul>
            <li>Observability without action: dashboards that nobody checks (94% of production agent developers report using observability tools<a class="fn" href="#ref24">[24]</a> — but observability ≠ governance)</li>
            <li>Guardrails as afterthought: bolted on after the first incident, not designed into the system</li>
            <li>"We have LangSmith" ≠ "We manage our agents"</li>
        </ul>

        <p><strong>Example:</strong> A fintech company deploys customer service agents via CrewAI. They track all conversations in Langfuse. They know cost per interaction ($0.35). They have guardrails that prevent the agent from discussing competitor products. But when the agent hallucinates a refund policy — confidently citing a policy that doesn't exist — nobody catches it until a customer screenshots it on Twitter. The observability was there. The governance wasn't.</p>

        <p><strong>Positive Counterexample:</strong> Klarna deployed customer service agents and reported $60M in annual savings.<a class="fn" href="#ref25">[25]</a> They had the dashboard. But they also discovered they had "overpivoted" — reducing human staff before the agent reliability warranted it. Even well-resourced, data-driven organizations can get stuck at Level 2 if they optimize for cost instead of trust.</p>

        <p><strong>What's Missing to Reach Level 3:</strong> Confidence scoring. Your agents need to know — and communicate — how certain they are. And that certainty needs to be calibrated against reality, not self-reported.</p>

        <!-- Level 3 -->
        <h3>Level 3: Calibrated — "The Trust Layer"</h3>

        <p>Agents produce measurable confidence scores. Outputs are calibrated — when an agent says it's 90% confident, it's right roughly 90% of the time. Human-in-the-loop is designed, not just required. Agent identity is managed with dedicated credentials. This is the minimum viable maturity for 2026.</p>

        <p><strong>Measurable Criteria:</strong></p>
        <ol>
            <li>Confidence scoring on agent outputs (calibrated, not self-reported via VCE)</li>
            <li>HITL triggers based on confidence thresholds (not random sampling)</li>
            <li>Agent identity management: dedicated credentials, not personal API keys</li>
            <li>Defined SLAs for agent reliability (e.g., &lt;2% hallucination rate for production tasks)</li>
            <li>Memory governance: agents don't accumulate unchecked, unversioned context</li>
        </ol>

        <p><strong>Typical Mistakes:</strong></p>
        <ul>
            <li>Using verbalized confidence ("I'm 85% sure") instead of calibrated methods. VCE is "systematically biased" — LLMs are overconfident 84% of the time.<a class="fn" href="#ref5">[5]</a><a class="fn" href="#ref6">[6]</a></li>
            <li>HITL without severity tiering leads directly to alert fatigue. 67% of SOC alerts are already ignored.<a class="fn" href="#ref8">[8]</a></li>
            <li>Memory without provenance: the agent "remembers" information nobody verified.</li>
            <li>Setting SLAs without the measurement infrastructure to enforce them.</li>
        </ul>

        <p><strong>Example (Hypothetical):</strong> An insurance company runs claims processing agents. Each output carries a confidence score from an external calibration method — a consistency-based approach that cross-checks the agent's output via multiple independent queries, costing approximately <span class="gold-stat">$0.005</span> per check.<a class="fn" href="#ref7">[7]</a> Claims below 85% confidence auto-escalate to a human reviewer. The reviewer sees context: why the agent is uncertain, what data points conflict, what the agent would have decided. The reviewer isn't asked "is this right?" — they're asked "given this uncertainty, what should we do?" Agent credentials are managed via dedicated service identities. Audit trails are complete. <em>No public example of a fully Level 3-mature organization exists yet — this scenario illustrates what the standard looks like in practice.</em></p>

        <p><strong>Evidence vs. Interpretation:</strong> The ~$0.005 calibration cost is estimated based on current API pricing for running consistency-based confidence checks.<a class="fn" href="#ref7">[7]</a> The 84% overconfidence rate is from a peer-reviewed study.<a class="fn" href="#ref5">[5]</a> The claim that Level 3 is "minimum viable for 2026" is my interpretation — it's based on EU AI Act requirements for human oversight (Article 14), but the Act doesn't reference a specific maturity level.<a class="fn" href="#ref13">[13]</a></p>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If VCE calibration improves dramatically (i.e., future LLMs can accurately self-assess confidence), then the case for external calibration weakens. Current research shows the opposite trend — larger models aren't better calibrated — but a breakthrough is possible.
        </div>

        <div class="so-what">
            <strong>So What?</strong> Level 3 is where trust becomes measurable. Below Level 3, you're running on vibes — hoping agents are right, assuming humans will catch errors, trusting that credentials won't leak. At Level 3, you have numbers. And numbers let you make decisions about where to expand autonomy, where to add oversight, and where to pull the plug.
        </div>

        <!-- Level 4 -->
        <h3>Level 4: Orchestrated — "The Network"</h3>

        <p>Multiple agents collaborate with defined trust relationships. Delegation, escalation, and inter-agent verification are systematic. Agents can challenge each other's outputs. The organization manages an agent network, not individual agents.</p>

        <p><strong>Measurable Criteria:</strong></p>
        <ol>
            <li>Multi-agent workflows with defined delegation rules and autonomy SLAs per agent class</li>
            <li>Inter-agent trust scoring: tracked as a rolling accuracy rate over the last 1,000 interactions</li>
            <li>Automated escalation chains (agent → agent → human) with MTTR &lt;4h for critical-path agents</li>
            <li>Cross-agent audit trail with end-to-end latency tracking</li>
            <li>Adversarial testing at least quarterly, with anomaly detection MTTR &lt;2h</li>
        </ol>

        <p><strong>Typical Mistakes:</strong></p>
        <ul>
            <li>Orchestration without trust: agents blindly pass outputs to each other. Multi-agent hijacking succeeds 45-64% of the time when trust isn't verified.<a class="fn" href="#ref9">[9]</a></li>
            <li>No cross-agent audit trail: you see what each agent did individually, but can't reconstruct the decision chain.</li>
            <li>Memory poisoning across agents: one compromised memory store infects the network. MINJA attacks succeed at rates above 95%.<a class="fn" href="#ref10">[10]</a></li>
        </ul>

        <p><strong>Example:</strong> A logistics company runs a multi-agent system: a demand forecasting agent feeds an inventory optimization agent, which feeds a procurement agent. Each agent's output includes a calibrated confidence score. The procurement agent won't execute purchase orders above $50K unless the forecasting agent's confidence exceeds 90% AND the optimization agent independently confirms. Weekly red-team exercises test for prompt injection, memory poisoning, and delegation manipulation.</p>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If single-agent systems prove sufficient for enterprise use cases (i.e., multi-agent orchestration turns out to be overengineered), then Level 4's criteria are unnecessarily complex. Some practitioners argue well-designed single agents outperform multi-agent systems. The jury is out.
        </div>

        <div class="so-what">
            <strong>So What?</strong> Level 4 is where agents become a system, not a collection of tools. It's also where the attack surface expands dramatically. If you're deploying multi-agent workflows without inter-agent trust verification, you're building a chain where every link is a potential point of compromise.
        </div>

        <!-- Level 5 -->
        <h3>Level 5: Autonomous — "The Organism"</h3>

        <p>Agent systems self-monitor, self-improve, and adapt. Human oversight is strategic (setting goals and boundaries), not tactical (reviewing individual decisions). The system learns from failures and automatically adjusts autonomy boundaries.</p>

        <p><strong>Measurable Criteria:</strong></p>
        <ol>
            <li>Agents adjust confidence thresholds based on historical accuracy, with drift alerts when thresholds deviate &gt;5% from target</li>
            <li>Automatic capability expansion/contraction via rolling 30-day accuracy window with defined triggers</li>
            <li>Self-healing: MTTR &lt;15min for compromised nodes without human intervention</li>
            <li>Continuous calibration: Expected Calibration Error (ECE) maintained below 1%</li>
            <li>Regulatory compliance automated, human intervention rate &lt;5% of decisions</li>
        </ol>

        <p><strong>Example:</strong> Nobody is here yet. This is the target state for 2027-2028. The closest analog is Waymo's autonomous driving system — continuous learning from real-world data, automated edge case detection, progressive autonomy expansion based on safety metrics. But even Waymo had a 1,212-unit recall for prediction software. If Waymo — purpose-built for autonomy — still has failure modes, general-purpose AI agent systems aren't reaching Level 5 any time soon.</p>

        <p><strong>Reality Check:</strong> Level 5 is aspirational. Including it defines the direction of travel and makes clear that premature autonomy — skipping the foundations — is a specific, identifiable failure mode.</p>

        <div class="so-what">
            <strong>So What?</strong> If anyone tells you they're at Level 5, they're either lying or they've redefined "autonomous" to mean something it doesn't. Use this as a filter.
        </div>
    </div>

    <!-- SECTION 5 -->
    <div class="section" id="s5">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M9 11l3 3L22 4"/><path d="M21 12v7a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h11"/></svg>
            <h2>Section 5: The 5-Minute Self-Assessment</h2>
        </div>

        <p>Ten questions. Binary answers. No ambiguity. Answer honestly — nobody's grading you.</p>

        <table>
            <thead>
                <tr>
                    <th>#</th>
                    <th>Question</th>
                    <th>If YES →</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>Do you know how many AI agents your organization runs right now?</td><td>Level 2</td></tr>
                <tr><td>2</td><td>Can you tell me the error rate of your agents from last month?</td><td>Level 2</td></tr>
                <tr><td>3</td><td>Do your agents produce confidence scores on their outputs?</td><td>Level 3</td></tr>
                <tr><td>4</td><td>Are those confidence scores calibrated against real outcomes (not self-reported)?</td><td>Level 3</td></tr>
                <tr><td>5</td><td>Do your agents have dedicated credentials (not personal API keys)?</td><td>Level 3</td></tr>
                <tr><td>6</td><td>When Agent A passes output to Agent B, does B independently verify it?</td><td>Level 4</td></tr>
                <tr><td>7</td><td>Do you have a cross-agent audit trail (not just per-agent logs)?</td><td>Level 4</td></tr>
                <tr><td>8</td><td>Do you red-team your agent systems at least quarterly?</td><td>Level 4</td></tr>
                <tr><td>9</td><td>Do your agents automatically adjust their own autonomy based on measured performance?</td><td>Level 5</td></tr>
                <tr><td>10</td><td>Does your agent system detect and recover from failures without human intervention?</td><td>Level 5</td></tr>
            </tbody>
        </table>

        <h3>How to Score</h3>

        <p><strong>Your level = the highest level where ALL questions for that level are answered "Yes."</strong></p>

        <div class="scoring">
            <p>If you answered No to questions 1 or 2: <strong>You're at Level 1.</strong> You don't have visibility.</p>
            <p>If you answered Yes to 1-2 but No to 3, 4, or 5: <strong>You're at Level 2.</strong> You have a dashboard. You don't have trust.</p>
            <p>If you answered Yes to 1-5 but No to 6, 7, or 8: <strong>You're at Level 3.</strong> You have calibration. You don't have orchestration.</p>
            <p>If you answered Yes to 1-8 but No to 9 or 10: <strong>You're at Level 4.</strong> You have a network. It's not self-governing.</p>
            <p>If you answered Yes to all 10: <strong>You're either at Level 5 or you're not being honest with yourself.</strong> Either way, I'd like to talk to you.</p>
        </div>

        <h3>The Honesty Problem</h3>

        <p>I predict 80%+ of organizations answering honestly will score Level 1.</p>

        <p>The reason organizations over-rate themselves is structural, not psychological. Most CTOs equate "we use AI agents" with "we're mature at AI agents." That's like equating "we have a website" with "we're mature at digital transformation." Usage isn't maturity. Governance is maturity.</p>

        <p>The self-assessment works only if you treat "Yes" as meaning "we have this in production, it's measured, and I could show you the data." Not "we're working on it" or "we have a plan for this" or "our vendor says they do this."</p>
    </div>

    <!-- SECTION 6 -->
    <div class="section" id="s6">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z"/></svg>
            <h2>Section 6: The Level 1 → Level 3 Playbook <span class="confidence">(Confidence: Medium)</span></h2>
        </div>

        <p>If the self-assessment put you at Level 1 — welcome to the majority. Here's what to do.</p>

        <h3>Step 0: Stop Adding False Confidence</h3>

        <p>Before building anything new, remove sources of false confidence. If your agents produce verbalized confidence ("I'm 87% sure about this") without external calibration, that number is worse than no number — it creates an illusion of reliability. Disable or ignore self-reported confidence until you have calibration infrastructure. This costs nothing and takes a day.</p>

        <h3>Step 1: Build the Inventory (Level 1 → Level 2, Timeline: 1-3 months)</h3>

        <p><strong>Do this first:</strong></p>
        <ul>
            <li>Catalog every AI agent in your organization. Include "shadow agents" — the ones employees are running on personal accounts.</li>
            <li>For each agent: what does it do, who deployed it, what credentials does it use, what data can it access?</li>
            <li>Set up centralized logging. Every agent action gets logged. Use LangSmith, Langfuse, or even a shared database — the tool matters less than the consistency.</li>
            <li>Create a separate incident category for agent failures. Don't log them as human errors.</li>
            <li>Define explicit use cases: what are agents allowed to do? Write it down. If it's not on the list, it's not allowed.</li>
        </ul>

        <p><strong>Cost:</strong> Low — primarily organizational effort. Observability tools: $0-500/month depending on scale.</p>

        <p><strong>The hard part:</strong> Shadow agents. In most organizations, individual employees are already using AI agents for tasks that never appear in any dashboard. The inventory will be uncomfortable.</p>

        <h3>Step 2: Add Calibration (Level 2 → Level 3, Timeline: 3-9 months)</h3>

        <p><strong>Do this second:</strong></p>
        <ul>
            <li>Implement external confidence calibration on agent outputs. Consistency-based methods cost approximately $0.005 per check.<a class="fn" href="#ref7">[7]</a> For an agent making 10,000 decisions per month, that's $50/month for calibrated trust.</li>
            <li>Design HITL triggers based on confidence thresholds. Not "review 10% randomly" — review everything below the confidence threshold you've set. Tier by severity.</li>
            <li>Migrate agent credentials from personal API keys to dedicated service identities. 90% of organizations haven't done this for agents.<a class="fn" href="#ref14">[14]</a></li>
            <li>Set measurable SLAs: hallucination rate, confidence calibration accuracy, escalation response time. Measure weekly.</li>
            <li>Implement memory governance: what goes into agent memory, how is it verified, when does it expire?</li>
        </ul>

        <p><strong>Cost:</strong> ~$0.005/check for calibration, plus identity management tooling. For context: the VW Cariad debacle resulted in $7.5B in losses.<a class="fn" href="#ref12">[12]</a> Level 3 maturity costs a fraction of a single governance failure.</p>

        <h3>Step 3: What to Stop Doing</h3>

        <ul>
            <li><strong>Stop using verbalized confidence as a decision input.</strong> It's systematically biased.<a class="fn" href="#ref5">[5]</a><a class="fn" href="#ref6">[6]</a></li>
            <li><strong>Stop treating all agent outputs equally.</strong> Without confidence scoring, high-stakes and low-stakes decisions get the same (non-)oversight.</li>
            <li><strong>Stop random-sampling for human review.</strong> It wastes human attention on outputs that don't need it and misses the ones that do.</li>
            <li><strong>Stop logging agent errors as human errors.</strong> You can't improve what you can't see.</li>
        </ul>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If the cost of calibration infrastructure exceeds the cost of agent failures for a given organization, then Level 3 may be over-engineering. For organizations using agents only for internal content generation, Level 2 might be sufficient. The playbook assumes agents are making decisions that affect customers, revenue, or compliance.
        </div>

        <div class="so-what">
            <strong>So What?</strong> The path from Level 1 to Level 3 is not a multi-year transformation program. It's 3-9 months of focused work. The first step — building an inventory — is free. The second step — adding calibration — costs $0.005 per decision. The barrier isn't cost or technology. It's the organizational willingness to admit you're at Level 1.
        </div>
    </div>

    <!-- SECTION 7 -->
    <div class="section" id="s7">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/></svg>
            <h2>Section 7: Why Level 3 Is the Real Goal for 2026 <span class="confidence">(Confidence: Medium-High)</span></h2>
        </div>

        <h3>The Regulatory Reality</h3>

        <p>EU AI Act enforcement begins August 2026. Article 14 requires human oversight for high-risk AI systems. Article 9 requires risk management systems. The maximum penalty: <strong>€35M or 7% of global annual revenue</strong>, whichever is higher.<a class="fn" href="#ref13">[13]</a></p>

        <p>Here's the translation into maturity levels:</p>
        <ul>
            <li><strong>Level 1 organizations</strong> cannot demonstrate human oversight because they don't know what their agents are doing.</li>
            <li><strong>Level 2 organizations</strong> can show dashboards but cannot demonstrate that human oversight is effective.</li>
            <li><strong>Level 3 organizations</strong> can demonstrate calibrated confidence scoring, designed HITL triggers, agent identity management, and audit trails — the minimum for Article 14 compliance.</li>
        </ul>

        <h3>The Insurance Angle</h3>

        <p>Agent liability insurance is emerging. The question insurers ask mirrors the maturity model: Can you demonstrate what your agents do? Can you show calibration data? Do you have audit trails? Organizations at Level 3 will get better terms — or get insured at all. Organizations at Level 1 are uninsurable.</p>

        <h3>Why Not Level 5?</h3>

        <p>Level 5 is a target for 2028+, not 2026. Aiming for Level 5 now is counterproductive — it leads to premature autonomy, which is the single most dangerous failure mode in the model. The right goal for 2026 is Level 3: calibrated, governed, auditable.</p>

        <h3>The Trilemma</h3>

        <ol>
            <li><strong>Deploy fast</strong> — competitive pressure says move now</li>
            <li><strong>Deploy compliant</strong> — regulatory pressure says move carefully</li>
            <li><strong>Don't deploy</strong> — risk avoidance says don't move at all</li>
        </ol>

        <p>Level 3 is the resolution. It's the minimum maturity that lets you deploy agents in high-risk categories with regulatory defensibility and measurable trust. Below Level 3, you're choosing between speed (and liability) or safety (and irrelevance).</p>

        <div class="invalidate">
            <strong>What would invalidate this?</strong> If EU AI Act enforcement is significantly delayed or watered down, the regulatory urgency for Level 3 diminishes. The Act is law, but enforcement precedents will take time to establish. Organizations gambling on weak enforcement may be right in the short term — but they're building on sand.
        </div>

        <div class="so-what">
            <strong>So What?</strong> Level 3 isn't aspirational. It's operational. If you deploy agents that touch customer data, financial decisions, or any high-risk category — and you're below Level 3 — you have a compliance gap with a deadline. August 2026.
        </div>
    </div>

    <!-- SECTION 8 -->
    <div class="section" id="s8">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
            <h2>Section 8: Predictions <span class="confidence">(Confidence: Low-Medium)</span></h2>
        </div>

        <p>I present these as structured bets, not certainties. Each includes what would prove me wrong.</p>

        <h3>Prediction 1: A &gt;$100M Agent Catastrophe Within 12 Months</h3>
        <p><strong>Confidence: 55%</strong></p>
        <p>An organization will suffer a &gt;$100M loss directly attributable to an AI agent failure. The ingredients exist: 45-64% multi-agent hijacking success rates<a class="fn" href="#ref9">[9]</a>, &gt;95% memory poisoning success rates<a class="fn" href="#ref10">[10]</a>, and expanding agent deployment in financial and healthcare contexts.</p>
        <p><em>What would prove me wrong:</em> If organizations are deploying agents more conservatively than the hype suggests, the attack surface may be smaller than I think.</p>

        <h3>Prediction 2: Agent Maturity Assessments Become a Procurement Requirement by 2027</h3>
        <p><strong>Confidence: 65%</strong></p>
        <p>Just as SOC 2 became a procurement prerequisite for SaaS vendors, agent maturity assessments will become a requirement for enterprises buying agentic AI systems.</p>
        <p><em>What would prove me wrong:</em> If the market consolidates around 2-3 agent platforms that handle governance internally, the need for external maturity assessments diminishes.</p>

        <h3>Prediction 3: Level 3 Becomes Table Stakes for Enterprise AI by 2028</h3>
        <p><strong>Confidence: 60%</strong></p>
        <p>Within 3 years, organizations deploying AI agents without calibrated confidence, HITL design, and identity management will be viewed the way organizations without basic cybersecurity are viewed today: negligent.</p>
        <p><em>What would prove me wrong:</em> If foundational models become inherently well-calibrated, Level 3's calibration requirements become redundant.</p>

        <h3>Prediction 4: The "DORA for Agents" Moment</h3>
        <p><strong>Confidence: 50%</strong></p>
        <p>A standardized benchmarking framework will create the same competitive pressure for agent maturity that DORA created for DevOps. Annual reports. Public benchmarks. CTOs citing their agent maturity level in board presentations.</p>
        <p><em>What would prove me wrong:</em> If agentic AI turns out to be a feature, not a paradigm, then a dedicated maturity framework is solving a problem that doesn't persist.</p>
    </div>

    <!-- CONCLUSION -->
    <div class="section" id="conclusion">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M18 8A6 6 0 0 0 6 8c0 7-3 9-3 9h18s-3-2-3-9"/><path d="M13.73 21a2 2 0 0 1-3.46 0"/></svg>
            <h2>Conclusion: The Model Is a Mirror</h2>
        </div>

        <p>The AGENT framework isn't a certification. It's a mirror.</p>

        <p>It reflects where your organization actually is — not where your AI vendor's marketing deck says you are, not where your internal champions hope you are, but where the evidence puts you when you answer 10 honest questions.</p>

        <p>Most organizations will look in this mirror and see Level 1. That's uncomfortable. It's also the starting point for everything useful.</p>

        <p>The path forward is concrete: build an inventory (free), add observability (weeks), implement calibration ($0.005/check), design HITL that respects human attention, manage agent identities like you manage employee identities. Level 3 in 6-9 months. That's the goal.</p>

        <p>The alternative is what Gartner predicts: &gt;40% of agentic AI projects abandoned by 2027.<a class="fn" href="#ref3">[3]</a> Not because the technology failed. Because the organizations deploying it weren't mature enough to govern what they built.</p>

        <p>The model is open. Use it, adapt it, prove it wrong. Just don't ignore the question it asks: <strong>How well do you govern actors that make decisions on your behalf?</strong></p>

        <p>If you can't answer that in 5 minutes, you have your answer.</p>
    </div>

    <!-- CLAIM REGISTER -->
    <div class="section" id="claim-register">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"/><rect x="8" y="2" width="8" height="4" rx="1" ry="1"/></svg>
            <h2>Appendix A: Claim Register</h2>
        </div>

        <table>
            <thead>
                <tr>
                    <th>#</th>
                    <th>Claim</th>
                    <th>Value</th>
                    <th>Source</th>
                    <th>Confidence</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>Only 6% are AI High Performers</td><td>6% (n=1,993)</td><td>McKinsey 2025</td><td>High</td></tr>
                <tr><td>2</td><td>62% experiment, &lt;10% enterprise-wide</td><td>62% / &lt;10%</td><td>McKinsey 2025</td><td>High</td></tr>
                <tr><td>3</td><td>&gt;40% agentic projects canceled by 2027</td><td>&gt;40%</td><td>Gartner 2025</td><td>Medium</td></tr>
                <tr><td>4</td><td>54% pilot→production</td><td>54%</td><td>Gartner 2024</td><td>Medium</td></tr>
                <tr><td>5</td><td>LLM overconfidence rate</td><td>84% (9 models, 351 scenarios)</td><td>PMC/12249208</td><td>High</td></tr>
                <tr><td>6</td><td>VCE systematically biased</td><td>Confirmed</td><td>arXiv:2602.00279</td><td>High</td></tr>
                <tr><td>7</td><td>External calibration cost</td><td>~$0.005/check</td><td>API pricing estimate</td><td>Medium</td></tr>
                <tr><td>8</td><td>SOC alerts ignored</td><td>67%</td><td>Vectra 2023 (n=2,000)</td><td>High</td></tr>
                <tr><td>9</td><td>MAS hijacking success</td><td>45-64%</td><td>arXiv:2503.12188</td><td>High</td></tr>
                <tr><td>10</td><td>MINJA success rate</td><td>&gt;95%</td><td>arXiv:2503.03704</td><td>High</td></tr>
                <tr><td>11</td><td>All prompt injection defenses broken</td><td>12/12</td><td>arXiv:2510.09023</td><td>High</td></tr>
                <tr><td>12</td><td>VW Cariad loss</td><td>$7.5B</td><td>VW public filing</td><td>High</td></tr>
                <tr><td>13</td><td>EU AI Act max penalty</td><td>€35M / 7%</td><td>Legislative text</td><td>High</td></tr>
                <tr><td>14</td><td>Orgs with NHI strategy</td><td>10%</td><td>WEF 2025</td><td>Medium</td></tr>
                <tr><td>15</td><td>Agent credential leaks</td><td>23%</td><td>Okta</td><td>Medium</td></tr>
                <tr><td>16</td><td>CMMI history</td><td>Verified</td><td>CMU SEI / ISACA</td><td>High</td></tr>
                <tr><td>17</td><td>DORA metrics</td><td>Verified</td><td>Google DORA</td><td>High</td></tr>
                <tr><td>18</td><td>40% apps with agents by 2026</td><td>40%</td><td>Gartner</td><td>Medium</td></tr>
                <tr><td>19</td><td>Deloitte AI maturity</td><td>4 tiers</td><td>Deloitte 2024</td><td>High</td></tr>
                <tr><td>20</td><td>Microsoft AI maturity</td><td>5 levels</td><td>Microsoft 2024</td><td>High</td></tr>
                <tr><td>21</td><td>Agent market forecast</td><td>$7.8B→$52B</td><td>Precedence Research</td><td>Medium</td></tr>
                <tr><td>22</td><td>IBM AI Ladder</td><td>4 rungs</td><td>IBM 2023</td><td>High</td></tr>
                <tr><td>23</td><td>Healthcare false positive</td><td>80-99%</td><td>PMC6904899</td><td>High</td></tr>
                <tr><td>24</td><td>94% use observability</td><td>94%</td><td>LangChain</td><td>Medium</td></tr>
                <tr><td>25</td><td>Klarna $60M saved</td><td>$60M</td><td>CEO earnings call</td><td>High</td></tr>
                <tr><td>26</td><td>Google Agent Trust theme</td><td>Confirmed</td><td>Google Cloud Blog</td><td>Medium</td></tr>
            </tbody>
        </table>
    </div>

    <!-- REFERENCES -->
    <div class="section" id="references">
        <div class="section-header">
            <svg class="section-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"/></svg>
            <h2>Appendix B: References</h2>
        </div>

        <p id="ref1" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[1] McKinsey & Company, "The State of AI in 2025," McKinsey Global Survey (n=1,993), 2025.</p>
        <p id="ref2" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[2] McKinsey & Company, "The State of AI in 2025" — agent experimentation and deployment data, 2025.</p>
        <p id="ref3" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[3] Gartner, "Predicts 2025: AI Agents" — projection on agentic project cancellation rates, 2025.</p>
        <p id="ref4" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[4] Gartner, "4 Levels of AI Maturity and How to Achieve Them" — pilot-to-production conversion rate, 2024.</p>
        <p id="ref5" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[5] PMC/12249208, "Overconfidence in Large Language Models" — study of 9 LLMs across 351 scenarios.</p>
        <p id="ref6" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[6] arXiv:2602.00279, "Verbalized Confidence Expressions in LLMs: Calibration and Reliability," January 2026.</p>
        <p id="ref7" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[7] Cost estimate based on API pricing for consistency-based confidence calibration (3 SLM calls per check). See also: Vashurin et al., "CoCoA: A Minimum Bayes Risk Framework," ICLR 2026.</p>
        <p id="ref8" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[8] Vectra AI, "2023 State of Threat Detection" — survey of 2,000 SOC analysts, 2023.</p>
        <p id="ref9" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[9] arXiv:2503.12188, "Hijacking Attacks on Multi-Agent Systems" — 45-64% success rates.</p>
        <p id="ref10" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[10] arXiv:2503.03704, "MINJA: Memory Injection Attacks on Multi-Agent Systems" — >95% success rates.</p>
        <p id="ref11" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[11] arXiv:2510.09023, Meta, "The Rule of Two: Adversarial Prompt Injection" — 12/12 defenses broken.</p>
        <p id="ref12" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[12] Volkswagen AG public filings — Cariad software unit cumulative losses of $7.5B.</p>
        <p id="ref13" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[13] European Parliament, "Regulation (EU) 2024/1689 — Artificial Intelligence Act" — Articles 9, 14; penalty Article 99.</p>
        <p id="ref14" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[14] World Economic Forum, "Navigating the AI-Cyber Nexus," 2025 — 10% with NHI strategy.</p>
        <p id="ref15" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[15] Okta, "The State of Digital Identity" — 23% agent credential leak incidents.</p>
        <p id="ref16" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[16] Carnegie Mellon SEI / ISACA CMMI Institute — CMMI V3.0, 1987-2023.</p>
        <p id="ref17" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[17] Google DORA Research Program (dora.dev) — 4 metrics, 4 tiers.</p>
        <p id="ref18" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[18] Gartner, "Top Strategic Technology Trends 2026" — 40% agent incorporation forecast.</p>
        <p id="ref19" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[19] Deloitte, "State of AI in the Enterprise," 7th edition, 2024.</p>
        <p id="ref20" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[20] Microsoft AI Maturity Assessment Tool, 2024.</p>
        <p id="ref21" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[21] Precedence Research — AI agent market forecast ($7.8B → $52B, 45.8% CAGR).</p>
        <p id="ref22" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[22] IBM "AI Ladder" framework, 2023.</p>
        <p id="ref23" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[23] PMC6904899 — False positive rates in AI-assisted clinical diagnostics (80-99%), 2019.</p>
        <p id="ref24" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[24] LangChain, "State of Agent Engineering" — 94% observability adoption.</p>
        <p id="ref25" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[25] Klarna CEO earnings call — $60M annual savings from AI agents.</p>
        <p id="ref26" style="font-size:0.85rem;color:var(--text-secondary);margin-bottom:0.5rem;">[26] Google Cloud Blog, 2025-2026 — Agent Trust as key theme.</p>
    </div>

    <!-- CTA -->
    <div class="cta">
        <p style="margin-bottom:0.5rem;color:var(--text-secondary);font-size:0.95rem;">This report is a proposed framework, not an empirically validated assessment.<br>It is designed to be tested, challenged, and improved.</p>
        <p><a href="mailto:florian@ainaryventures.com">Get in touch → florian@ainaryventures.com</a></p>
    </div>

    <!-- FOOTER -->
    <div class="footer">
        <p><a href="mailto:florian@ainaryventures.com">florian@ainaryventures.com</a> · <a href="https://ainaryventures.com">ainaryventures.com</a></p>
        <p class="copyright">© 2026 Florian Ziesche. All rights reserved.</p>
    </div>

</div>

</body>
</html>