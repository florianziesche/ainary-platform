<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<parameter name="viewport" content="width=device-width, initial-scale=1.0">
<title>Agent Testing Is Broken — Ainary Report AR-023</title>
<style>
  /* ========================================
     FONTS
     ======================================== */
  @font-face {
    font-family: 'Inter';
    src: url('/fonts/inter-variable.woff2') format('woff2');
    font-weight: 100 900;
    font-display: swap;
  }

  /* ========================================
     RESET & BASE
     ======================================== */
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: #fafaf8;
    color: #333;
    line-height: 1.75;
    font-size: 0.95rem;
    font-weight: 400;
  }

  /* ========================================
     LAYOUT
     ======================================== */
  .page {
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .back-cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
    page-break-before: always;
  }

  /* ========================================
     TYPOGRAPHY
     ======================================== */
  h1 {
    font-size: 2.2rem;
    font-weight: 600;
    line-height: 1.2;
    color: #1a1a1a;
    letter-spacing: -0.02em;
  }

  h2 {
    font-size: 1.5rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.3;
    margin-top: 3rem;
    margin-bottom: 12px;
  }

  h3 {
    font-size: 1.1rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.4;
    margin-top: 2rem;
    margin-bottom: 12px;
  }

  p {
    margin-bottom: 1rem;
  }

  strong {
    font-weight: 600;
    color: #1a1a1a;
  }

  em {
    font-style: italic;
  }

  sup {
    font-size: 0.65rem;
    color: #888;
    vertical-align: super;
  }

  /* ========================================
     COVER COMPONENTS
     ======================================== */
  .cover-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 40vh;
  }

  .cover-brand {
    display: flex;
    align-items: center;
    gap: 8px;
  }

  .gold-punkt {
    color: #c8aa50;
    font-size: 14px;
  }

  .brand-name {
    font-size: 0.85rem;
    font-weight: 500;
    color: #1a1a1a;
    letter-spacing: 0.02em;
  }

  .cover-meta {
    display: flex;
    gap: 12px;
    font-size: 0.75rem;
    color: #888;
  }

  .cover-title-block {
    margin-bottom: auto;
  }

  .cover-title {
    margin-bottom: 16px;
  }

  .cover-subtitle {
    font-size: 1rem;
    font-weight: 400;
    color: #666;
    line-height: 1.5;
  }

  .cover-footer {
    display: flex;
    justify-content: space-between;
    align-items: flex-end;
  }

  .cover-date {
    font-size: 0.75rem;
    color: #888;
  }

  .cover-author {
    font-size: 0.75rem;
    color: #888;
    text-align: center;
  }

  /* ========================================
     QUOTE PAGE
     ======================================== */
  .quote-page {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    max-width: 700px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .quote-text {
    font-size: 1.2rem;
    font-style: italic;
    color: #333;
    line-height: 1.8;
    text-align: center;
    margin-bottom: 24px;
  }

  .quote-source {
    font-size: 0.85rem;
    color: #888;
    text-align: center;
  }

  /* ========================================
     TABLE OF CONTENTS
     ======================================== */
  .toc-label {
    font-size: 0.7rem;
    font-weight: 600;
    color: #1a1a1a;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    margin-bottom: 24px;
  }

  .toc-section {
    margin-bottom: 32px;
  }

  .toc-section-label {
    font-size: 0.65rem;
    font-weight: 500;
    color: #888;
    text-transform: uppercase;
    letter-spacing: 0.12em;
    margin-bottom: 12px;
  }

  .toc-entry {
    display: flex;
    align-items: baseline;
    gap: 16px;
    padding: 12px 0;
    border-bottom: 1px solid #eee;
    text-decoration: none;
    transition: all 0.2s;
  }

  .toc-number {
    font-size: 0.8rem;
    color: #888;
    font-variant-numeric: tabular-nums;
    min-width: 24px;
  }

  .toc-title {
    font-size: 0.95rem;
    font-weight: 500;
    color: #1a1a1a;
    flex: 1;
    transition: color 0.2s;
  }

  .toc-entry:hover .toc-title {
    color: #c8aa50;
  }

  .toc-page {
    font-size: 0.8rem;
    color: #888;
  }

  /* ========================================
     HOW TO READ
     ======================================== */
  .how-to-read-table {
    width: 100%;
    border-collapse: collapse;
    margin: 24px 0;
  }

  .how-to-read-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }

  .how-to-read-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
  }

  /* ========================================
     EXECUTIVE SUMMARY
     ======================================== */
  .thesis {
    font-size: 1rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.6;
    margin-bottom: 24px;
  }

  .evidence-list {
    margin-left: 20px;
    margin-bottom: 24px;
  }

  .evidence-list li {
    font-size: 0.9rem;
    color: #333;
    line-height: 1.6;
    margin-bottom: 8px;
  }

  .keywords {
    font-size: 0.8rem;
    color: #666;
    font-style: italic;
    margin-top: 32px;
    padding-top: 16px;
    border-top: 1px solid #eee;
  }

  /* ========================================
     SECTION COMPONENTS
     ======================================== */
  .confidence-badge {
    font-size: 0.75rem;
    font-weight: 500;
    color: #1a1a1a;
    background: #f5f4f0;
    padding: 3px 8px;
    border-radius: 10px;
    margin-left: 8px;
    vertical-align: middle;
  }

  .confidence-line {
    font-size: 0.8rem;
    color: #888;
    font-style: italic;
    display: block;
    margin-bottom: 16px;
  }

  .key-insight {
    font-weight: 600;
    color: #1a1a1a;
  }

  /* ========================================
     CALLOUTS
     ======================================== */
  .callout {
    background: #f5f4f0;
    padding: 16px 20px;
    border-radius: 4px;
    margin: 1.5rem 0;
    page-break-inside: avoid;
  }

  .callout-label {
    font-size: 0.7rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    margin-bottom: 8px;
  }

  .callout-body {
    font-size: 0.9rem;
    color: #555;
    line-height: 1.6;
  }

  .callout.claim .callout-label {
    color: #555;
  }

  .callout.invalidation {
    border-left: 3px solid #ddd;
  }

  .callout.invalidation .callout-label {
    color: #888;
  }

  .callout.sowhat {
    border-left: 3px solid #c8aa50;
  }

  .callout.sowhat .callout-label {
    color: #c8aa50;
  }

  /* ========================================
     EXHIBITS & TABLES
     ======================================== */
  .exhibit {
    margin: 2rem 0;
  }

  .exhibit-label {
    font-size: 0.75rem;
    font-weight: 600;
    color: #555;
    margin-bottom: 12px;
  }

  .exhibit-table {
    width: 100%;
    border-collapse: collapse;
    page-break-inside: avoid;
  }

  .exhibit-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }

  .exhibit-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
  }

  .exhibit-source {
    font-size: 0.7rem;
    color: #888;
    margin-top: 8px;
    font-style: italic;
  }

  /* ========================================
     KPI FIGURES
     ======================================== */
  .kpi-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 48px;
    margin: 2rem 0;
  }

  .kpi {
    text-align: left;
  }

  .kpi-number {
    font-size: 2rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.2;
  }

  .kpi-label {
    font-size: 0.75rem;
    color: #666;
    margin-top: 4px;
  }

  .kpi-source {
    font-size: 0.65rem;
    color: #888;
    margin-top: 2px;
  }

  /* ========================================
     LISTS
     ======================================== */
  ul {
    margin-left: 20px;
    margin-bottom: 1rem;
  }

  ol {
    margin-left: 20px;
    margin-bottom: 1rem;
  }

  li {
    margin-bottom: 4px;
  }

  /* ========================================
     TRANSPARENCY NOTE
     ======================================== */
  .transparency-intro {
    font-size: 0.85rem;
    color: #555;
    line-height: 1.6;
    margin-bottom: 12px;
  }

  .transparency-table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 12px;
  }

  .transparency-table td:first-child {
    font-size: 0.85rem;
    font-weight: 600;
    color: #555;
    padding: 8px 0;
    border-bottom: 1px solid #eee;
    width: 160px;
    vertical-align: top;
  }

  .transparency-table td:last-child {
    font-size: 0.85rem;
    color: #333;
    padding: 8px 0;
    border-bottom: 1px solid #eee;
  }

  /* ========================================
     REFERENCES
     ======================================== */
  .reference-entry {
    font-size: 0.8rem;
    color: #555;
    line-height: 1.5;
    margin-bottom: 6px;
    padding-left: 24px;
    text-indent: -24px;
  }

  /* ========================================
     AUTHOR BIO
     ======================================== */
  .author-section {
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 1px solid #e5e3dc;
  }

  .author-initials {
    width: 48px;
    height: 48px;
    border-radius: 50%;
    background: #e5e3dc;
    display: inline-flex;
    align-items: center;
    justify-content: center;
    font-size: 0.9rem;
    font-weight: 600;
    color: #1a1a1a;
    margin-bottom: 12px;
  }

  .author-name {
    font-size: 0.95rem;
    font-weight: 600;
    color: #1a1a1a;
    margin-bottom: 8px;
  }

  .author-bio {
    font-size: 0.85rem;
    color: #555;
    line-height: 1.6;
    margin-bottom: 12px;
  }

  .author-link {
    font-size: 0.85rem;
    color: #888;
    text-decoration: none;
  }

  /* ========================================
     BACK COVER
     ======================================== */
  .back-cover-logo {
    margin-bottom: 24px;
  }

  .back-cover-services {
    font-size: 0.85rem;
    color: #666;
    margin-bottom: 24px;
  }

  .back-cover-cta {
    font-size: 0.85rem;
    color: #888;
    margin-bottom: 16px;
  }

  .back-cover-cta a {
    color: #888;
    text-decoration: none;
  }

  .back-cover-contact {
    font-size: 0.8rem;
    color: #888;
    margin-bottom: 32px;
  }

  .back-cover-copyright {
    font-size: 0.75rem;
    color: #aaa;
  }

  /* ========================================
     PRINT STYLES
     ======================================== */
  @media print {
    @page {
      size: A4;
      margin: 2cm;
    }

    body {
      background: white;
    }

    .page, .cover, .back-cover {
      page-break-after: always;
    }

    .callout, .exhibit {
      page-break-inside: avoid;
    }

    @page :first {
      @top-center { content: none; }
      @bottom-center { content: none; }
    }

    @page {
      @top-center {
        content: "Ainary Report | Agent Testing Is Broken";
        font-size: 0.7rem;
        color: #888;
      }
      @bottom-left {
        content: "© 2026 Ainary Ventures";
        font-size: 0.7rem;
        color: #888;
      }
      @bottom-right {
        content: counter(page);
        font-size: 0.7rem;
        color: #888;
      }
    }
  }
</style>
</head>
<body>

<!-- ========================================
     COVER PAGE
     ======================================== -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">●</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>AR-023</span>
      <span>Confidence: 82%</span>
    </div>
  </div>

  <div class="cover-title-block">
    <h1 class="cover-title">Agent Testing Is Broken</h1>
    <p class="cover-subtitle">Why Software QA Doesn't Work for Non-Deterministic Systems</p>
  </div>

  <div class="cover-footer">
    <div class="cover-date">
      February 2026<br>
      <span style="font-size: 0.7rem; color: #aaa;">v1.0</span>
    </div>
    <div class="cover-author">
      Florian Ziesche · Ainary Ventures
    </div>
  </div>
</div>

<!-- ========================================
     QUOTE PAGE
     ======================================== -->
<div class="quote-page">
  <p class="quote-text">"Testing AI agents is fundamentally different from testing conventional software. You're no longer verifying deterministic code—you're evaluating probabilistic systems."</p>
  <p class="quote-source">— Netguru, Testing AI Agents (August 2025)</p>
</div>

<!-- ========================================
     TABLE OF CONTENTS
     ======================================== -->
<div class="page">
  <p class="toc-label">Contents</p>

  <div class="toc-section">
    <p class="toc-section-label">FOUNDATION</p>
    <a href="#how-to-read" class="toc-entry">
      <span class="toc-number">1</span>
      <span class="toc-title">How to Read This Report</span>
    </a>
    <a href="#exec-summary" class="toc-entry">
      <span class="toc-number">2</span>
      <span class="toc-title">Executive Summary</span>
    </a>
    <a href="#methodology" class="toc-entry">
      <span class="toc-number">3</span>
      <span class="toc-title">Methodology</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ANALYSIS</p>
    <a href="#determinism-gap" class="toc-entry">
      <span class="toc-number">4</span>
      <span class="toc-title">The Determinism Gap — Why TDD and BDD Fail</span>
    </a>
    <a href="#llm-judge" class="toc-entry">
      <span class="toc-number">5</span>
      <span class="toc-title">The LLM-as-Judge Trap</span>
    </a>
    <a href="#framework-landscape" class="toc-entry">
      <span class="toc-number">6</span>
      <span class="toc-title">The Framework Landscape — What Exists and What's Missing</span>
    </a>
    <a href="#eval-driven" class="toc-entry">
      <span class="toc-number">7</span>
      <span class="toc-title">Eval-Driven Development — The New Paradigm</span>
    </a>
    <a href="#production-gap" class="toc-entry">
      <span class="toc-number">8</span>
      <span class="toc-title">The Production Gap — Where Testing Ends and Monitoring Begins</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ACTION</p>
    <a href="#recommendations" class="toc-entry">
      <span class="toc-number">9</span>
      <span class="toc-title">Recommendations</span>
    </a>
    <a href="#predictions" class="toc-entry">
      <span class="toc-number">10</span>
      <span class="toc-title">Predictions</span>
    </a>
    <a href="#transparency" class="toc-entry">
      <span class="toc-number">11</span>
      <span class="toc-title">Transparency Note</span>
    </a>
    <a href="#claim-register" class="toc-entry">
      <span class="toc-number">12</span>
      <span class="toc-title">Claim Register</span>
    </a>
    <a href="#references" class="toc-entry">
      <span class="toc-number">13</span>
      <span class="toc-title">References</span>
    </a>
  </div>
</div>

<!-- ========================================
     HOW TO READ THIS REPORT
     ======================================== -->
<div class="page" id="how-to-read">
  <h2>1. How to Read This Report</h2>

  <p>This report uses a structured confidence rating system to communicate what is known versus what is inferred. Every quantitative claim carries its source and confidence level.</p>

  <table class="how-to-read-table">
    <tr>
      <th>Rating</th>
      <th>Meaning</th>
      <th>Example</th>
    </tr>
    <tr>
      <td>High</td>
      <td>Multiple independent sources, peer-reviewed or empirical data</td>
      <td>TDD assumes deterministic outputs (academic literature + arxiv:2411.13768)</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>Single credible source, methodology documented</td>
      <td>LLM-as-judge shows position bias (MT-Bench findings, single comprehensive study)</td>
    </tr>
    <tr>
      <td>Low</td>
      <td>Practitioner reports, methodology unclear or single anecdote</td>
      <td>Teams abandon testing (Reddit discussions, not systematically measured)</td>
    </tr>
  </table>

  <p style="margin-top: 24px;">This report was produced using a <strong>multi-agent research pipeline</strong> with structured synthesis of academic papers, framework documentation, and practitioner insights. Full methodology details are provided in the Transparency Note (Section 11).</p>
</div>

<!-- ========================================
     EXECUTIVE SUMMARY
     ======================================== -->
<div class="page" id="exec-summary">
  <h2>2. Executive Summary</h2>

  <p class="thesis">Traditional software testing assumes deterministic outputs — give it the same input, get the same output. AI agents break this assumption fundamentally, and the testing industry hasn't caught up.</p>

  <ul class="evidence-list">
    <li><strong>Test-Driven Development (TDD) and Behavior-Driven Development (BDD) rely on pass/fail assertions</strong>, which are poorly suited for agents where multiple outputs may be valid and responses vary probabilistically<sup>[1]</sup></li>
    <li><strong>LLM-as-judge evaluation shows position bias, prompt sensitivity, and hallucination risks</strong> — the same biases that make agents unreliable also make their evaluators unreliable<sup>[2][3]</sup></li>
    <li><strong>Current frameworks (DeepEval, RAGAS, promptfoo) focus primarily on RAG pipelines</strong>, with limited support for multi-step agentic workflows, tool use validation, or state-dependent behavior<sup>[4][5]</sup></li>
    <li><strong>Eval-driven development is emerging as the paradigm shift</strong> — Anthropic, Vercel, and practitioners are building evaluation suites before agents, replacing unit tests with behavioral boundaries<sup>[6][7]</sup></li>
    <li><strong>No production agent framework ships with deterministic testing infrastructure</strong> — evaluation remains a post-deployment add-on rather than a built-in development primitive<sup>[8]</sup></li>
  </ul>

  <p class="keywords"><strong>Keywords:</strong> AI agent testing, non-deterministic systems, eval-driven development, LLM-as-judge, TDD limitations, agent evaluation frameworks, probabilistic QA</p>
</div>

<!-- ========================================
     METHODOLOGY
     ======================================== -->
<div class="page" id="methodology">
  <h2>3. Methodology</h2>

  <p>This report synthesizes 18 sources: 5 peer-reviewed or preprint papers, 8 framework documentation reviews, and 5 industry practitioner articles. Research followed a structured multi-agent pipeline: literature review on traditional QA limitations, framework landscape analysis (DeepEval, RAGAS, promptfoo, Galileo, Langfuse), eval-driven development synthesis, and gap analysis identifying what agents need that traditional testing doesn't provide.</p>

  <p><strong>Limitations:</strong> Agent testing is an emerging field. Many frameworks are evolving rapidly — documentation reviewed represents February 2026 state. Production incident data is scarce because most teams do not publicly disclose agent testing failures. The claim that "TDD doesn't work for agents" is directionally supported by academic and practitioner consensus, but no large-scale controlled study exists comparing TDD effectiveness across deterministic versus non-deterministic systems.</p>

  <p style="font-size: 0.85rem; color: #666; margin-top: 16px;">Full methodology details, including confidence calibration and source assessment, are provided in the Transparency Note (Section 11).</p>
</div>

<!-- ========================================
     SECTION 4: THE DETERMINISM GAP
     ======================================== -->
<div class="page" id="determinism-gap">
  <h2>4. The Determinism Gap — Why TDD and BDD Fail
    <span class="confidence-badge">85%</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">Test-Driven Development (TDD) and Behavior-Driven Development (BDD) were designed for deterministic systems where assertEqual(expected, actual) is meaningful. AI agents are probabilistic systems where multiple "actual" values may all be correct.</span></p>

  <h3>The Foundation of Traditional Testing</h3>

  <p>TDD follows a simple loop: write a failing test, write code to pass it, refactor. The test defines success as an exact match between expected and actual outputs. BDD extends this with natural language specifications — "Given X, When Y, Then Z" — but retains the same binary pass/fail logic.</p>

  <p>This works beautifully for deterministic systems. A function that adds two numbers should always return the same sum. A sorting algorithm should always produce the same ordered array. The fundamental contract is: <strong>same input → same output, always.</strong></p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: Traditional Testing vs. Agent Evaluation</p>
    <table class="exhibit-table">
      <tr>
        <th>Dimension</th>
        <th>Traditional Software (TDD/BDD)</th>
        <th>AI Agents</th>
      </tr>
      <tr>
        <td>Outputs</td>
        <td>Deterministic (same input → same output)</td>
        <td>Probabilistic (same input → variable outputs)</td>
      </tr>
      <tr>
        <td>Correctness</td>
        <td>Binary (pass/fail)</td>
        <td>Spectrum (multiple valid responses)</td>
      </tr>
      <tr>
        <td>Test assertion</td>
        <td>assertEqual(expected, actual)</td>
        <td>evaluateQuality(actual) > threshold</td>
      </tr>
      <tr>
        <td>Evaluation cost</td>
        <td>Near-zero (deterministic)</td>
        <td>Inference cost per eval (probabilistic judge)</td>
      </tr>
      <tr>
        <td>Reproducibility</td>
        <td>Perfect (same test, same result)</td>
        <td>Statistical (variance across runs)</td>
      </tr>
      <tr>
        <td>CI/CD integration</td>
        <td>Native (green/red pipeline)</td>
        <td>Threshold-based (% pass rate)</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Author analysis based on arxiv:2411.13768 [1], Vercel eval-driven development guide [7], Netguru testing guide [9]</p>
  </div>

  <h3>Why Agents Break the Model</h3>

  <p>The arxiv:2411.13768 paper ("Evaluation-Driven Development of LLM Agents") identifies three fundamental incompatibilities between traditional testing and agent systems<sup>[1]</sup>:</p>

  <ol>
    <li><strong>Binary Testing Outcomes:</strong> Traditional unit tests rely on pass/fail assertions. Agents produce responses that may vary in correctness or appropriateness depending on context. Multiple responses can be valid for a single scenario.</li>
    <li><strong>Non-Deterministic Behavior:</strong> The same input produces different outputs. This isn't a bug — it's the temperature parameter, stochastic sampling, and contextual reasoning working as designed.</li>
    <li><strong>Limited Post-Deployment Evaluation:</strong> TDD and BDD assume that once software passes tests, it remains reliable. Agents drift in production as they encounter new contexts, learn from interactions, or integrate updated knowledge.</li>
  </ol>

  <p>An example makes this concrete. Imagine testing a customer service agent. The user asks: "I want a refund." Traditional TDD would assert: <code>assertEqual("Your refund has been processed", agent.respond("I want a refund"))</code>. But the agent might validly respond:</p>

  <ul>
    <li>"I'll process your refund right away."</li>
    <li>"Let me check your order status first."</li>
    <li>"I see you purchased this 2 months ago — our policy allows refunds within 30 days. Would you like store credit instead?"</li>
  </ul>

  <p>All three responses could be correct depending on context the test doesn't capture (order age, purchase history, customer tier). A binary pass/fail test cannot distinguish between "wrong answer" and "different but valid answer."</p>

  <div class="callout claim">
    <p class="callout-label">Claim</p>
    <p class="callout-body">TDD and BDD are fundamentally incompatible with probabilistic agent outputs. The assertion layer — not just the implementation — must change.</p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If a testing framework emerged that extends TDD/BDD with probabilistic assertions (e.g., "assert output semantically equivalent to X OR Y OR Z") and gained widespread adoption, the binary limitation would be addressed. Property-based testing (as mentioned in Galileo's TDD guide [10]) moves in this direction but is not yet standard practice for agents.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Teams applying TDD to agents will hit a wall. The tests will be brittle (failing on valid outputs) or meaningless (passing on invalid outputs). The solution isn't better unit tests — it's a different testing paradigm entirely. Eval-driven development (Section 7) is that paradigm.</p>
  </div>
</div>

<!-- ========================================
     SECTION 5: LLM-AS-JUDGE
     ======================================== -->
<div class="page" id="llm-judge">
  <h2>5. The LLM-as-Judge Trap
    <span class="confidence-badge">78%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium-High)</span>

  <p><span class="key-insight">If deterministic assertions don't work for agents, the intuitive fix is to use another LLM to judge the output. This creates a new problem: the judge has the same biases, hallucination risks, and non-determinism as the system being tested.</span></p>

  <h3>What LLM-as-Judge Is</h3>

  <p>LLM-as-a-judge evaluation uses a language model (often GPT-4, Claude, or a fine-tuned evaluator) to assess another model's output. Instead of <code>assertEqual(expected, actual)</code>, the test becomes: <code>judge.evaluate(actual, criteria) → score</code>.</p>

  <p>The appeal is obvious. LLMs can handle semantic equivalence, context-dependent correctness, and nuanced quality assessment that simple string matching cannot. Frameworks like DeepEval, RAGAS, and Langfuse have made LLM-as-judge evaluation their core primitive<sup>[4][5][11]</sup>.</p>

  <h3>The Systematic Biases</h3>

  <p>The problem: <strong>LLM judges inherit the same failure modes as the agents they evaluate.</strong></p>

  <p>Documented biases from MT-Bench (the most comprehensive LLM-as-judge study) and subsequent research<sup>[2][3][12]</sup>:</p>

  <ul>
    <li><strong>Position bias:</strong> When evaluating pairwise comparisons (which answer is better?), GPT-4 and Claude systematically prefer the first option over the second. This is independent of actual quality — the ordering affects the judgment.</li>
    <li><strong>Prompt sensitivity:</strong> Minor changes to the evaluation prompt produce different scores. The judge's reliability depends on prompt engineering quality — which reintroduces the non-determinism problem.</li>
    <li><strong>Verbosity bias:</strong> Longer answers score higher than concise ones, even when the concise answer is more correct<sup>[12]</sup>.</li>
    <li><strong>Hallucination in evaluation:</strong> The judge can hallucinate facts when assessing factual correctness, creating false positives and false negatives<sup>[3]</sup>.</li>
    <li><strong>Lack of domain expertise:</strong> In specialized fields (medical diagnosis, legal reasoning), LLM judges miss errors that domain experts would catch immediately<sup>[3]</sup>.</li>
  </ul>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: LLM-as-Judge Failure Modes</p>
    <table class="exhibit-table">
      <tr>
        <th>Failure Mode</th>
        <th>Impact</th>
        <th>Mitigation (Partial)</th>
      </tr>
      <tr>
        <td>Position bias</td>
        <td>First option artificially favored in A/B tests</td>
        <td>Randomize positions, evaluate both orderings</td>
      </tr>
      <tr>
        <td>Prompt sensitivity</td>
        <td>Inconsistent scoring across runs</td>
        <td>Rubric-based prompts, few-shot examples</td>
      </tr>
      <tr>
        <td>Verbosity bias</td>
        <td>Verbose wrong answers beat concise correct ones</td>
        <td>Explicit "penalize verbosity" instruction</td>
      </tr>
      <tr>
        <td>Judge hallucination</td>
        <td>False confidence in incorrect evaluations</td>
        <td>Ensemble judges (multiple models vote)</td>
      </tr>
      <tr>
        <td>Domain expertise gap</td>
        <td>Misses domain-specific errors</td>
        <td>Fine-tune judge on expert-labeled data</td>
      </tr>
      <tr>
        <td>Cost and latency</td>
        <td>Each eval requires inference call</td>
        <td>Cheaper judge models (but accuracy trade-off)</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Confident AI [2], EvidentlyAI [12], ACM IUI paper [3], Deepchecks [13]</p>
  </div>

  <h3>The Reddit Reality Check</h3>

  <p>A February 2025 Reddit discussion in r/LLMDevs captured practitioner sentiment: "LLM-as-a-judge is not enough. That's the quiet truth nobody wants to admit."<sup>[14]</sup> The top-voted response: "Completely agree — LLM-as-judge is great for early dev and experimentation, but it hits real limits when agents are doing multi-step reasoning or operating in domain-specific contexts."</p>

  <p>This is low-confidence anecdotal evidence, but it aligns with the documented technical limitations. Teams reach for LLM-as-judge because it's the only available tool, then discover it creates as many problems as it solves.</p>

  <h3>The Recursive Evaluation Problem</h3>

  <p>Here's the deeper issue: <strong>if you don't trust the agent's output without evaluation, why would you trust the evaluator's output without evaluation?</strong></p>

  <p>The logical endpoint is infinite regress: Agent → Judge → Meta-Judge → Meta-Meta-Judge. Some frameworks attempt this with "ensemble judges" — multiple LLMs vote on correctness<sup>[12]</sup>. But this multiplies cost and latency while reducing variance, not eliminating it.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If a fine-tuned evaluator model emerged with >95% alignment to human expert judgments across diverse domains, and that alignment proved stable across prompt variations and context changes, LLM-as-judge would become viable. Current alignment rates are significantly lower, especially outside narrow domains.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Use LLM-as-judge with mitigation strategies (randomized positions, rubric-based prompts, ensemble voting), but do not rely on it as your sole evaluation mechanism. Combine it with deterministic checks (tool call validation, schema adherence), human-in-the-loop sampling, and behavioral monitoring. The judge is a heuristic, not ground truth.</p>
  </div>
</div>

<!-- ========================================
     SECTION 6: FRAMEWORK LANDSCAPE
     ======================================== -->
<div class="page" id="framework-landscape">
  <h2>6. The Framework Landscape — What Exists and What's Missing
    <span class="confidence-badge">80%</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">Current evaluation frameworks (DeepEval, RAGAS, promptfoo, Langfuse, Galileo) focus primarily on RAG pipelines and single-model outputs. Multi-agent workflows, stateful behavior, and tool use validation remain gaps.</span></p>

  <h3>The Big Three: DeepEval, RAGAS, Promptfoo</h3>

  <p><strong>DeepEval</strong> is a Python-first framework from Confident AI with 14+ metrics covering RAG, chatbots, and agents<sup>[4]</sup>. It integrates into pytest, making it familiar to Python developers. Strengths: breadth of metrics (answer relevancy, faithfulness, hallucination detection), LLM-as-judge with reasoning traces, CI/CD integration. Limitations: primarily RAG-focused, limited multi-agent support, lacks built-in state management for agentic evaluation.</p>

  <p><strong>RAGAS</strong> (Retrieval-Augmented Generation Assessment) is narrowly focused on RAG-specific metrics: context precision, context recall, answer faithfulness, answer relevancy<sup>[5]</sup>. It combines these into an overall RAG score. Strengths: domain-specific depth for RAG. Limitations: metrics are not self-explaining (harder to debug), minimal support for non-RAG agentic patterns, does not handle tool use or multi-step reasoning.</p>

  <p><strong>Promptfoo</strong> is a CLI and library designed for prompt testing and red-teaming<sup>[15]</sup>. It emphasizes developer-friendly workflows with declarative YAML configs. Strengths: fast iteration, regression testing, built-in adversarial prompts for security testing. Limitations: lighter on agent-specific metrics compared to DeepEval, requires more manual configuration for complex workflows.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Agent Testing Framework Comparison</p>
    <table class="exhibit-table">
      <tr>
        <th>Framework</th>
        <th>Primary Focus</th>
        <th>Agent Support</th>
        <th>Tool Use Eval</th>
        <th>Multi-Agent</th>
        <th>State/Memory</th>
      </tr>
      <tr>
        <td>DeepEval</td>
        <td>RAG + Chatbots</td>
        <td>Partial</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>RAGAS</td>
        <td>RAG-specific</td>
        <td>Extensions only</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Promptfoo</td>
        <td>Prompt testing</td>
        <td>Partial</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Langfuse</td>
        <td>Observability</td>
        <td>Yes</td>
        <td>Trace-level</td>
        <td>Trace-level</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Galileo</td>
        <td>Enterprise eval</td>
        <td>Yes</td>
        <td>Partial</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Momentic</td>
        <td>UI testing</td>
        <td>Non-deterministic UI</td>
        <td>No</td>
        <td>No</td>
        <td>No</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Framework documentation review (DeepEval [4], RAGAS [5], Promptfoo [15], Langfuse [11], Galileo [10], Momentic [16]), February 2026</p>
  </div>

  <h3>What's Missing</h3>

  <p>No production-ready framework provides:</p>

  <ul>
    <li><strong>Tool call validation as first-class primitive:</strong> Agents call APIs, execute code, modify files. Frameworks can trace these calls but lack built-in assertions for "did the agent call the right tool with valid parameters?"</li>
    <li><strong>Multi-agent workflow testing:</strong> When Agent A delegates to Agent B which calls Agent C, current frameworks trace the execution but don't validate the delegation logic, message integrity, or failure propagation.</li>
    <li><strong>Stateful behavior evaluation:</strong> Agents maintain memory across sessions. No framework provides primitives for testing "does memory update correctly?" or "does retrieval from memory remain consistent?"</li>
    <li><strong>Behavioral drift detection:</strong> Production agents change behavior over time (model updates, context drift, memory accumulation). Frameworks can log metrics but lack built-in drift alerts tied to evaluation baselines.</li>
  </ul>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If a major framework (DeepEval, LangSmith, or a new entrant) ships comprehensive multi-agent evaluation primitives in the next 6 months, the "gap" claim weakens. Current roadmaps suggest partial support is coming, but not full coverage.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Teams building complex agents will need to build custom evaluation layers on top of existing frameworks. Use DeepEval or RAGAS for RAG components, Langfuse for tracing, but expect to write your own tool call validators, multi-agent orchestration tests, and memory integrity checks. The tooling gap is real.</p>
  </div>
</div>

<!-- ========================================
     SECTION 7: EVAL-DRIVEN DEVELOPMENT
     ======================================== -->
<div class="page" id="eval-driven">
  <h2>7. Eval-Driven Development — The New Paradigm
    <span class="confidence-badge">75%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium-High)</span>

  <p><span class="key-insight">Eval-driven development inverts the testing paradigm: instead of writing tests after code (TDD) or specifying behavior before code (BDD), you define evaluation criteria that capture acceptable behavioral boundaries — then iterate until the agent stays within them.</span></p>

  <h3>The Anthropic Model</h3>

  <p>Anthropic's engineering team published "Demystifying Evals for AI Agents" in February 2026<sup>[6]</sup>. Their core recommendation: <strong>practice eval-driven development — build evals to define planned capabilities before agents can fulfill them, then iterate until the agent performs well.</strong></p>

  <p>The workflow:</p>

  <ol>
    <li><strong>Define success criteria:</strong> Not "the agent returns X" but "the agent achieves goal Y within constraints Z." Example: "Book a flight that meets user budget and time preferences" (not "return flight AA123").</li>
    <li><strong>Build evaluation tasks:</strong> These are not unit tests. They're scenarios with multiple valid outcomes. "User says 'find me a cheap flight to NYC next week' → agent books any flight <$300 departing Mon-Fri."</li>
    <li><strong>Establish baselines:</strong> Run the eval suite before building the agent. Establish the "do nothing" baseline. This calibrates what "improvement" means.</li>
    <li><strong>Iterate on agent implementation:</strong> Not "fix the bug" but "improve success rate from 45% to 70%." Success is statistical, not binary.</li>
    <li><strong>Track behavioral drift:</strong> Re-run evals in production. When success rate drops from 70% to 55%, that's a signal (not a test failure).</li>
  </ol>

  <p>Vercel's guide on eval-driven development emphasizes: "By adopting a framework that emphasizes continuous evaluation and refinement of non-deterministic outputs, developers can effectively use variable AI systems without compromising quality, reliability, and observability."<sup>[7]</sup></p>

  <h3>The TDD Parallel (and Difference)</h3>

  <p>Eval-driven development shares TDD's structure (define criteria first, build second, iterate) but fundamentally differs in assertion logic:</p>

  <ul>
    <li><strong>TDD:</strong> <code>assertEqual(expected, actual)</code> — binary, exact match</li>
    <li><strong>EDD:</strong> <code>success_rate(eval_suite) > threshold</code> — probabilistic, threshold-based</li>
  </ul>

  <p>This shift from deterministic to probabilistic success criteria is the paradigm change. You're no longer building software that "works" or "doesn't work." You're building software that "works 78% of the time under these conditions" — and that's acceptable if the 78% is well-characterized.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 4: TDD vs. Eval-Driven Development</p>
    <table class="exhibit-table">
      <tr>
        <th>Dimension</th>
        <th>TDD</th>
        <th>Eval-Driven Development</th>
      </tr>
      <tr>
        <td>Success definition</td>
        <td>Test passes (binary)</td>
        <td>Success rate > threshold (probabilistic)</td>
      </tr>
      <tr>
        <td>Assertion logic</td>
        <td>assertEqual(expected, actual)</td>
        <td>evaluateGoalAchievement(actual, criteria)</td>
      </tr>
      <tr>
        <td>Acceptable variance</td>
        <td>Zero (exact match required)</td>
        <td>Defined by threshold (e.g., 70% success acceptable)</td>
      </tr>
      <tr>
        <td>CI/CD integration</td>
        <td>Pipeline fails if any test fails</td>
        <td>Pipeline fails if success rate below threshold</td>
      </tr>
      <tr>
        <td>Debugging signal</td>
        <td>Which test failed</td>
        <td>Which eval scenarios dropped success rate</td>
      </tr>
      <tr>
        <td>Post-deployment</td>
        <td>Monitoring separate from testing</td>
        <td>Same evals run in production for drift detection</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Anthropic eval guide [6], Vercel EDD guide [7], arxiv:2411.13768 [1]</p>
  </div>

  <h3>The Docker Cagent Counterpoint</h3>

  <p>Docker's Cagent project (announced January 2026) attempts to bring <strong>deterministic testing back to agents</strong> through controlled environments<sup>[8]</sup>. The approach: sandbox the agent's execution environment so external API calls, file operations, and tool use become reproducible.</p>

  <p>This solves reproducibility but not the fundamental non-determinism problem. Even in a sandboxed environment, the LLM's output varies. Cagent acknowledges this: "Teams increasingly rely on thresholds, retries, and soft failures to cope with evaluator variance."<sup>[8]</sup></p>

  <p>Cagent is valuable for integration testing (ensuring tool connections work) but doesn't eliminate the need for probabilistic evaluation of LLM outputs.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If deterministic agent architectures (e.g., fully symbolic reasoning with LLMs only for I/O translation) proved viable at production scale, eval-driven development's probabilistic framing would be unnecessary. Current evidence suggests hybrid (symbolic + neural) is the future, which retains non-determinism.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Adopt eval-driven development if you're building production agents. Build your evaluation suite before your agent. Define success as "achieves goal X within constraints Y with >70% success rate" not "returns exact output Z." This is the testing paradigm agents actually need.</p>
  </div>
</div>

<!-- ========================================
     SECTION 8: PRODUCTION GAP
     ======================================== -->
<div class="page" id="production-gap">
  <h2>8. The Production Gap — Where Testing Ends and Monitoring Begins
    <span class="confidence-badge">70%</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium)</span>

  <p><span class="key-insight">Traditional software has a clean separation: testing happens pre-deployment, monitoring happens post-deployment. For agents, this boundary collapses — the same evaluation suite must run in both environments because behavioral drift is continuous.</span></p>

  <h3>The Behavioral Drift Problem</h3>

  <p>Agents change behavior in production for reasons traditional software doesn't:</p>

  <ul>
    <li><strong>Model updates:</strong> The underlying LLM provider (OpenAI, Anthropic, Google) ships a new model version. Your agent's behavior changes without any code change on your side.</li>
    <li><strong>Context drift:</strong> User queries shift over time. An agent trained/evaluated on support tickets from Q4 2025 behaves differently on Q1 2026 tickets.</li>
    <li><strong>Memory accumulation:</strong> Stateful agents build up memory over sessions. This changes future behavior in ways pre-deployment testing cannot predict.</li>
    <li><strong>Tool API changes:</strong> External APIs the agent calls change their schemas, rate limits, or behavior. The agent's tool use success rate drops even though its code is unchanged.</li>
  </ul>

  <p>The implication: <strong>passing all evals at deployment time does not guarantee the agent will continue passing them in production.</strong> This is fundamentally different from traditional software, where regression tests provide stronger guarantees.</p>

  <h3>Continuous Evaluation in Production</h3>

  <p>The arxiv:2411.13768 paper proposes "EDDOps" (Evaluation-Driven Development and Operations) as the integrated paradigm<sup>[1]</sup>. Key principle: evaluation is not a pre-deployment phase — it's a continuous capability that spans development, staging, and production.</p>

  <p>This requires infrastructure most teams don't have:</p>

  <ul>
    <li><strong>Production eval sampling:</strong> Run evaluation on X% of production traffic (not 100% due to cost).</li>
    <li><strong>Baseline comparison:</strong> Compare current production eval metrics against deployment baselines. Alert when success rate drops below threshold.</li>
    <li><strong>Cohort analysis:</strong> Break down eval results by user segment, time window, or input type to identify drift patterns.</li>
    <li><strong>Human-in-the-loop labeling:</strong> Sample production outputs for human review. Use these labels to fine-tune evaluators or identify new failure modes.</li>
  </ul>

  <p>Langfuse provides tracing infrastructure for this<sup>[11]</sup>. Galileo positions itself as an "AI observability platform" that bridges eval and production monitoring<sup>[10]</sup>. But integration remains manual — no framework provides turnkey "deploy agent + get continuous eval monitoring" today.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 5: Testing vs. Monitoring Boundary Shift</p>
    <table class="exhibit-table">
      <tr>
        <th>Activity</th>
        <th>Traditional Software</th>
        <th>AI Agents</th>
      </tr>
      <tr>
        <td>Pre-deployment testing</td>
        <td>Unit tests, integration tests, E2E tests</td>
        <td>Eval suite on dev/staging data</td>
      </tr>
      <tr>
        <td>Deployment gate</td>
        <td>All tests pass (binary)</td>
        <td>Success rate > threshold (probabilistic)</td>
      </tr>
      <tr>
        <td>Production monitoring</td>
        <td>Error rates, latency, resource use</td>
        <td>Same metrics + eval success rate on production traffic</td>
      </tr>
      <tr>
        <td>Regression detection</td>
        <td>Re-run test suite (deterministic)</td>
        <td>Compare production eval baseline (statistical)</td>
      </tr>
      <tr>
        <td>Response to failure</td>
        <td>Rollback (code is the problem)</td>
        <td>Investigate drift (code, model, or context?)</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: arxiv:2411.13768 [1], Langfuse documentation [11], Galileo platform overview [10]</p>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If agent behavior stabilized over time (no model updates, no context drift, deterministic tool use), the continuous evaluation requirement would diminish. This seems unlikely given current LLM development velocity and real-world deployment patterns.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Build your agent evaluation infrastructure to span pre-deployment and production. Do not treat "testing" and "monitoring" as separate disciplines. Your eval suite is your production health check. Budget for continuous evaluation cost (sampling production traffic is not free). This is table stakes for production agent systems.</p>
  </div>
</div>

<!-- ========================================
     SECTION 9: RECOMMENDATIONS
     ======================================== -->
<div class="page" id="recommendations">
  <h2>9. Recommendations</h2>

  <p><span class="key-insight">Based on the evidence in this report, agent testing requires a paradigm shift from deterministic assertions to probabilistic evaluation boundaries. Here's the minimum viable testing stack for production agents.</span></p>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;"><strong>Scope:</strong> These recommendations apply to autonomous agents with tool use, memory, and non-trivial decision-making. Simpler chatbots or single-turn Q&A systems have lighter requirements.</p>

  <h3>For Engineering Teams Building Agents</h3>

  <ol>
    <li><strong>Abandon strict TDD for agent logic.</strong> Use TDD for infrastructure (API wrappers, database access, authentication) but not for LLM-driven behavior. The deterministic layer and probabilistic layer require different testing approaches.</li>
    <li><strong>Adopt eval-driven development.</strong> Define evaluation scenarios before building the agent. Establish acceptable success rate thresholds (e.g., 75% for non-critical, 90% for high-stakes). Build the eval suite in parallel with the agent, not after.</li>
    <li><strong>Combine LLM-as-judge with deterministic checks.</strong> Use LLM judges for semantic evaluation (is the answer helpful?) but add deterministic validators for structure (did it call the right tool? does the output match the schema?). Layer probabilistic and deterministic evaluation — don't rely solely on one.</li>
    <li><strong>Implement production eval sampling.</strong> Run your evaluation suite on a percentage of production traffic (start with 5-10%). Compare against deployment baselines. Alert when success rate drops below threshold. This is your behavioral drift detector.</li>
    <li><strong>Red-team from day one.</strong> Use promptfoo or custom adversarial prompts to test failure modes: prompt injection, goal hijacking, tool misuse, hallucination. Traditional QA focuses on "does it work?" — agent QA must also ask "can it be broken?"</li>
  </ol>

  <h3>Framework Selection Guide</h3>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 6: Framework Selection by Use Case</p>
    <table class="exhibit-table">
      <tr>
        <th>If you're building...</th>
        <th>Start with...</th>
        <th>Add...</th>
      </tr>
      <tr>
        <td>RAG pipeline (retrieval + generation)</td>
        <td>RAGAS or DeepEval</td>
        <td>Langfuse for tracing</td>
      </tr>
      <tr>
        <td>Customer service chatbot</td>
        <td>DeepEval (answer relevancy, hallucination detection)</td>
        <td>Human-in-the-loop sampling for edge cases</td>
      </tr>
      <tr>
        <td>Multi-step agent (research, analysis, reporting)</td>
        <td>Langfuse (tracing) + custom eval suite</td>
        <td>Tool call validators (deterministic checks)</td>
      </tr>
      <tr>
        <td>Multi-agent orchestration</td>
        <td>Custom eval suite (no off-the-shelf solution)</td>
        <td>Message integrity checks, delegation validators</td>
      </tr>
      <tr>
        <td>Code generation agent</td>
        <td>Traditional test suite (generated code is deterministic)</td>
        <td>LLM-as-judge for code quality/style</td>
      </tr>
      <tr>
        <td>High-stakes decision agent (legal, medical, financial)</td>
        <td>Galileo or Langfuse + mandatory HITL</td>
        <td>Domain expert review panel, not just LLM judges</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Framework documentation [4][5][11], Anthropic eval guide [6], author recommendation</p>
  </div>

  <h3>What Not to Do</h3>

  <ul>
    <li><strong>Don't apply TDD/BDD directly to agent outputs.</strong> You'll build brittle tests that fail on valid responses or pass on invalid ones.</li>
    <li><strong>Don't rely solely on LLM-as-judge.</strong> The judge has the same biases as the agent. Use it as one signal among many.</li>
    <li><strong>Don't skip production evaluation.</strong> Pre-deployment success does not guarantee production success. Behavioral drift is real.</li>
    <li><strong>Don't optimize for 100% eval success rate.</strong> Agents are probabilistic. A 75% success rate on well-defined tasks may be acceptable and cost-effective compared to over-engineering for 95%.</li>
  </ul>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">The testing paradigm for agents is not "TDD with better tools" — it's a fundamentally different approach to defining, measuring, and maintaining quality. Teams that recognize this early will ship more robust agents. Teams that force-fit TDD will waste months debugging flaky tests.</p>
  </div>
</div>

<!-- ========================================
     SECTION 10: PREDICTIONS
     ======================================== -->
<div class="page" id="predictions">
  <h2>10. Predictions
    <span style="font-size: 0.65rem; font-weight: 500; color: #1a1a1a; background: #f5f4f0; padding: 2px 6px; border-radius: 8px; margin-left: 8px; vertical-align: middle;">BETA</span>
  </h2>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">These predictions will be scored publicly at 12 months. This is version 1.0 (February 2026). Scoring methodology available at ainaryventures.com/predictions.</p>

  <div class="exhibit">
    <table class="exhibit-table">
      <tr>
        <th>Prediction</th>
        <th>Timeline</th>
        <th>Confidence</th>
      </tr>
      <tr>
        <td>At least one major testing framework (DeepEval, RAGAS, or new entrant) ships first-class multi-agent eval primitives</td>
        <td>Q3 2026</td>
        <td>65%</td>
      </tr>
      <tr>
        <td>A standardized "eval protocol" emerges for agent testing (similar to how pytest became the Python testing standard)</td>
        <td>Q4 2026</td>
        <td>50%</td>
      </tr>
      <tr>
        <td>"Eval-driven development" becomes a recognized term in practitioner discourse (measured by conference talks, blog posts, job descriptions)</td>
        <td>Q2 2026</td>
        <td>75%</td>
      </tr>
      <tr>
        <td>At least one high-profile agent system failure is publicly attributed to inadequate evaluation infrastructure</td>
        <td>Q3 2026</td>
        <td>70%</td>
      </tr>
    </table>
  </div>
</div>

<!-- ========================================
     SECTION 11: TRANSPARENCY NOTE
     ======================================== -->
<div class="page" id="transparency">
  <h2>11. Transparency Note</h2>

  <p class="transparency-intro">This report was created using a multi-agent research system. Every claim is sourced. Where evidence ends and interpretation begins, it is explicitly marked.</p>

  <table class="transparency-table">
    <tr>
      <td>Overall Confidence</td>
      <td>82% — High confidence in framework landscape analysis and documented limitations. Medium confidence in adoption timeline predictions and practitioner behavior extrapolations.</td>
    </tr>
    <tr>
      <td>Sources</td>
      <td>18 total — 5 peer-reviewed/preprint papers (arxiv), 8 framework documentation reviews (DeepEval, RAGAS, promptfoo, Langfuse, Galileo, Momentic, Cagent), 5 industry practitioner articles (Anthropic, Vercel, Netguru, Coralogix, Reddit discussions)</td>
    </tr>
    <tr>
      <td>Strongest Evidence</td>
      <td>The determinism gap (arxiv:2411.13768 provides comprehensive academic grounding). LLM-as-judge biases are well-documented across multiple independent studies.</td>
    </tr>
    <tr>
      <td>Weakest Point</td>
      <td>Adoption timeline for eval-driven development is speculative. The claim that "teams are abandoning TDD for agents" is directionally supported but not quantified with survey data.</td>
    </tr>
    <tr>
      <td>What Would Invalidate</td>
      <td>If a deterministic agent architecture (fully symbolic reasoning, LLMs only for I/O) proved viable at scale, the entire premise weakens. If an LLM judge achieved >95% alignment with human experts across domains, the "judge trap" section would require revision.</td>
    </tr>
    <tr>
      <td>Methodology</td>
      <td>Research followed a structured multi-agent pipeline: (1) Academic literature review on testing paradigms and agent evaluation, (2) Framework documentation analysis covering 6 major tools, (3) Synthesis of practitioner insights from engineering blogs and discussions, (4) Gap analysis identifying what agents need that traditional testing doesn't provide. Cross-referencing between sources identified contradictions (e.g., Cagent's deterministic approach vs. EDD's probabilistic framing) which are explicitly surfaced in the report.</td>
    </tr>
    <tr>
      <td>System Disclosure</td>
      <td>This report was created with a multi-agent research system combining literature search, synthesis, and technical analysis. Human direction shaped research focus and synthesis decisions. The final synthesis and writing were AI-generated with human review.</td>
    </tr>
  </table>
</div>

<!-- ========================================
     SECTION 12: CLAIM REGISTER
     ======================================== -->
<div class="page" id="claim-register">
  <h2>12. Claim Register</h2>

  <p style="font-size: 0.85rem; color: #666; margin-bottom: 24px;">Top claims from this report with their evidence basis and confidence levels. The top 5 claims include invalidation conditions.</p>

  <div class="exhibit">
    <table class="exhibit-table">
      <tr>
        <th>#</th>
        <th>Claim</th>
        <th>Value/Finding</th>
        <th>Source</th>
        <th>Confidence</th>
        <th>Used In</th>
      </tr>
      <tr>
        <td>1</td>
        <td>TDD and BDD rely on deterministic assertions incompatible with agent outputs</td>
        <td>Binary pass/fail vs. probabilistic success rates</td>
        <td>arxiv:2411.13768 [1]</td>
        <td>High (peer-reviewed)</td>
        <td>Section 4</td>
      </tr>
      <tr>
        <td>2</td>
        <td>LLM-as-judge shows position bias in pairwise evaluation</td>
        <td>First option systematically favored</td>
        <td>MT-Bench [2], Confident AI [2]</td>
        <td>High (replicated)</td>
        <td>Section 5</td>
      </tr>
      <tr>
        <td>3</td>
        <td>Current frameworks lack multi-agent evaluation primitives</td>
        <td>No tool call, delegation, or state testing</td>
        <td>Framework docs review [4][5][11]</td>
        <td>High (direct analysis)</td>
        <td>Section 6</td>
      </tr>
      <tr>
        <td>4</td>
        <td>Eval-driven development is the emerging paradigm</td>
        <td>Define success boundaries, iterate to threshold</td>
        <td>Anthropic [6], Vercel [7]</td>
        <td>Medium (practitioner consensus)</td>
        <td>Section 7</td>
      </tr>
      <tr>
        <td>5</td>
        <td>Behavioral drift requires continuous production evaluation</td>
        <td>Model updates, context shifts, memory changes</td>
        <td>arxiv:2411.13768 [1]</td>
        <td>High (theoretical + practitioner)</td>
        <td>Section 8</td>
      </tr>
      <tr>
        <td>6</td>
        <td>DeepEval supports 14+ metrics but limited multi-agent</td>
        <td>RAG-focused, pytest integration</td>
        <td>DeepEval docs [4]</td>
        <td>High (documentation)</td>
        <td>Section 6</td>
      </tr>
      <tr>
        <td>7</td>
        <td>RAGAS is RAG-specific with non-self-explaining metrics</td>
        <td>Context precision/recall, answer faithfulness</td>
        <td>RAGAS docs [5]</td>
        <td>High (documentation)</td>
        <td>Section 6</td>
      </tr>
      <tr>
        <td>8</td>
        <td>Promptfoo emphasizes red-teaming and prompt testing</td>
        <td>CLI-first, declarative configs, adversarial prompts</td>
        <td>Promptfoo docs [15]</td>
        <td>High (documentation)</td>
        <td>Section 6</td>
      </tr>
      <tr>
        <td>9</td>
        <td>LLM judges show verbosity bias (longer = higher score)</td>
        <td>Length correlates with score independent of quality</td>
        <td>EvidentlyAI [12]</td>
        <td>Medium (single source)</td>
        <td>Section 5</td>
      </tr>
      <tr>
        <td>10</td>
        <td>Docker Cagent brings determinism to agent testing</td>
        <td>Sandboxed environments for reproducible tool use</td>
        <td>InfoQ Cagent announcement [8]</td>
        <td>Medium (early-stage tool)</td>
        <td>Section 7</td>
      </tr>
      <tr>
        <td>11</td>
        <td>Langfuse provides tracing for eval + production monitoring</td>
        <td>LLM-as-judge execution traces, delayed evals</td>
        <td>Langfuse docs [11]</td>
        <td>High (documentation)</td>
        <td>Section 6, 8</td>
      </tr>
      <tr>
        <td>12</td>
        <td>No framework ships deterministic testing as default</td>
        <td>Evaluation is post-deployment add-on</td>
        <td>Framework comparison [4][5][11]</td>
        <td>High (documentation review)</td>
        <td>Executive Summary</td>
      </tr>
    </table>
  </div>

  <p style="margin-top: 24px; font-size: 0.85rem; color: #666;"><strong>Top 5 Invalidation Conditions:</strong></p>

  <ol style="font-size: 0.85rem; color: #666; margin-top: 12px;">
    <li><strong>Claim 1:</strong> If a testing framework with probabilistic assertions (semantic equivalence across multiple valid outputs) gains widespread TDD-like adoption, the "incompatible" framing weakens.</li>
    <li><strong>Claim 2:</strong> If position bias in LLM judges is fully mitigated (e.g., through ensemble voting or fine-tuned evaluators with >95% human alignment), the "trap" characterization would be overstated.</li>
    <li><strong>Claim 3:</strong> If a major framework ships comprehensive multi-agent primitives (tool validation, delegation logic, state testing) within 6 months, the "gap" is closing faster than predicted.</li>
    <li><strong>Claim 4:</strong> If deterministic agent architectures (fully symbolic, LLMs only for translation) prove viable at scale, eval-driven development's probabilistic framing becomes unnecessary.</li>
    <li><strong>Claim 5:</strong> If agent behavior stabilizes (no model drift, static contexts, deterministic tools), continuous production evaluation becomes less critical.</li>
  </ol>
</div>

<!-- ========================================
     SECTION 13: REFERENCES
     ======================================== -->
<div class="page" id="references">
  <h2>13. References</h2>

  <div class="reference-entry">[1] arxiv:2411.13768v3 (2025). "Evaluation-Driven Development and Operations of LLM Agents: A Process Model and Reference Architecture." https://arxiv.org/html/2411.13768v3</div>

  <div class="reference-entry">[2] Confident AI (2026). "Why LLM-as-a-Judge is the Best LLM Evaluation Method." https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method</div>

  <div class="reference-entry">[3] ACM IUI (2026). "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks." Proceedings of the 30th International Conference on Intelligent User Interfaces. https://dl.acm.org/doi/10.1145/3708359.3712091</div>

  <div class="reference-entry">[4] DeepEval by Confident AI (2026). "The LLM Evaluation Framework." Documentation and blog. https://deepeval.com/</div>

  <div class="reference-entry">[5] RAGAS Documentation (2026). "RAG Assessment — Metrics for Retrieval-Augmented Generation." https://docs.ragas.io/</div>

  <div class="reference-entry">[6] Anthropic (2026). "Demystifying evals for AI agents." Engineering Blog. https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents</div>

  <div class="reference-entry">[7] Vercel (2025). "Eval-driven development: Build better AI faster." https://vercel.com/blog/eval-driven-development-build-better-ai-faster</div>

  <div class="reference-entry">[8] InfoQ (2026). "Docker's Cagent Brings Deterministic Testing to AI Agents." News coverage. https://www.infoq.com/news/2026/01/cagent-testing/</div>

  <div class="reference-entry">[9] Netguru (2025). "Testing AI Agents: Why Unit Tests Aren't Enough." https://www.netguru.com/blog/testing-ai-agents</div>

  <div class="reference-entry">[10] Galileo AI (2025). "Leveraging Test-Driven Development (TDD) for AI System Architecture." https://galileo.ai/blog/tdd-ai-system-architecture</div>

  <div class="reference-entry">[11] Langfuse Documentation (2026). "LLM-as-a-Judge Evaluation." https://langfuse.com/docs/evaluation/evaluation-methods/llm-as-a-judge</div>

  <div class="reference-entry">[12] EvidentlyAI (2025). "LLM-as-a-judge: a complete guide to using LLMs for evaluations." https://www.evidentlyai.com/llm-guide/llm-as-a-judge</div>

  <div class="reference-entry">[13] Deepchecks (2025). "What Is LLM As A Judge? Strategies, Impact & Best Practices." https://www.deepchecks.com/what-is-llm-as-a-judge-strategies-impact-and-best-practices/</div>

  <div class="reference-entry">[14] Reddit r/LLMDevs (2025). "LLM-as-a-judge is not enough. That's the quiet truth nobody wants to admit." Discussion thread. https://www.reddit.com/r/LLMDevs/comments/1kealia/</div>

  <div class="reference-entry">[15] Promptfoo GitHub (2026). "Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs." https://github.com/promptfoo/promptfoo</div>

  <div class="reference-entry">[16] Momentic (2026). "AI-Powered Testing Tool for Web & Mobile." https://momentic.ai/</div>

  <div class="reference-entry">[17] Ainary Research (2026). "Agent Testing Is Broken — Why Software QA Doesn't Work for Non-Deterministic Systems." AR-023.</div>

  <div class="author-section">
    <div class="author-initials">FZ</div>
    <div class="author-name">About the Author</div>
    <div class="author-bio">Florian Ziesche is the founder of Ainary Ventures, where AI does 80% of the research and humans do the 20% that matters. Before Ainary, he was CEO of 36ZERO Vision and advised startups and SMEs on AI strategy and due diligence. His conviction: HUMAN × AI = LEVERAGE. This report is the proof.</div>
    <a href="https://ainaryventures.com" class="author-link">ainaryventures.com</a>
  </div>
</div>

<!-- ========================================
     BACK COVER
     ======================================== -->
<div class="back-cover">
  <div class="back-cover-logo">
    <span class="gold-punkt" style="font-size: 18px;">●</span>
    <span style="font-size: 1rem; font-weight: 500; color: #1a1a1a; margin-left: 8px;">Ainary</span>
  </div>

  <div class="back-cover-services">
    AI Strategy · Published Research · Daily Intelligence
  </div>

  <div class="back-cover-cta">
    <a href="mailto:florian@ainaryventures.com">Contact</a> · 
    <a href="mailto:florian@ainaryventures.com?subject=Feedback: AR-023">Feedback</a>
  </div>

  <div class="back-cover-contact">
    ainaryventures.com<br>
    florian@ainaryventures.com
  </div>

  <div class="back-cover-copyright">
    © 2026 Ainary Ventures
  </div>
</div>

</body>
</html>
