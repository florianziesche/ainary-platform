# 10 LinkedIn Posts — Ready to Copy-Paste

**Created:** 2026-02-06
**Author voice:** Florian Ziesche — Founder/CEO turned VC, AI builder, European in NYC
**Format:** 800-1300 chars each | Hook → Story/Insight → Question | No links in body

---

## Post 1: Network Analysis (4,779 Connections)

```
I analyzed 4,779 LinkedIn connections with algorithms.

The results were brutal.

I built a classifier to cluster my entire network by industry, seniority, and engagement overlap.

Here's what the data showed:

→ 68% of my connections have never interacted with my content
→ The top 3% drive 80% of all meaningful engagement
→ Industry clusters are shockingly isolated — my AI contacts and my VC contacts almost never overlap
→ 41% of connections are from a single city I no longer live in

The insight that hit hardest:

Your network isn't your net worth.
Your **active** network is.

Most people optimize for connection count.
The real metric is connection density — how many of your contacts actually know each other and amplify your work.

I had 4,779 connections and maybe 140 that mattered.

Since this analysis, I've changed my strategy:

Quality engagement > mass connecting.
5 real conversations > 50 connection requests.

The algorithm doesn't care about your follower count.
It cares about who stops scrolling when they see your name.

When's the last time you audited who's actually in your network?
```

**Chars:** ~1,012

---

## Post 2: RAG System <0.2% Hallucination

```
I built a RAG system with a 0.18% hallucination rate.

Most AI companies can't get below 5%. Here's what they get wrong.

The mistake isn't the model. It's the architecture.

Everyone throws documents into a vector database and calls it "AI-powered."

That's like giving a surgeon a scalpel and no training.

What actually works:

1. **Multi-agent verification**
   Four agents — Questioner, Researcher, Validator, Reporter — each catching what others miss.

2. **Hybrid retrieval**
   BM25 for exact matches + dense vectors for semantic meaning. One alone isn't enough.

3. **Claim-level citations**
   Every single statement gets linked to its source. No source? No output.

4. **Confidence scoring**
   High, Medium, Low — shown explicitly. Users know when to double-check.

The result: Lawyer adoption jumped from 12% to 78%.

Not because the AI got "smarter."
Because we engineered trust into the system.

AI hallucination isn't a model problem.
It's an engineering problem.

And most teams aren't solving it because they're too busy shipping demos instead of production systems.

What's the biggest trust gap you've seen in AI products?
```

**Chars:** ~1,050

---

## Post 3: Founder → VC Transition

```
I raised €5.5M as a startup CEO.

Now I'm on the other side of the table. Here's what changed.

As a founder, I thought VCs had it easy.
See pitches. Write checks. Collect returns.

I was wrong about all three.

**What I didn't understand as a founder:**

→ VCs see 1,000+ decks per year. You get 30 minutes. Maybe.
→ Pattern matching is real — and dangerously flawed. Most investors bet on what they've seen work before.
→ The best investors aren't smarter. They have better deal flow.

**What changed in how I evaluate startups:**

→ I now watch the founder's eyes when I ask about churn. The flinch tells me more than the deck.
→ I care less about TAM slides and more about founder-market obsession.
→ I notice which founders follow up and which disappear. Persistence is signal.

**The hardest part:**

Going from "I'll outwork the problem" to "I need to pick the right person to outwork the problem."

As a founder, you control the outcome.
As an investor, you bet on someone else's execution.

That shift broke my brain for about three months.

What surprised you most when switching from building to investing?
```

**Chars:** ~1,070

---

## Post 4: 85% Never Raise Series A

```
85% of seed-stage companies never raise a Series A.

The pattern is always the same.

I've reviewed hundreds of startups. The ones that die between Seed and Series A share the same DNA:

**1. They confuse traction with progress**
Revenue going up ≠ product-market fit. If your CAC is 3x your LTV, you're buying growth, not earning it.

**2. The founder stops selling**
At seed, the CEO is the sales team. The ones who hire a sales lead at $500K ARR and step back? They stall.

**3. They solve a vitamin problem, not a painkiller**
"Nice to have" gets funded at seed because the story is compelling. But Series A investors want proof people can't live without it.

**4. They build for investors, not users**
Every feature designed to hit the next milestone instead of solving the next user problem.

**5. They ignore unit economics until it's too late**
"We'll figure out margins at scale." No. You won't. Scale amplifies problems.

The companies that make it?

They're obsessed with one metric.
They have a founder who still talks to customers daily.
And they'd rather be profitable than impressive.

Seed to Series A is where stories meet reality.

Which of these patterns have you seen kill the most startups?
```

**Chars:** ~1,130

---

## Post 5: Median VC Fund Underperforms S&P 500

```
The median VC fund underperforms the S&P 500.

Here's the data nobody wants to talk about.

Cambridge Associates tracks VC fund returns going back decades. The numbers are uncomfortable:

→ Median VC fund: ~1.6x net TVPI over 10 years
→ S&P 500 index fund over same period: comparable or better returns — with zero management fees
→ Top quartile funds return 3x+. Bottom quartile? You'd have been better off in a savings account.

The dirty secret:

VC as an asset class works because of **power law** returns. A tiny number of funds generate almost all the value.

The top 5% of funds return 10-25x.
The other 95% are average or worse.

Yet every GP pitches their fund as if they'll be in the top 5%.

Why does this matter?

If you're a founder: understand that your VC is under pressure too. They need you to be a 100x outcome — not a solid 3x business.

If you're an LP: index-level returns with a 10-year lockup and 2/20 fees is a bad deal.

If you're building a fund: the only strategy is differentiation. Access, thesis, or value-add. Pick one and be the best.

What do you think separates the top 5% of VC funds from the rest?
```

**Chars:** ~1,120

---

## Post 6: Munich to NYC — Cultural Differences

```
I moved from Munich to NYC to build in AI.

The cultural difference will surprise you.

In Germany, I spent 3 months writing a business plan before talking to a single customer.

In NYC, I watched a founder raise $2M with a napkin sketch and pure conviction.

Here's what actually differs:

**Speed of decision-making**
Germany: 6 meetings to decide. NYC: 1 coffee.

**Failure tolerance**
Germany: Failure = shame. NYC: Failure = credential.

**Fundraising culture**
Germany: "Show me 18 months of revenue." NYC: "Show me why this could be 100x."

**Network density**
In Munich, I knew every AI founder personally. In NYC, there's someone better at everything — and they're at the coffee shop next to you.

**The thing nobody talks about:**

German engineering + American ambition is an insane combo.

The rigor I learned building systems in Munich makes my AI products better. The urgency I absorbed in NYC makes me ship faster.

European founders in the US have an underrated edge: we build things that actually work, not just things that demo well.

But we also need to learn: perfection kills startups. Done beats perfect. Every time.

What's the biggest cultural difference you've noticed between US and European startup scenes?
```

**Chars:** ~1,150

---

## Post 7: AI BS Detection

```
Every AI startup claims "we use AI."

Here's how to tell who actually does.

After 5 years building AI products — from computer vision to legal RAG systems — I've developed a simple BS filter.

**Ask these 5 questions:**

1. **"What happens when the AI is wrong?"**
   Real answer: "Here's our fallback system and confidence thresholds."
   BS answer: "Our AI is very accurate."

2. **"What's your training data?"**
   Real answer: Specific dataset, size, freshness, bias mitigation.
   BS answer: "We use the latest models."

3. **"Can I see it fail?"**
   Real answer: "Sure, here are the edge cases we're still working on."
   BS answer: Nervous laughter.

4. **"What did you build vs. what's an API wrapper?"**
   Real answer: "We built X, Y is third-party, here's why."
   BS answer: "Our proprietary technology..."

5. **"What's your latency at scale?"**
   Real answer: Specific P50/P99 numbers.
   BS answer: "It's fast."

The pattern: Real AI teams talk about **limitations.**
Fake AI teams only talk about capabilities.

If a company can't tell you where their AI breaks, they either don't know or don't want you to know.

Both are red flags.

What's the best BS-detection question you've used on an AI startup?
```

**Chars:** ~1,150

---

## Post 8: CNC Planer — 50x Budget Underdog Story

```
My manufacturing software competed against tools with 50x our budget.

We won. Here's how.

In 2019, I built CNC Planer — software that turns CAD files into machine code for CNC manufacturing.

Our competitors: established industrial software companies. Millions in funding. Sales teams of 50+. Enterprise contracts locked in for years.

Our team: Me and two engineers.
Our budget: Basically zero.

**What we did differently:**

1. **We solved the actual problem**
   Big tools do everything. We did one thing: take a CAD-PDF, generate Heidenhain machine code, and calculate production time. That's it. Perfectly.

2. **We sat on the shop floor**
   I spent weeks next to CNC operators. Watching. Asking dumb questions. Our UX was designed for calloused hands, not corner offices.

3. **We shipped in days, not quarters**
   Enterprise competitors needed 6-month implementation cycles. We were running in an afternoon.

4. **We priced for reality**
   Small manufacturers don't have $50K/year software budgets. We made it accessible.

The result: Operators preferred us because we actually solved their daily pain.

Big budgets build features.
Small budgets build solutions.

The underdog advantage is focus. When you can only do one thing, you make it exceptional.

What's a time you beat a competitor with 10x more resources?
```

**Chars:** ~1,220

---

## Post 9: Pitch Deck Genome — 200 Real Decks Scored

```
I scored 200 real pitch decks using 10 criteria.

The #1 predictor of success isn't what you think.

Not TAM. Not team slide. Not financial projections.

It's **narrative coherence.**

The decks that raised money told one story from slide 1 to slide 15. Problem → why now → why us → how we win. Every slide built on the last.

The decks that failed? They read like a checklist. Problem slide. Solution slide. Market slide. No connective tissue.

Here's what else the data showed:

→ Decks under 15 slides raised 2.3x more than decks over 20
→ Founders who led with a customer story outperformed those who led with market size
→ The "Why Now" slide was missing in 60% of failed decks
→ Teams that showed specific metrics (even small ones) beat teams that showed projections

**The 3 slides that matter most:**
1. Problem (is it real and painful?)
2. Why Now (what changed?)
3. Traction (proof, not promises)

Financial projections? Almost no investor believes your Year 3 revenue forecast. They're checking if you understand your own business model.

The best pitch deck isn't the prettiest.
It's the one that makes the investor feel like they'd be stupid to pass.

What's the worst pitch deck mistake you've seen?
```

**Chars:** ~1,180

---

## Post 10: AI Won't Replace VCs

```
AI won't replace VCs.

But VCs who use AI will replace those who don't.

I've been on both sides — building AI products and evaluating startups. Here's what I see coming:

**What AI already does better than most VCs:**

→ Screen 1,000 decks and surface the top 50 in hours, not weeks
→ Track founder signals across LinkedIn, GitHub, Twitter, patent filings — automatically
→ Benchmark a startup's metrics against 10,000 comparable companies instantly
→ Flag portfolio risks before the board meeting

**What AI can't do (yet):**

→ Read the room in a founder meeting
→ Sense when a CEO is hiding bad news behind good metrics
→ Build trust over a 10-year fund relationship
→ Make the gut call on a contrarian bet everyone else passes on

The future isn't AI vs. human investors.
It's AI-augmented investors vs. spreadsheet-and-gut investors.

The funds that win the next decade will:
1. Use AI for deal sourcing and screening
2. Use AI for portfolio monitoring
3. Use humans for relationship and judgment

That's the thesis behind what I'm building.

The VCs still doing manual deal screening in 2026 are like the traders who ignored algorithms in 2010.

How much of your investment process could be automated today?
```

**Chars:** ~1,140

---

## Publishing Notes

**Schedule:** 2x/week (Tue & Thu, 08:30-10:00 EST)

**Suggested order (thematic variety):**
1. Post 1 — Network Analysis (data/personal)
2. Post 7 — AI BS Detection (technical/practical)
3. Post 3 — Founder→VC (career/personal)
4. Post 5 — VC Fund Returns (contrarian/data)
5. Post 6 — Munich→NYC (personal/cultural)
6. Post 2 — RAG Hallucination (technical/credibility)
7. Post 4 — 85% Series A (insight/founder)
8. Post 8 — CNC Planer Underdog (story/entrepreneurship)
9. Post 9 — Pitch Deck Genome (data/tactical)
10. Post 10 — AI+VC Thesis (vision/forward-looking)

**Pre-post checklist:**
- [ ] Hook grabs in first 2 lines
- [ ] 800-1300 characters ✓
- [ ] No links in body ✓
- [ ] Ends with question ✓
- [ ] Put any relevant links in first comment
- [ ] Be online first hour to reply to comments

---

*Ready to post. Just copy the text between the ``` blocks.*
