# Sequoia sagt, AGI ist da. Sie haben recht ‚Äî und auch nicht.

*Die sch√§rfsten VCs der Welt haben den Meilenstein richtig erkannt ‚Äî aber die falsche Ziellinie benannt.*

---

Sequoia Capital hat gerade "2026: This is AGI" ver√∂ffentlicht. Nicht "wir kommen n√§her". Nicht "fast da". **Das ist AGI.**

Von einem der f√ºhrenden Tech-Investoren der Welt ist das keine Hype-Behauptung ‚Äî das ist eine These. Und die Daten sind tats√§chlich beeindruckend.

METR-Benchmarks zeigen, dass sich AI-Task-Horizonte alle ~7 Monate verdoppeln ‚Äî und beschleunigen. Claude Opus 4.5 l√∂st inzwischen etwa 50% der Aufgaben, f√ºr die menschliche Experten f√ºnf Stunden brauchen. Diese Systeme verketten Reasoning, nutzen Tools, debuggen ihre eigenen Fehler und arbeiten mehrst√ºndige Aufgaben ab. Sequoias Definition: AGI ist "die F√§higkeit, Dinge herauszufinden". Drei Zutaten ‚Äî Pre-Training (Wissen), Inference-Time Compute (Reasoning), Long-Horizon Agents (Iteration).

Ich will dieses Argument ernst nehmen. Wenn Sequoia so eine steile These aufstellt, verdient das fundierte Auseinandersetzung, keinen Quick Take.

Aber als jemand, der t√§glich AI in Fabriken und Unternehmen ausrollt, sehe ich etwas, das ihr Framework √ºbersieht. Nicht weil sie falsch liegen, was AI heute kann ‚Äî sondern weil ihre Definition verschleiert, was sie immer noch nicht kann.

**Die L√ºcke ist nicht Streaming-Input. Es sind Weltmodelle.**

---

## Was sie richtig sehen

Seien wir ehrlich: Die Agenten, die gerade ankommen, sind qualitativ anders als die Chatbots von 2023.

Sarah Guos Framing trifft es: "Bald kannst du einen Agenten einstellen." Nicht prompten. *Einstellen*. Gib ihm ein Problem, geh weg, komm zu Ergebnissen zur√ºck. Sequoias Recruiting-Beispiel ‚Äî wo ein Agent autonom von LinkedIn zu YouTube-Talks zu Twitter-Engagement zu personalisierter Outreach-Mail pivotiert, in 31 Minuten ‚Äî das ist real. Ich habe meinen eigenen AI-Agenten Research-Briefs produzieren, technische Dokumente draften und mehrstufige Analysen durchf√ºhren sehen, f√ºr die ich Tage gebraucht h√§tte.

Die METR-Daten sind schwer zu bestreiten: sechs Jahre konsistentes exponentielles Wachstum bei Task-Horizonten. Neuere Modelle revidieren nach *oben*. Die Verdopplungszeit verk√ºrzt sich von 7 auf 3 Monate. Wenn man dem Trend glaubt, sind ganzt√§gige Tasks bis 2028 und jahrelange Tasks bis 2034 keine unrealistischen Extrapolationen.

Also ja ‚Äî hier ist tats√§chlich etwas grundlegend Neues. Sequoia liegt nicht falsch beim Meilenstein.

Sie liegen falsch bei der Bedeutung.

---

## Die Weltmodell-L√ºcke

Das macht aktuelle AI wirklich, egal wie ausgekl√ºgelt das Agent-Scaffolding ist:

1. Empf√§ngt ein Context Window (einen Snapshot der Welt)
2. Verarbeitet diesen Snapshot
3. Liefert Output
4. Wartet auf den n√§chsten Snapshot

Du kannst diese Schleife schneller machen. Du kannst das Context Window vergr√∂√üern (Gemini macht 1M+ Tokens). Du kannst Tool Use hinzuf√ºgen, sodass das Modell zwischen Steps Datenbanken und APIs abfragt. OpenAIs Realtime API verarbeitet sogar Streaming-Audio mit <200ms Latenz.

Aber nichts davon l√∂st das fundamentale Problem: **Aktuelle AI h√§lt kein pr√§diktives Modell der Welt aufrecht, das kontinuierlich updated.**

Ein Mensch, der an einer komplexen Aufgabe arbeitet, verarbeitet nicht nur Information ‚Äî er *antizipiert*. Du siehst eine Tasse Richtung Tischkante rutschen und greifst danach, *bevor* sie f√§llt. Du sp√ºrst eine Verhandlung kippen und passt deine Strategie an, *w√§hrend* es passiert, nicht nachdem dir jemand das Meeting zusammengefasst hat. Du baust kausale Modelle aus kontinuierlicher Beobachtung: "die Linie l√§uft heute hei√ü", "dieser Kunde wirkt abgelenkt", "der Markt-Ton hat sich diese Woche ver√§ndert".

Aktuelle AI ‚Äî selbst Sequoias "generally intelligent agents" ‚Äî reagiert auf das, was passiert ist. Sie antizipiert nicht, was gleich passieren wird.

Wie Angjoo Kanazawa von UC Berkeley es formuliert: "Wie entwickelt man ein intelligentes System, das tats√§chlich Streaming-Input haben und sein Verst√§ndnis der Welt updaten kann? Das ist ein gro√ües offenes Problem. Ich denke, AGI ist ohne L√∂sung dieses Problems nicht m√∂glich."

---

## Wo das in Production auftaucht

Die L√ºcke ist unsichtbar in Benchmarks. Sie ist offensichtlich im Deployment.

**In einer Fabrik:** Wir haben Computer Vision zu Automotive-OEMs ausgerollt ‚Äî BMW, Siemens, Bosch. Die Modelle waren exzellent bei statischen Bildern. Aber Fertigung ist kontinuierlich: Licht ver√§ndert sich, Material-Batches variieren, Operatoren justieren Einstellungen. Ein Snapshot-basiertes System verarbeitet Frame 1, liefert ein Urteil, verarbeitet Frame 2. Jedes Urteil ist isoliert. Es baut nie ein Modell von "diese Schicht l√§uft anders als gestern". Die Operatoren wissen das. Die AI nicht.

**In einer Redaktion:** AI kann Artikel aus Briefings draften. Aber ein Reporter an einer Breaking Story integriert kontinuierlich neue Signale ‚Äî eine Quelle textet zur√ºck, ein anderes Outlet ver√∂ffentlicht einen Winkel, ein Livestream offenbart etwas Unerwartetes. Das mentale Modell des Reporters updated in Echtzeit. Aktuelle AI erfordert Re-Prompting: "Hier ist das Update, regeneriere." Es ist ein Writing-Tool, kein Kollege, der die Story mit dir trackt.

**In Rechtsverhandlungen:** AI liest Vertr√§ge und flaggt Risiken brillant ‚Äî bei statischen Dokumenten. Aber w√§hrend eines Live-Deals verschieben sich Konditionen √ºber E-Mails, Calls und Nebengespr√§che. Ein menschlicher Verhandler h√§lt ein lebendes Modell von "wo wir gerade stehen" aufrecht. Ein AI-Agent verarbeitet jedes Update als separaten Batch.

Factory.ai hat die tiefere technische Einschr√§nkung identifiziert: Selbst Million-Token-Context-Windows leiden unter "Context Rot" ‚Äî Modelle nutzen ihren Kontext nicht gleichm√§√üig, Performance degradiert mit wachsendem Input. Gr√∂√üere Windows sind nicht die L√∂sung. Bessere Weltmodelle sind es.

---

## Das ehrliche Gegenargument

Ich will Sequoias Position stahlm√§nnern, weil die st√§rkste Version ihres Arguments Biss hat:

**"Ist kontinuierliches Weltmodellieren f√ºr Wissensarbeit √ºberhaupt wichtig?"**

Vielleicht nicht ‚Äî f√ºr die meisten Tasks. Code schreiben, Daten analysieren, Research betreiben, Dokumente draften ‚Äî das sind deliberative Tasks, wo Batch-Processing tats√§chlich ausreicht. Du brauchst keine Echtzeit-Wahrnehmung, um eine Funktion zu debuggen oder ein Legal Brief zu schreiben.

Selbst menschliche Kognition ist "gechunkter", als wir zugeben wollen. Wir erleben attentional blinks, change blindness, saccadic suppression. Unsere Wahrnehmung ist auch nicht wirklich kontinuierlich ‚Äî sie ist nur schneller und integrierter als aktuelle AI-Schleifen.

Und das Engineering konvergiert auf etwas, das Streaming *approximiert*: Fast Inference + Tool Use + Long Context + MCP erzeugen Beobachtungsschleifen, die im Sekunden-Frequenzbereich laufen. F√ºr viele Anwendungen ist das gut genug.

Deshalb funktioniert Sequoias Definition *f√ºr Wissensarbeit-AGI*. Wenn du das Problem definierst als "√∂konomisch wertvolle kognitive Tasks automatisieren", dann ja ‚Äî Snapshot-basierte Agenten mit engen Iterations-Schleifen werden den Gro√üteil des Values capturen.

**Aber Sequoia hat nicht "Wissensarbeit-AGI" gesagt. Sie haben "AGI" gesagt.**

Und generelle Intelligenz erfordert etwas, das diese Systeme fundamental nicht haben: ein persistentes, pr√§diktives Modell der Welt, das in Echtzeit updated. Die Art von Verst√§ndnis, die dich einen Raum betreten und sofort sp√ºren l√§sst, dass etwas nicht stimmt. Die Art, die einen Meisterhandwerker f√ºhlen l√§sst, wann eine Maschine kurz vor dem Ausfall steht. Die Art, die einen gro√üartigen Investor die Mikro-Expressionen eines Founders lesen und seine These mid-conversation anpassen l√§sst.

---

## Was das bedeutet

**F√ºr Builder:** Die gr√∂√üte Opportunity ist, die Weltmodell-L√ºcke zu schlie√üen. Nicht nur schnellere Schleifen ‚Äî echtes kontinuierliches Verst√§ndnis. Persistenter State, pr√§diktive Modelle, antizipatives Verhalten. Die Interfaces der Zukunft werden nicht Prompt-and-Response sein; es wird AI sein, die *pr√§sent* ist ‚Äî Awareness aufrechterh√§lt und Insights surfaced, ohne gefragt zu werden.

**F√ºr Investoren:** Sequoias "hire an agent"-Framing wird in den n√§chsten 24 Monaten massive Value Creation in Wissensarbeit treiben. Das ist die offensichtliche Wette. Die asymmetrische Wette liegt in persistenten Weltmodellen ‚Äî die Companies, die AI bauen, die die Welt nicht nur verarbeitet, sondern ihren Zustand in Echtzeit versteht. Embodied AI, Ambient Computing, kontinuierliche Monitoring-Systeme. Da liegt der n√§chste Platform-Shift.

**F√ºr Unternehmen:** Deployt aktuelle Agenten aggressiv f√ºr klar definierte kognitive Tasks. Sie sind ready. Aber d√§mpft Erwartungen f√ºr Rollen, die kontinuierliches Situationsbewusstsein erfordern. Eure AI kann die Quartalszahlen analysieren. Sie kann noch nicht durch die Fabrikhalle laufen und sp√ºren, was heute anders ist.

---

## Das Gold in den Rissen

Sequoias Essay wird retrospektiv richtungsweisend richtig sein. Der Meilenstein, den sie benennen, ist real. Die Agenten, die jetzt ankommen, werden Industrien reshapen und enormen Value kreieren.

Aber das AGI zu nennen ‚Äî ohne Qualifikation ‚Äî √ºberklebt die wichtigste Frontier in AI. Wir haben brillante Batch-Prozessoren gebaut. Wir haben noch keine Systeme gebaut, die kontinuierliches Verst√§ndnis der Welt aufrechterhalten.

Die Unterscheidung ist wichtig, weil **der n√§chste Durchbruch nicht gr√∂√üere Context Windows oder tieferes Reasoning bei statischen Problemen ist. Es sind persistente Weltmodelle ‚Äî AI, die nicht nur Snapshots der Realit√§t verarbeitet, sondern Zeit wirklich bewohnt.**

Wenn das abstrakt klingt, hier ein konkreter Test: Kann eine AI ein 10-Minuten-Video schauen und vorhersagen, was als n√§chstes passiert ‚Äî nicht aus Pattern-Matching, sondern aus Verst√§ndnis der Physik, Intentionen und Dynamiken im Spiel? Kann sie Object Permanence aufrechterhalten, wenn Dinge off-screen gehen? Kann sie einen Stimmungswechsel aus einem sich entfaltenden Gespr√§ch detektieren?

Heute lautet die Antwort nein. Wenn die Antwort ja ist, haben wir etwas, das es wert ist, AGI genannt zu werden.

Bis dahin haben wir etwas Bemerkenswertes, Transformatives und grundlegend Neues. Nur nicht ganz das, was Sequoia es genannt hat.

Das Gold liegt in den Rissen zwischen dem, was wir gebaut haben, und dem, was wir als n√§chstes bauen.

---

*Florian Ziesche ist ehemaliger Startup-CEO (‚Ç¨5,5M+ raised, AI shipped zu BMW, Siemens und Bosch) und arbeitet jetzt als AI-Berater und VC Lab Fellow. Er schreibt √ºber angewandte AI, Venture Capital und die Zukunft menschlich-AI-Kollaboration auf [ainaryventures.com](https://ainaryventures.com).*

---

**üê¶ Twitter (280 Zeichen):**
Sequoia hat AGI f√ºr angekommen erkl√§rt. Die Daten sind beeindruckend. Aber sie haben die falsche Ziellinie benannt. Aktuelle AI verarbeitet Snapshots brillant. Sie h√§lt keine Weltmodelle aufrecht. Der Unterschied ist nicht akademisch ‚Äî da liegt der n√§chste Platform-Shift.

**üìä LinkedIn Hook:**
Sequoia Capital hat gerade gesagt "Das ist AGI". Als jemand, der AI zu BMW und Bosch shipped ‚Äî sie haben den Meilenstein richtig erkannt, aber die falsche Ziellinie benannt. Was Builder sehen, das VCs √ºbersehen. [link]

---

### √úbersetzungsnotizen

**Angepasst f√ºr deutsche Leser:**
- Sequoia Capital als "einer der f√ºhrenden Tech-Investoren der Welt" kontextualisiert (in Deutschland weniger Haushaltsname als im Valley)
- Technische Begriffe teilweise englisch gelassen, wo im deutschen Tech-Diskurs √ºblich (Reasoning, Inference, Context Window, Batch Processing, Streaming)
- "Stahlm√§nnen" statt "Steelman" (eingedeutschte Form, im rationalistischen Diskurs etabliert)
- Kulturelle Referenzen beibehalten (Kintsugi ist universal, Factory Floor ist in Deutschland mit Automotive-Tradition sogar relevanter)

**Voice beibehalten:**
- Direktheit: "Seien wir ehrlich", "Das ist real", "Die Antwort ist nein"
- Technische Pr√§zision ohne Jargon-Overload
- Operator-Perspektive: Konkrete Beispiele aus Production (BMW, Siemens, Bosch)
- Nuanciert statt polemisch: "Sie haben recht ‚Äî und auch nicht"
