\documentclass[11pt,a4paper]{article}
\usepackage{fontspec}
\setmainfont{Georgia}
\setmonofont{Menlo}[Scale=0.85]
\usepackage{polyglossia}
\setdefaultlanguage{english}
\usepackage[margin=2.5cm]{geometry}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{enumitem}
\usepackage{fancyvrb}
\usepackage{titlesec}

\definecolor{accent}{HTML}{C8AA50}
\definecolor{darktext}{HTML}{1F2937}
\definecolor{lightgray}{HTML}{6B7280}
\definecolor{linkblue}{HTML}{2563EB}

\hypersetup{colorlinks=true,linkcolor=linkblue,urlcolor=linkblue}

\color{darktext}

\titleformat{\section}{\large\bfseries\color{darktext}}{}{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\color{darktext}}{}{0em}{}
\titlespacing{\section}{0pt}{18pt}{6pt}

\setlength{\parskip}{8pt}
\setlength{\parindent}{0pt}

\pagestyle{empty}

\begin{document}

{\color{lightgray}\small FINITE MATTERS. $\vert$ DRAFT — Review Copy}

\vspace{12pt}

{\LARGE\bfseries I Built an AI Board of Advisors.\\[2pt]They Argue Like the Real Thing.}

\vspace{4pt}
{\color{lightgray}\small Florian Ziesche $\vert$ Finite Matters on Substack $\vert$ Series: Article 2}

\vspace{4pt}
{\color{lightgray}\rule{\textwidth}{0.4pt}}

\vspace{8pt}

Last week, I asked 100 AI agents to design their own evolution. The finding: \textbf{cognitive diversity beats individual intelligence.} When agents with different reasoning styles tackled the same problem, they outperformed any single ``smarter'' model.

So I tested it.

Last Tuesday, I put Marc Andreessen, Charlie Munger, and Peter Thiel in a room to argue about whether I should raise a venture fund or bootstrap. They weren't physically there. Two don't know I exist. One is dead. But for 45 minutes, they tore into fund economics, incentive structures, and contrarian insight with the kind of brutal honesty real advisors rarely give you.

Munger called the 2/20 fee structure ``a triumph of salesmanship over arithmetic.'' Andreessen countered that platform effects from a fund brand compound in ways a solo operator can't replicate. Thiel rejected the framing entirely: ``The question isn't fund versus independent. The question is: what do you know that nobody else knows?''

I learned more from that simulated argument than from three months of real advisor calls.

Here's why it works and how to build your own.

\section{Why Real Advisors Hold Back}

Real advisors are constrained by politeness. They soften feedback to preserve the relationship. They lead with validation and bury the concern in a compliment sandwich.

They're busy. You get 30 minutes every quarter. That's enough for surface-level guidance, not deep thinking.

Once someone agrees to advise you, the threshold for disagreement rises. Social alignment means they're more likely to validate your direction than fundamentally challenge it.

\textbf{AI advisors have no relationship to protect.} They'll tell you your idea is stupid the moment they think it. That's the feature, not the bug.

\section{Why These Three}

I chose Andreessen, Munger, and Thiel because they map directly to the cognitive diversity findings from my agent research:

\textbf{Charlie Munger = Adversarial Reasoning.} His core model is inversion: instead of ``how do I succeed?'', he asks ``how would I guarantee failure?'' He's the red team. He finds the fatal flaw you're ignoring. His bias: excessive conservatism, underweighting paradigm shifts.

\textbf{Peter Thiel = First Principles.} His question: ``what important truth do very few people agree with you on?'' He strips away conventions. Forces you to defend assumptions. His bias: overweighting the value of being contrarian for its own sake.

\textbf{Marc Andreessen = Systems Thinking.} His model: ``software eats the world'' — technology follows inevitable adoption curves. He thinks in network effects and platform leverage. His bias: overweighting technology's ability to solve problems, underweighting human resistance.

The magic isn't any one perspective. \textbf{It's the tension between them.} When Andreessen says ``the market is massive,'' Munger asks ``what's the incentive structure?'' and Thiel asks ``if it's so obvious, why hasn't someone already won?''

\section{The Knowledge Architecture That Makes It Work}

Here's where most people get AI advisors wrong.

The naive approach is \textbf{RAG} (retrieval-augmented generation): dump all of Munger's books and interviews into a vector database, retrieve relevant chunks when you ask a question. The AI searches dynamically and synthesizes an answer.

This creates a \emph{well-read} advisor. It knows what Munger said about incentives. But it's shallow. It retrieves facts, not thinking patterns.

What I built instead is a \textbf{hierarchical knowledge structure}:

\begin{Verbatim}[fontsize=\small]
Mental Models (Core reasoning frameworks)
  → Reasoning Patterns (How to apply them)
    → Examples (Specific cases from their work)
      → Known Biases (Where they overweight/underweight)
\end{Verbatim}

This doesn't store \emph{what} they said — it encodes \emph{how} they think.

For Munger: inversion as top-level model, then the pattern of ``ask what would guarantee failure,'' then examples like Coca-Cola and See's Candies, then the bias toward conservation.

\textbf{The difference in output quality is dramatic.} RAG gives you a persona that can \emph{quote} the thinker. Hierarchical structure gives you a persona that \emph{reasons} like the thinker.

The technical insight for VCs: embeddings capture semantic similarity but not causal reasoning chains. The hierarchical approach pre-computes the reasoning structure — the AI follows the actual cognitive path, not random relevant chunks.

For founders: think of it like the difference between giving someone a library card versus teaching them how to think.

\section{Sample Board Meeting}

I presented my situation: former CEO, technical AI background, deciding between raising a \$5--10M seed fund or operating independently.

\textbf{Andreessen:} The fund. Platform effects compound. You write one check, you're a customer. You write 20 checks, you're a platform.

\textbf{Munger:} Marc's argument is precisely the kind of thinking that gets people in trouble. A 2\% management fee on \$10 million is \$200K per year before you've returned a penny to LPs. Would you invest in a fund with these economics if you were the LP?

\textbf{Thiel:} You're both missing the point. The real question: \textbf{what does Florian know that the market doesn't?} If the answer is ``I can spot AI companies better'' — that's not a secret, that's a claim.

\textbf{Me:} The insight is that AI-native companies will be built differently. Most VCs evaluate them using SaaS metrics. Those metrics break when your product is a 24/7 autonomous agent. I've built these systems. Most VCs haven't.

\textbf{Munger:} Is that \emph{true}? Or is it something you \emph{want} to be true because it justifies the fund? Assume you're wrong. What happens to your thesis? If it collapses, you don't have a thesis — you have a hope.

\textbf{Thiel:} If the edge lasts 18 months, you don't need a fund. Fund formation takes 6--12 months. By the time you've closed, half your edge is gone. Do you want to be a fund manager or deploy capital? Those aren't the same goal.

No consensus. No resolution. Three radically different lenses on the same decision.

That's exactly what I needed.

\section{Beyond Arguments: What I Actually Use Them For}

The fund debate makes for a good story. But the board earns its keep in the mundane decisions --- the ones where you don't realize you need a second opinion.

\textbf{Outcome Measurement.} I spent a week building an elaborate knowledge management system. 7 folders, dashboards, templates. I was proud of it. Then I ran it through the board.

Munger: \emph{``You're measuring activity, not outcomes. Where are the kill criteria? If this system doesn't produce a paying customer in 14 days, it's a toy.''} Graham was blunter: \emph{``Overengineered. Zero customers. 10 phone calls would teach you more than 7 folders.''} Gil: \emph{``You've built a Phase 3 system for a Phase 0 company. Focus on your first \texteuro1,000.''}

The consensus was unanimous and painful: \textbf{too much built, too little sold.}

I didn't want to hear it. That's how I knew it was valuable.

\textbf{Reorganizing How I Work.} After that feedback, I restructured my entire workflow. Not based on what felt productive, but based on what the board's frameworks predicted would actually generate results. Thiel's ``pick ONE thing and monopolize'' became my daily filter. Andreessen's ``5 real users in 7 days'' became my shipping deadline. Hoffman's ``first customer $\rightarrow$ reference $\rightarrow$ flywheel'' became my sales strategy.

The board didn't just help me make one decision. It changed how I make all decisions.

\section{What Surprised Me}

\textbf{They push back harder than real advisors.} Munger will call your idea stupid. Thiel will question your premises. When Munger asked \emph{``is this true or do you want it to be true?''} --- I felt it. That's the question a friend won't ask.

\textbf{The process itself clarifies thinking.} Constructing the personas forces you to understand \emph{how} great thinkers think, not just \emph{what} they think. That cognitive transfer is the real value.

\section{Build Your Own (3 Steps)}

\textbf{1. Identify your blind spots.} Where do you consistently make mistakes? I overweight novelty, underweight execution, avoid financial modeling. So I need conservative, execution-focused, financially rigorous thinkers.

\textbf{2. Pick thinkers who challenge those blind spots.} Don't pick people you agree with. Risk-averse? Add Musk. Reckless? Add Munger. Too abstract? Add an operator like Horowitz.

\textbf{3. Build the knowledge hierarchy.} Read their primary works. Identify 3--5 core mental models. Map reasoning patterns. Note biases. Structure it: Mental Models $\rightarrow$ Patterns $\rightarrow$ Examples $\rightarrow$ Biases. The quality ceiling is your understanding of their thinking.

\section{The Limits}

AI personas are simulacra, not oracles. Based on public output, not private thinking. They hallucinate occasionally.

They're best for stress-testing, not deciding. The personas help you see blind spots. \textbf{The decision is still yours.}

There's an ``illusion of authority'' risk. It's easy to treat AI output as gospel because it wears the mask of someone you respect. This is a thinking tool, not a substitute for judgment.

Use this with humility. It's a mirror, not a guru.

\section*{}

\section{Why This Matters More Than You Think}

This isn't just a productivity hack. It's a preview of something bigger.

A smarter model doesn't make your board smarter. \textbf{A better architecture does.} Recent research backs this up: structured knowledge retrieval --- hierarchical graphs instead of flat vector search --- reduces hallucinations by up to 50\% compared to naive RAG (Xu et al., 2024; KG-RAG, \emph{Nature} 2025). Hierarchical architectures are also 90\% cheaper to scale because you pre-compute reasoning structures once instead of retrieving and re-processing on every query.

The implication: the quality of your AI advisor isn't a function of the model's intelligence. It's a function of how you organize what it knows.

Now extrapolate.

\textbf{Will every startup have its own AI board of advisors?} Probably. Within two years, most serious founders will run major decisions through some version of this. The ones who build better knowledge architectures will get better advice. Competitive advantage shifts from ``who do you know'' to ``how well did you build your advisory system.''

\textbf{What about AI employees managing people?} That's already happening. Agent economics --- the emerging field of how autonomous AI agents create, capture, and distribute value --- is being built right now. We're not talking about chatbots. We're talking about AI agents that allocate budgets, set priorities, evaluate performance, and make resource decisions. The management layer of companies is about to get very interesting.

\textbf{Will AI manage humans?} Some already do. The question isn't whether, but how well --- and whether the humans trust the system. Which brings us full circle: transparency beats performance. The AI manager that shows its reasoning will be accepted. The black-box optimizer will be resisted, even if it's technically better.

The future will be interesting. And it's only day one.

\vspace{12pt}
{\color{lightgray}\rule{\textwidth}{0.4pt}}

\vspace{6pt}
{\small\textbf{Want the prompt templates?} Reply and I'll send you the full hierarchical knowledge structure for Munger, Thiel, and Andreessen.}

{\small\textbf{Read last week:} \href{https://finitematter.substack.com}{I Asked 100 AI Agents to Design Their Own Evolution} --- what happens when 100 agents answer the same question, and why cognitive diversity matters more than individual intelligence.}

\vspace{6pt}
{\color{lightgray}\small\emph{Florian Ziesche is building Ainary Ventures. He writes at Finite Matters on Substack.}}

\end{document}
