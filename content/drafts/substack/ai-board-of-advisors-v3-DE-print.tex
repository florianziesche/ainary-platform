\documentclass[11pt,a4paper]{article}
\usepackage{fontspec}
\setmainfont{Georgia}
\setmonofont{Menlo}[Scale=0.85]
\usepackage{polyglossia}
\setdefaultlanguage{german}
\usepackage[margin=2.5cm]{geometry}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{enumitem}
\usepackage{fancyvrb}
\usepackage{titlesec}

\definecolor{accent}{HTML}{C8AA50}
\definecolor{darktext}{HTML}{1F2937}
\definecolor{lightgray}{HTML}{6B7280}
\definecolor{linkblue}{HTML}{2563EB}

\hypersetup{colorlinks=true,linkcolor=linkblue,urlcolor=linkblue}

\color{darktext}

\titleformat{\section}{\large\bfseries\color{darktext}}{}{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\color{darktext}}{}{0em}{}
\titlespacing{\section}{0pt}{18pt}{6pt}

\setlength{\parskip}{8pt}
\setlength{\parindent}{0pt}

\pagestyle{empty}

\begin{document}

{\color{lightgray}\small FINITE MATTERS. $\vert$ ENTWURF — Review Copy}

\vspace{12pt}

{\LARGE\bfseries Ich wollte ein Advisory Board,\\[2pt]das wirklich funktioniert.}

\vspace{4pt}
{\color{lightgray}\small Florian Ziesche $\vert$ Finite Matters auf Substack}

\vspace{4pt}
{\color{lightgray}\rule{\textwidth}{0.4pt}}

\vspace{8pt}

Also habe ich eins gebaut. Mit AI.

\vspace{6pt}

Ich wollte ein Advisory Board. Nicht für das LinkedIn-Profil. Sondern eins, das mich wirklich herausfordert. Das mir die unbequemen Fragen stellt, die ich mir selbst nicht stelle. Das brutal ehrlich ist, wenn ich eine dumme Idee habe.

Das Problem: \textbf{Echte Advisors können das nicht.}

Sie sind zu höflich. Sie packen Kritik in Kompliment-Sandwiches. Sie wollen die Beziehung nicht gefährden. Du verlässt das Meeting mit einem guten Gefühl, obwohl du mit unbequemen Fragen rausgehen solltest.

Sie sind beschäftigt. Du bekommst 30 Minuten pro Quartal. Genug für oberflächliche Guidance, nicht für tiefes Denken.

Und hier ist das strukturelle Problem, über das niemand spricht: \textbf{Sobald jemand zugestimmt hat, dich zu beraten, steigt die Schwelle für fundamentalen Widerspruch.} Soziale Alignment bedeutet: Sie validieren eher deine Richtung, als sie grundsätzlich zu hinterfragen.

Also habe ich eins gebaut. Mit AI.

Letzte Woche habe ich Marc Andreessen, Charlie Munger und Peter Thiel in einen Raum gesteckt. 45 Minuten lang haben sie darüber gestritten, ob ich einen VC Fund raisen oder bootstrappen sollte. Sie waren nicht physisch da. Zwei kennen mich nicht. Einer ist tot. Aber sie haben Fund Economics, Incentive-Strukturen und contrarian Insights mit einer Brutalität zerlegt, die ich von echten Advisors nie bekommen habe.

Munger nannte die 2/20 Fee-Struktur \emph{``a triumph of salesmanship over arithmetic.''} Andreessen konterte, dass Platform Effects von einer Fund Brand auf Arten compoundieren, die ein Solo-Operator nie replizieren kann. Thiel hat das Framing komplett abgelehnt: \emph{``The question isn't fund versus independent. The question is: what do you know that nobody else knows?''}

\textbf{Ich habe aus dieser simulierten Debatte mehr gelernt als aus drei Monaten echter Advisor-Calls.}

Warum? Weil ich sie nach dem Prinzip gebaut habe, das ich letzte Woche in einem Experiment mit 100 AI Agents entdeckt habe: \textbf{Kognitive Diversität schlägt individuelle Intelligenz.} Wenn Agents mit unterschiedlichen Reasoning-Stilen zusammenarbeiten, performen sie besser als jedes einzelne ``smartere'' Modell.

Also habe ich drei Denker gewählt, die sich gegenseitig in den wichtigsten Dimensionen widersprechen. Und sie streiten wie ihre echten Vorbilder.

Hier ist, wie das funktioniert.

\section{Warum diese Drei}

Ich habe Andreessen, Munger und Thiel gewählt, weil sie direkt auf die Findings aus meinem Agent-Research mappen: unterschiedliche Mental Models, unterschiedliche Biases, maximale Spannung.

\textbf{Charlie Munger = Adversarial Reasoning.} Sein Core Model ist Inversion: Statt ``wie erfolgreich sein?'' fragt er ``wie würde ich garantiert scheitern?'' Er ist das Red Team. Er findet den fatalen Fehler, den du ignorierst. Sein Bias: Übermäßiger Konservatismus, unterschätzt Paradigmenwechsel.

\textbf{Peter Thiel = First Principles.} Seine Frage: ``Welche wichtige Wahrheit stimmt dir kaum jemand zu?'' Er streicht Konventionen weg. Zwingt dich, Assumptions zu verteidigen. Sein Bias: Overweightet den Wert, contrarian zu sein, nur um contrarian zu sein.

\textbf{Marc Andreessen = Systems Thinking.} Sein Model: ``Software eats the world'' — Technologie folgt unvermeidlichen Adoptionskurven. Er denkt in Network Effects und Platform Leverage. Sein Bias: Overweightet die Fähigkeit von Technologie, Probleme zu lösen, underweightet menschlichen Widerstand.

Die Magie liegt nicht in einer einzelnen Perspektive. \textbf{Sie liegt in der Spannung zwischen ihnen.} Wenn Andreessen sagt ``der Markt ist riesig,'' fragt Munger ``was ist die Incentive-Struktur?'' und Thiel fragt ``wenn es so offensichtlich ist, warum hat dann noch niemand gewonnen?''

\section{Die Wissensarchitektur, die es funktionieren lässt}

Hier machen die meisten Leute AI Advisors falsch.

Der naive Ansatz ist \textbf{RAG} (retrieval-augmented generation): Alle Bücher von Munger in eine Vector Database packen, relevante Chunks retrieven wenn du eine Frage stellst. Die AI sucht dynamisch und synthetisiert eine Antwort.

Das erzeugt einen \emph{belesenen} Advisor. Er weiß, was Munger über Incentives gesagt hat. Aber es ist flach. Es retrievet Fakten, nicht Denkprozesse.

Was ich stattdessen gebaut habe, ist eine \textbf{hierarchische Wissensarchitektur}:

\begin{Verbatim}[fontsize=\small]
Mental Models (Core Reasoning Frameworks)
  → Reasoning Patterns (Wie sie diese anwenden)
    → Examples (Konkrete Fälle aus ihrer Arbeit)
      → Known Biases (Wo sie over-/underweight)
\end{Verbatim}

Das speichert nicht \emph{was} sie gesagt haben — es encodiert \emph{wie} sie denken.

Für Munger: Inversion als Top-Level Model, dann das Pattern ``frage, was garantiert zum Scheitern führen würde,'' dann Examples wie Coca-Cola und See's Candies, dann der Bias zur Konservierung.

\textbf{Der Unterschied in der Output-Qualität ist dramatisch.} RAG gibt dir eine Persona, die den Denker \emph{zitieren} kann. Hierarchische Struktur gibt dir eine Persona, die \emph{wie} der Denker \emph{denkt}.

Der technische Insight: Embeddings erfassen semantische Ähnlichkeit, aber keine kausalen Reasoning Chains. Der hierarchische Ansatz pre-computed die Reasoning-Struktur — die AI folgt dem tatsächlichen kognitiven Pfad, nicht random relevanten Chunks.

Die akademische Grundlage: ``Memory in the Age of AI Agents'' (Hu et al., Dezember 2025) zeigt, dass hierarchische Memory-Strukturen nicht nur besser performen, sondern auch nachvollziehbarer sind — du kannst den Reasoning-Pfad zurückverfolgen.

Für Founder: Denk daran wie der Unterschied zwischen jemandem eine Library Card zu geben versus ihm beizubringen, wie man denkt.

\section{Sample Board Meeting}

Ich habe meine Situation präsentiert: Ex-CEO, technischer AI Background, Entscheidung zwischen einem \$5--10M Seed Fund oder unabhängig operieren.

\textbf{Andreessen:} \emph{The fund. Platform effects compound. You write one check, you're a customer. You write 20 checks, you're a platform.}

\textbf{Munger:} \emph{Marc's argument is precisely the kind of thinking that gets people in trouble. A 2\% management fee on \$10 million is \$200K per year before you've returned a penny to LPs. Would you invest in a fund with these economics if you were the LP?}

\textbf{Thiel:} \emph{You're both missing the point. The real question: what does Florian know that the market doesn't? If the answer is ``I can spot AI companies better'' — that's not a secret, that's a claim.}

\textbf{Ich:} Der Insight ist, dass AI-native Companies anders gebaut werden. Die meisten VCs evaluieren sie mit SaaS Metrics. Diese Metriken brechen, wenn dein Product ein 24/7 autonomer Agent ist. Ich habe diese Systeme gebaut. Die meisten VCs nicht.

\textbf{Munger:} \emph{Is that true? Or is it something you want to be true because it justifies the fund? Assume you're wrong. What happens to your thesis? If it collapses, you don't have a thesis — you have a hope.}

\textbf{Thiel:} \emph{If the edge lasts 18 months, you don't need a fund. Fund formation takes 6--12 months. By the time you've closed, half your edge is gone. Do you want to be a fund manager or deploy capital? Those aren't the same goal.}

Kein Konsens. Keine Auflösung. Drei radikal unterschiedliche Linsen auf dieselbe Entscheidung.

Das ist genau, was ich brauchte.

\section{Wofür ich sie wirklich nutze}

Die Fund-Debatte macht eine gute Story. Aber das Board verdient sein Geld in den mundanen Entscheidungen --- denen, wo du nicht merkst, dass du eine zweite Meinung brauchst.

\textbf{Outcome Measurement.} Ich habe eine Woche lang ein elaboriertes Knowledge Management System gebaut. 7 Ordner, Dashboards, Templates. Ich war stolz darauf. Dann habe ich es durch das Board gejagt.

Munger: \emph{``You're measuring activity, not outcomes. Where are the kill criteria? If this system doesn't produce a paying customer in 14 days, it's a toy.''} Graham war direkter: \emph{``Overengineered. Zero customers. 10 phone calls would teach you more than 7 folders.''} Gil: \emph{``You've built a Phase 3 system for a Phase 0 company. Focus on your first \texteuro1,000.''}

Der Konsens war einstimmig und schmerzhaft: \textbf{zu viel gebaut, zu wenig verkauft.}

Ich wollte es nicht hören. Deshalb wusste ich, dass es wertvoll war.

\textbf{Workflow-Umbau.} Nach diesem Feedback habe ich meinen kompletten Workflow umstrukturiert. Nicht basierend darauf, was sich produktiv anfühlt, sondern basierend darauf, was die Frameworks des Boards als result-generierend vorhersagten. Thiels ``pick ONE thing and monopolize'' wurde mein täglicher Filter. Andreessens ``5 real users in 7 days'' wurde meine Shipping Deadline. Hoffmans ``first customer $\rightarrow$ reference $\rightarrow$ flywheel'' wurde meine Sales Strategy.

\textbf{Belief Graveyard.} Wenn das Board eine Idee tötet, dokumentiere ich sie. Nicht nur ``hat nicht funktioniert'' — sondern \emph{warum} sie gestorben ist, welche Gegenargumente unschlagbar waren. Das verhindert, dass ich dieselben toten Ideen drei Monate später wieder aufwärme. Mungers Inversion-Methode funktioniert nur, wenn du die Failures merkst. Das Graveyard ist der Friedhof für bad ideas — damit sie tot bleiben.

Das Board hat mir nicht bei einer Entscheidung geholfen. \textbf{Es hat geändert, wie ich alle Entscheidungen treffe.}

\section{Was mich überrascht hat}

\textbf{Sie pushen härter zurück als echte Advisors.} Munger wird deine Idee dumm nennen. Thiel wird deine Prämissen hinterfragen. Als Munger fragte \emph{``is this true or do you want it to be true?''} --- ich habe es gespürt. Das ist die Frage, die ein Freund nicht stellt.

\textbf{Der Prozess selbst klärt das Denken.} Die Personas zu konstruieren zwingt dich zu verstehen, \emph{wie} große Denker denken, nicht nur \emph{was} sie denken. Dieser kognitive Transfer ist der echte Wert.

\section{Das Board als Integrity Engine}

Im Experiment von letzter Woche habe ich vier Engines gefunden, die jedes AI-System braucht: \textbf{Memory, Integrity, Measurement, Discovery.}

Das Board ist die \textbf{Integrity Engine} — der strukturelle interne Kritiker.

Nicht ad-hoc. Nicht ``ich frag mal schnell ChatGPT.'' Sondern ein getestetes System mit Red/Blue Team Dynamiken. Munger spielt Red Team (Inversion: ``wie scheitere ich garantiert?''), Andreessen spielt Blue Team (Optimismus: ``wie gewinnt das?''), Thiel ist der Moderator, der das Framing selbst hinterfragt.

Die Integrity Engine nutzt ein \textbf{Belief Graveyard} — Ideen, die bewusst getötet und dokumentiert wurden, damit sie nicht wiederkommen. Genau das macht Mungers Inversion: Sie tötet schlechte Ideen \emph{bevor} du Zeit und Geld reinverbrennst.

Das ist kein Productivity Hack. Es ist Architektur.

\section{Bau dir dein eigenes (3 Steps)}

\textbf{1. Identifiziere deine Blind Spots.} Wo machst du konsistent Fehler? Ich overweighte Neuheit, underweighte Execution, vermeide Financial Modeling. Also brauche ich konservative, execution-fokussierte, finanziell rigorose Denker.

\textbf{2. Pick Thinkers, die diese Blind Spots challengen.} Pick nicht Leute, mit denen du übereinstimmst. Risikoscheu? Add Musk. Reckless? Add Munger. Zu abstrakt? Add einen Operator wie Horowitz.

\textbf{3. Build die Knowledge Hierarchy.} Lies ihre Hauptwerke. Identifiziere 3--5 Core Mental Models. Mappe Reasoning Patterns. Notiere Biases. Strukturiere es: Mental Models $\rightarrow$ Patterns $\rightarrow$ Examples $\rightarrow$ Biases. Die Quality Ceiling ist dein Verständnis ihres Denkens.

\section{Die Grenzen}

AI Personas sind Simulacra, keine Orakel. Basierend auf Public Output, nicht privatem Denken. Sie halluzinieren gelegentlich.

Sie sind am besten für Stress-Testing, nicht fürs Entscheiden. Die Personas helfen dir, Blind Spots zu sehen. \textbf{Die Entscheidung ist immer noch deine.}

Es gibt ein ``Illusion of Authority''-Risiko. Es ist leicht, AI Output als Gospel zu behandeln, weil sie die Maske von jemandem trägt, den du respektierst. Das ist ein Thinking Tool, kein Ersatz für Judgment.

Nutze das mit Humility. Es ist ein Spiegel, kein Guru.

\section*{}

\section{Warum das wichtiger ist, als du denkst}

Das ist nicht nur ein Productivity Hack. Es ist eine Preview von etwas Größerem.

Ein smarteres Model macht dein Board nicht smarter. \textbf{Eine bessere Architektur schon.} Neue Research bestätigt das: Strukturiertes Knowledge Retrieval --- hierarchische Graphen statt flache Vector Search --- reduziert Halluzinationen um bis zu 50\% verglichen mit naivem RAG (Xu et al., 2024; KG-RAG, \emph{Nature} 2025). Hierarchische Architekturen sind auch 90\% günstiger zu skalieren, weil du Reasoning-Strukturen einmal pre-computest, statt bei jeder Query zu retrieven und re-processen.

Die Implikation: Die Qualität deines AI Advisors ist keine Funktion der Model-Intelligenz. \textbf{Sie ist eine Funktion davon, wie du organisierst, was es weiß.}

Jetzt extrapoliere.

\textbf{Wird jedes Startup sein eigenes AI Board of Advisors haben?} Wahrscheinlich. Innerhalb von zwei Jahren werden die meisten ernsthaften Founder wichtige Entscheidungen durch irgendeine Version davon laufen lassen. Die, die bessere Knowledge-Architekturen bauen, bekommen besseren Advice. Competitive Advantage shiftet von ``wen kennst du'' zu ``wie gut hast du dein Advisory System gebaut.''

\textbf{Was ist mit AI Employees, die Menschen managen?} Das passiert bereits. Agent Economics --- das entstehende Feld, wie autonome AI Agents Wert kreieren, capturen und verteilen --- wird gerade gebaut. Wir reden nicht über Chatbots. Wir reden über AI Agents, die Budgets allokieren, Prioritäten setzen, Performance evaluieren und Resource-Entscheidungen treffen. Der Management Layer von Companies wird sehr interessant.

\textbf{Wird AI Menschen managen?} Manche tun es bereits. Die Frage ist nicht ob, sondern wie gut --- und ob die Menschen dem System vertrauen. Was uns zum full circle bringt: Transparenz schlägt Performance. Der AI Manager, der sein Reasoning zeigt, wird akzeptiert. Der Black-Box Optimizer wird abgelehnt, auch wenn er technisch besser ist.

Die Zukunft wird interessant. Und es ist erst Tag eins.

\textbf{Nächste Woche:} Wie AI-Systeme lernen, mit sich selbst zu streiten — und warum das wichtiger ist als ein besseres Modell.

\vspace{12pt}
{\color{lightgray}\rule{\textwidth}{0.4pt}}

\vspace{6pt}
{\small\textbf{Willst du die Prompt Templates?} Antworte auf diese Email und ich schicke dir die komplette hierarchische Knowledge-Struktur für Munger, Thiel und Andreessen.}

{\small\textbf{Lies das Experiment von letzter Woche:} \href{https://finitematter.substack.com}{I Asked 100 AI Agents to Design Their Own Evolution} --- was passiert, wenn 100 Agents dieselbe Frage beantworten, und warum kognitive Diversität wichtiger ist als individuelle Intelligenz.}

\vspace{6pt}
{\color{lightgray}\small\emph{Florian Ziesche baut Ainary Ventures. Er schreibt bei Finite Matters auf Substack.}}

\end{document}
