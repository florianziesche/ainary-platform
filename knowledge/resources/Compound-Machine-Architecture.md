---
type: note
last_verified: 2026-02-15
status: active
created: 2026-02-11
tags: []
tier: KNOWLEDGE
expires: 2027-02-19
---

# Compound Machine Architektur

**Version:** 1.0  
**Stand:** 11. Februar 2026  
**Autor:** Mia (OpenClaw Agent) fÃ¼r Florian Ziesche  
**Status:** Living Document

---

## Executive Summary

Wir bauen eine **Personal Intelligence Compound Machine** â€” kein General-Purpose [[AI]] wie ChatGPT, sondern ein maÃŸgeschneidertes System, das mit jedem Tag, jedem Paper, jedem Projekt smarter wird. Ein System, das nicht skalieren MUSS, sondern fÃ¼r EINEN Menschen (Florian) optimal wird.

**Die vier SÃ¤ulen:**

1. **Research Machine:** Automatisierte Paper-Intake, Bewertung, Synthese
2. **Content Machine:** Research â†’ Artikel-Ideen â†’ Drafts â†’ Distribution
3. **Hierarchical Memory:** Von Raw Data zu Principles, wie menschliches GedÃ¤chtnis
4. **Self-Improvement Loop:** Lernt aus Feedback, kalibriert sich selbst

**Unser Moat:** WÃ¤hrend Meta/[[Google]] General Intelligence bauen (die fÃ¼r alle funktionieren muss, aber fÃ¼r niemanden optimal ist), bauen wir **Personal Intelligence** â€” tief personalisiert, domain-spezifisch, compound Ã¼ber Zeit.

---

## Teil 1: Warum WIR besser sind als Meta/[[Google]]/OpenAI

### Der fundamentale Unterschied

| Dimension | Meta/[[Google]]/OpenAI | Compound Machine |
|-----------|-------------------|------------------|
| **Ziel** | General Intelligence | Personal Intelligence |
| **User** | Milliarden | Einer (Florian) |
| **Personalisierung** | Null (Privacy, Skalierung) | 100% (alles ist custom) |
| **Memory** | Stateless oder Generic RAG | Hierarchisch, personalisiert, consolidating |
| **Lernen** | Pre-Training, RLHF | Continuous Learning von Florians Feedback |
| **DomÃ¤ne** | Generalist | Spezialist ([[VC]], [[AI]], Content, Ops) |
| **Constraint** | MUSS skalieren | DARF nicht skalieren (QualitÃ¤t > Scale) |
| **Vorteil** | Breite | Tiefe |

### Warum Personal Intelligence gewinnt (fÃ¼r Florian)

**1. Deep Context beats General Knowledge**

ChatGPT weiÃŸ viel. Aber es weiÃŸ NICHTS Ã¼ber:
- Florians [[VC]]-Thesis ([[AI]]-first B2B SaaS, European founders)
- Florians Writing Style (direkt, no-fluff, founder-operator Perspektive)
- Florians Obsidian Vault (300+ Notes, akkumuliertes Wissen seit Jahren)
- Florians vergangene Projekte, Fehler, Learnings

**Die Compound Machine weiÃŸ das alles.** Und mit jedem Tag wird der Context tiefer.

**2. Hierarchical Memory beats Flat RAG**

MemGPT (Paper #7) zeigt: Hierarchisches Memory ist mÃ¤chtiger als Flat Retrieval. Aber MemGPT ist generic. **Unsere Implementierung ist personalisiert:**

- **Layer 4 (Raw):** Jedes Paper, jedes Meeting, jede Notiz
- **Layer 3 (Episodic):** "Florian hat diese Woche 5 Papers Ã¼ber Multi-Agent Systems gelesen"
- **Layer 2 (Semantic):** "Florian's Thesis: Multi-Agent > Monolith fÃ¼r komplexe Workflows"
- **Layer 1 (Principles):** "Simplicity > Complexity, Ship > Perfect, Learn > Plan"

Meta's RAG hat nur Layer 4. Wir haben alle vier, und sie konsolidieren automatisch.

**3. Self-Improvement ohne Skalierungs-Constraint**

Reflexion (Paper #12) und Constitutional [[AI]] (Paper #14) zeigen: Agents kÃ¶nnen sich selbst verbessern. Aber bei Meta/[[Google]] mÃ¼ssen Verbesserungen fÃ¼r ALLE User funktionieren.

**Bei uns:** Jede Verbesserung ist Florian-spezifisch.
- "Florian bevorzugt knappe Emails" â†’ kÃ¼rzer schreiben
- "Florian liked Paper X, not Y" â†’ besseres Scoring-Modell
- "Florian's Artikel Ã¼ber Z performed gut" â†’ mehr solche Themen

Das ist **Compound Learning**: Jeder Loop macht das System ein bisschen besser FÃœR FLORIAN.

**4. Domain Expertise beats Generalist Knowledge**

ChatGPT weiÃŸ alles ein bisschen. **Compound Machine wird Experte in Florians Domains:**
- [[VC]] (durch Papers, Fund Research, Deal Memos)
- [[AI]] Agents (durch Paper-Intake, Synthese, Experimentation)
- Content Creation (durch Writing, Feedback, Performance-Tracking)
- Operations (durch Florians SOPs, Workflows, Tools)

Nach 6 Monaten: **Compound Machine > ChatGPT fÃ¼r Florians Use Cases.**

### Was bestehende Frameworks nicht haben

**MemGPT (Paper #7):**
- âœ… Hierarchical Memory mit Paging
- âŒ Keine Personalisierung (generic fÃ¼r alle)
- âŒ Kein Content-Creation Loop
- âŒ Keine Self-Improvement basierend auf User-Feedback

**Generative Agents (Paper #8):**
- âœ… Memory: Observations â†’ Reflections â†’ Planning
- âŒ Designed fÃ¼r Simulation, nicht Production
- âŒ Keine Research-Integration
- âŒ Keine Content-Pipeline

**AutoGen/MetaGPT (Papers #4, #5):**
- âœ… Multi-Agent-Orchestrierung
- âŒ Generic (keine Personalisierung)
- âŒ Keine Memory-Hierarchie
- âŒ Kein Self-Improvement

**Wir kombinieren das Beste aus allen:**
- MemGPT's Hierarchical Memory
- Generative Agents' Reflection Loop
- AutoGen's Multi-Agent Architecture
- Reflexion's Self-Improvement
- **PLUS:** Personalisierung, Domain-Fokus, Content-Pipeline

---

## Teil 2: System-Architektur

### 2.1 Gesamtsystem (ASCII)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPOUND MACHINE CORE                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   RESEARCH   â”‚  â”‚   CONTENT    â”‚  â”‚     SELF     â”‚          â”‚
â”‚  â”‚   MACHINE    â”‚â†’â†’â”‚   MACHINE    â”‚â†’â†’â”‚ IMPROVEMENT  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â†‘                                      â†“                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                   Feedback Loop                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†•
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      HIERARCHICAL MEMORY SYSTEM        â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚ L1: Principles (permanent)       â”‚  â”‚
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
         â”‚  â”‚ L2: Domain Knowledge (growing)   â”‚  â”‚
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
         â”‚  â”‚ L3: Episodic (consolidating)     â”‚  â”‚
         â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
         â”‚  â”‚ L4: Raw (decay, high-volume)     â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†•
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        INFRASTRUCTURE LAYER            â”‚
         â”‚  OpenClaw | Obsidian | MCP | Skills   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Research Machine

**Aufgabe:** Automatisch Papers finden, bewerten, zusammenfassen, und Patterns erkennen.

#### Komponenten

**1. Intake Agent (LIVE)**
- **Was:** TÃ¤glich ArXiv, [[Google]] Scholar, Twitter scrapen
- **Wie:** Cron Job (bereits aktiv: `blogwatcher`)
- **Papers:** ReAct (#1) fÃ¼r Tool-Use, Toolformer (#2) fÃ¼r self-extending
- **Output:** Liste neuer Papers/Posts â†’ Obsidian Inbox

**Implementation:**
```python
# Pseudo-Code
def intake_agent():
    sources = ['arxiv_rss', 'google_scholar', 'twitter_ai_feed']
    for source in sources:
        papers = fetch_new_papers(source)  # ReAct: Action
        relevant = filter_by_keywords(papers, florian_interests)  # Heuristic
        for paper in relevant:
            save_to_obsidian_inbox(paper)
```

**Build vs Use:**
- âœ… **Build:** Custom Scraper (bestehende Tools wie Zotero sind zu generisch)
- âš ï¸ **Use:** ArXiv [[API]] (robust, maintained)

---

**2. Scorer Agent**
- **Was:** Bewertet Papers nach Relevanz fÃ¼r Florian
- **Wie:** [[LLM]]-basiert (Sonnet 4.5), Custom Scoring-Modell
- **Papers:** Few-Shot Learning (implizit in allen [[LLM]]s)
- **Output:** Score 1-10, Reasoning

**Scoring-Kriterien:**
- Praxis-Relevanz (kann ich das umsetzen?)
- Neuheit (ist das wirklich neu oder Hype?)
- Rigor (solide Methodik?)
- Florian's Domain-Fit ([[VC]], [[AI]], Content, Ops)

**Implementation:**
```python
def score_paper(paper):
    prompt = f"""
    Bewerte dieses Paper fÃ¼r Florian (VC, AI Agent Builder, Content Creator):
    
    Title: {paper.title}
    Abstract: {paper.abstract}
    
    Kriterien:
    1. Praxis-Relevanz (1-10)
    2. Neuheit (1-10)
    3. Rigor (1-10)
    4. Domain-Fit (1-10)
    
    Output: JSON mit Scores + Reasoning
    """
    response = [[LLM]].generate(prompt, response_format='json')
    return response
```

**Build vs Use:**
- âœ… **Build:** Custom Scoring (bestehende Tools wie Semantic Scholar haben generic metrics)
- ğŸ¤” **Hybrid:** Nutze Semantic Scholar's Citation Count als Feature, aber eigenes [[LLM]]-Scoring

---

**3. Reader Agent**
- **Was:** Liest Paper-PDFs, extrahiert Text, erstellt Zusammenfassungen
- **Wie:** PDF â†’ Text Extraktion, Summarization mit [[LLM]]
- **Papers:** Chain-of-Thought (#17) fÃ¼r structured reasoning
- **Output:** 200-Wort-Summary + Key Insights + Obsidian Note

**Implementation:**
```bash
# PDF â†’ Text
pdftotext paper.pdf paper.txt

# Summarization
python summarize.py paper.txt > summary.md
```

**Build vs Use:**
- âš ï¸ **Use:** `pdftotext` (robust, maintained)
- âœ… **Build:** Custom Summarization-Prompt (generic summaries sind zu oberflÃ¤chlich)

---

**4. Critic Agent (Red Team)**
- **Was:** Kritisiert Papers (Was kÃ¶nnte falsch sein? Was fehlt?)
- **Wie:** Adversarial Prompting, Reflexion-Pattern
- **Papers:** Reflexion (#12), Constitutional [[AI]] (#14)
- **Output:** Kritik-Notizen im Obsidian

**Warum wichtig:** Verhindert Hype-Bias. Nicht jedes Paper mit 1000 Citations ist gut.

**Implementation:**
```python
def critique_paper(paper_summary):
    prompt = f"""
    Du bist ein skeptischer Wissenschaftler. Kritisiere dieses Paper:
    
    {paper_summary}
    
    Fragen:
    - Was kÃ¶nnte methodisch falsch sein?
    - Welche Annahmen sind unrealistisch?
    - Was fehlt in der Evaluation?
    - Ist das wirklich neu oder nur Re-Branding?
    
    Sei hart, aber fair.
    """
    return [[LLM]].generate(prompt)
```

**Build vs Use:**
- âœ… **Build:** Custom Critic-Prompting (keine bestehenden Tools)

---

**5. Synthesizer Agent**
- **Was:** WÃ¶chentliche Synthese: Patterns Ã¼ber Papers erkennen
- **Wie:** Multi-Paper Analysis, Graph of Thoughts (#20)
- **Papers:** Tree of Thoughts (#18), Graph of Thoughts (#20)
- **Output:** "Diese Woche: 3 Papers Ã¼ber Multi-Agent, 2 Ã¼ber Memory â†’ Trend erkannt"

**Implementation:**
```python
def weekly_synthesis(papers_this_week):
    prompt = f"""
    Analysiere diese {len(papers_this_week)} Papers als Ganzes:
    
    {paper_titles_and_summaries}
    
    Fragen:
    - Welche Themen wiederholen sich?
    - Gibt es einen Trend?
    - Welche Papers ergÃ¤nzen sich?
    - Was ist die "big idea" dieser Woche?
    
    Output: 500-Wort-Synthese
    """
    return [[LLM]].generate(prompt)
```

**Build vs Use:**
- âœ… **Build:** Custom Synthesis (keine Tools fÃ¼r personalisierte Trend-Erkennung)

---

### 2.3 Content Machine

**Aufgabe:** Research â†’ Content (Blog, LinkedIn, Twitter)

#### Workflow

```
Research Papers
       â†“
  Ideator Agent (Research â†’ Artikel-Ideen)
       â†“
  Outliner Agent (Idee â†’ Struktur)
       â†“
  Writer Agent (Struktur â†’ Draft, Florians Voice)
       â†“
  Critic Agent (Draft â†’ Feedback)
       â†“
  Refiner Agent (Self-Refine Loop)
       â†“
  Distributor Agent (Substack, LinkedIn, Twitter)
       â†“
  Performance Tracker
       â†“
  Feedback â†’ Research Priorities (Loop schlieÃŸt sich)
```

#### Komponenten

**1. Ideator Agent**
- **Was:** Generiert Artikel-Ideen basierend auf Papers
- **Papers:** ReAct (#1) fÃ¼r reasoning
- **Prompt:** "Du hast diese 3 Papers gelesen. Welche 5 Artikel-Ideen ergeben sich daraus fÃ¼r einen [[VC]]/[[AI]] Blog?"

**Build vs Use:**
- âœ… **Build:** Custom (keine Tools fÃ¼r Research â†’ Content Ideation)

---

**2. Writer Agent**
- **Was:** Schreibt Drafts in Florians Voice
- **Wie:** Few-Shot mit Florians bestehenden Artikeln
- **Papers:** Constitutional [[AI]] (#14) fÃ¼r style consistency
- **Output:** 1000-Wort-Draft

**Florians Voice (aus FLORIAN.md):**
- Direkt, no-fluff
- Founder-Operator Perspektive
- Praktisch > Akademisch
- Ehrlich (auch Ã¼ber Unsicherheiten)

**Implementation:**
```python
def write_article(outline, florians_articles):
    prompt = f"""
    Schreibe einen Blog-Artikel basierend auf diesem Outline:
    
    {outline}
    
    Style Guide (aus Florians bisherigen Artikeln):
    {florians_writing_samples}
    
    Regeln:
    - Direkt, keine Buzzwords
    - Praktische Takeaways
    - Ehrlich, nicht verkaufen
    - 1000 WÃ¶rter
    """
    return [[LLM]].generate(prompt)
```

**Build vs Use:**
- âœ… **Build:** Custom Voice-Modeling (generic writing assistants sind zu bland)
- âš ï¸ **Use:** [[Claude]]'s long-context fÃ¼r few-shot examples

---

**3. Self-Refine Loop**
- **Was:** Draft â†’ Selbstkritik â†’ Revision â†’ Repeat
- **Papers:** Self-Refine (#13), Reflexion (#12)
- **Output:** Polierter Artikel nach 2-3 Iterationen

**Implementation:**
```python
def refine_article(draft, max_iterations=3):
    for i in range(max_iterations):
        critique = critic_agent(draft)
        if critique.score > 8:
            break
        draft = revise_draft(draft, critique.feedback)
    return draft
```

**Build vs Use:**
- âœ… **Build:** Custom Self-Refine Loop (einfach zu implementieren)

---

**4. Repurposer Agent**
- **Was:** 1 Blog-Artikel â†’ LinkedIn Post + Twitter Thread + Carousel
- **Wie:** Format-spezifische Prompts
- **Output:** Multi-Platform Content

**Implementation:**
```python
def repurpose_article(article):
    linkedin = create_linkedin_post(article)  # 300 WÃ¶rter, Hook + Key Points
    twitter = create_twitter_thread(article)  # 10 Tweets, numbered
    carousel = create_carousel(article)       # 10 Slides, visual
    return {'linkedin': linkedin, 'twitter': twitter, 'carousel': carousel}
```

**Build vs Use:**
- âœ… **Build:** Custom Repurposing (Florian's specific platforms)
- âš ï¸ **Use:** Tools wie Buffer/Hootsuite fÃ¼r Scheduling (nicht Creation)

---

**5. Distributor Agent**
- **Was:** Publishing zu Substack, LinkedIn, Twitter
- **Wie:** APIs (Substack [[API]], LinkedIn [[API]], Twitter [[API]])
- **Papers:** Toolformer (#2) fÃ¼r [[API]]-Learning
- **Output:** Published Content + Links

**Build vs Use:**
- âš ï¸ **Use:** Platform APIs (robust)
- âœ… **Build:** Custom Orchestration (wann, was, wohin)

---

**6. Performance Tracker**
- **Was:** Trackt Views, Likes, Shares, Comments
- **Wie:** Platform APIs + Analytics
- **Output:** "Artikel X: 500 Views, 20 Likes â†’ gut" â†’ Feedback Loop

**Build vs Use:**
- âš ï¸ **Use:** [[Google]] Analytics, LinkedIn Analytics (bestehend)
- âœ… **Build:** Custom Aggregation + Feedback-Mapping

---

### 2.4 Hierarchical Memory System

**Papers:** MemGPT (#7), Generative Agents (#8), RAG (#9), Workflow Memory (#10), A-Mem (#11)

#### Memory-Hierarchie (Detail)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: PRINCIPLES (Permanent, Abstract, Rare Updates)        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ "Simplicity > Complexity"                                       â”‚
â”‚ "Ship > Perfect"                                                â”‚
â”‚ "Personal Intelligence > General Intelligence"                 â”‚
â”‚                                                                 â”‚
â”‚ Storage: ~/Obsidian/00-Principles/                             â”‚
â”‚ Size: ~10 notes                                                 â”‚
â”‚ Decay: None (permanent)                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘ Promotion (rare)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: DOMAIN KNOWLEDGE (Structured, Growing)                â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ "Multi-Agent Architectures: AutoGen vs MetaGPT vs LangGraph"   â”‚
â”‚ "VC Fundraising: Best Practices from 50 Fund Decks"            â”‚
â”‚ "Florians Content Voice: Direct, No-Fluff, Practical"          â”‚
â”‚                                                                 â”‚
â”‚ Storage: ~/Obsidian/20-Knowledge/                              â”‚
â”‚ Size: ~300 notes (growing)                                      â”‚
â”‚ Decay: None (curated)                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘ Promotion (weekly)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: EPISODIC (Temporal, Consolidating)                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ "2026-02-10: Read 3 papers on Multi-Agent Systems"             â”‚
â”‚ "2026-02-09: Meeting with VC X, discussed AI Agent thesis"     â”‚
â”‚ "2026-02-08: Published article on ReAct, 200 views"            â”‚
â”‚                                                                 â”‚
â”‚ Storage: ~/Obsidian/01-Daily/ + memory/YYYY-MM-DD.md           â”‚
â”‚ Size: ~1000 notes/year                                          â”‚
â”‚ Decay: Consolidate to L2 after 30 days, delete after 90 days   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘ Promotion (daily)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4: RAW (Unfiltered, High-Volume, Short-Lived)            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Paper PDFs, Twitter Feeds, Email Threads, Code Commits         â”‚
â”‚                                                                 â”‚
â”‚ Storage: ~/FZ/Inbox/, Vector DB (Embeddings)                   â”‚
â”‚ Size: ~10k items/year                                           â”‚
â”‚ Decay: Delete after 7 days if not promoted                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Automatic Promotion (wie menschliches Sleep-Memory)

**Inspiration:** Hippocampus â†’ Neocortex Consolidation im Schlaf

**Process:**
1. **Daily (Raw â†’ Episodic):** Cron Job jeden Abend
   - Scan L4 (Raw Inbox)
   - [[LLM]]: "Was war heute wichtig?"
   - Create Episodic Note in L3
   - Delete uninteresting L4 items

2. **Weekly (Episodic â†’ Semantic):** Cron Job jeden Sonntag
   - Review L3 (letzte 7 Tage)
   - [[LLM]]: "Welche Patterns? Was ist wiederverwendbar?"
   - Update L2 (Domain Knowledge)
   - Archive L3 (nach 30 Tagen)

3. **Monthly (Semantic â†’ Principles):** Manual Review
   - Florian reviewed L2 changes
   - Decide: "Ist das ein neues Principle?"
   - Update L1 (sehr selten)

**Implementation:**
```python
# Daily Consolidation (L4 â†’ L3)
def daily_consolidation():
    raw_items = load_raw_inbox()  # L4
    prompt = f"""
    Florian's Day Review:
    
    Raw Events: {raw_items}
    
    Was war heute wichtig? Erstelle eine Zusammenfassung (200 WÃ¶rter).
    """
    summary = [[LLM]].generate(prompt)
    save_to_episodic(date.today(), summary)  # L3
    cleanup_raw_inbox()  # Delete L4

# Weekly Consolidation (L3 â†’ L2)
def weekly_consolidation():
    episodic_notes = load_episodic_last_7_days()  # L3
    prompt = f"""
    Weekly Review:
    
    {episodic_notes}
    
    - Welche Learnings sind wiederverwendbar?
    - Welche Patterns?
    - Was gehÃ¶rt ins Domain Knowledge?
    
    Output: Updates fÃ¼r L2 (Domain Knowledge)
    """
    updates = [[LLM]].generate(prompt)
    apply_to_domain_knowledge(updates)  # L2
```

**Build vs Use:**
- âœ… **Build:** Custom Consolidation Logic (keine Tools fÃ¼r personalisierte Memory-Hierarchie)
- âš ï¸ **Use:** Vector DB (Pinecone, Weaviate) fÃ¼r L4 Embeddings

---

#### Memory Traversal (Top-Down, Bottom-Up, Lateral)

**Top-Down (Principle â†’ Implementation):**
- Start: "Simplicity > Complexity" (L1)
- Traverse: Welche L2 Knowledge unterstÃ¼tzt das?
- Example: "ReAct Loop ist simpler als LATS" (L2)
- Example: "Florian hat ReAct implementiert, nicht LATS" (L3)

**Bottom-Up (Event â†’ Abstraction):**
- Start: "Paper Ã¼ber Multi-Agent gelesen" (L3)
- Traverse: Passt zu welchem L2 Knowledge?
- Example: "Multi-Agent Architectures" (L2)
- Promote: Update L2 Note mit neuem Insight

**Lateral (Cross-Domain Connections):**
- Connect: "[[VC]] Fundraising Tactics" (L2) + "Content Marketing" (L2)
- Insight: "Fundraising ist Content Marketing fÃ¼r Investoren"
- Save: New L2 Note

**Implementation:**
```python
# Top-Down Traversal
def top_down_query(principle):
    l2_notes = find_related_knowledge(principle)
    l3_notes = find_related_episodes(l2_notes)
    return {'principle': principle, 'knowledge': l2_notes, 'episodes': l3_notes}

# Bottom-Up Traversal
def bottom_up_promotion(event):
    related_knowledge = find_related_l2(event)
    if should_update(related_knowledge, event):
        update_l2(related_knowledge, extract_insight(event))
```

**Build vs Use:**
- âœ… **Build:** Custom Traversal Logic (bestehende Graph DBs sind zu generisch)
- ğŸ¤” **Hybrid:** Nutze Neo4j fÃ¼r Graph Storage, aber custom Traversal-Algorithmen

---

### 2.5 Self-Improvement Loop

**Papers:** Reflexion (#12), Self-Refine (#13), Constitutional [[AI]] (#14), Voyager (#15)

#### Komponenten

**1. Calibrator (Predicted vs Actual)**
- **Was:** Vergleicht Predictions mit Reality
- **Example:** "Scorer predicted Paper X = 9/10, Florian rated it 5/10 â†’ Fehler"
- **Learning:** Update Scoring-Modell

**Implementation:**
```python
def calibrate_scorer(predictions, actuals):
    errors = []
    for pred, actual in zip(predictions, actuals):
        error = actual - pred
        errors.append({'paper': pred.paper, 'error': error, 'features': pred.features})
    
    # Analyze errors
    prompt = f"""
    Scorer Calibration:
    
    Errors: {errors}
    
    Welche Features sind schlecht kalibriert?
    Was muss ich am Scoring-Modell Ã¤ndern?
    """
    improvements = [[LLM]].generate(prompt)
    update_scorer_prompt(improvements)
```

**Build vs Use:**
- âœ… **Build:** Custom Calibration (keine Tools fÃ¼r domain-specific model calibration)

---

**2. Reflector (WÃ¶chentliche Selbstreflexion)**
- **Was:** WÃ¶chentliche Retrospektive: Was lief gut? Was nicht?
- **Papers:** Reflexion (#12), Generative Agents (#8)
- **Output:** Reflection Note in L3

**Implementation:**
```python
def weekly_reflection():
    week_summary = load_episodic_last_7_days()
    prompt = f"""
    Weekly Reflection:
    
    {week_summary}
    
    - Was lief gut?
    - Was lief schlecht?
    - Was habe ich gelernt?
    - Was sollte ich nÃ¤chste Woche anders machen?
    
    Output: 300-Wort-Reflection
    """
    reflection = [[LLM]].generate(prompt)
    save_to_memory(reflection)
```

**Build vs Use:**
- âœ… **Build:** Custom Reflection (einfach, keine Tools nÃ¶tig)

---

**3. Feedback-Loop: Content Performance â†’ Research Priorities**
- **Was:** Welche Artikel performed gut? â†’ Mehr Research in diesem Bereich
- **Example:** "Artikel Ã¼ber Multi-Agent Systems: 500 Views, 50 Likes" â†’ "PrioritÃ¤t: Multi-Agent Papers"

**Implementation:**
```python
def adjust_research_priorities(content_performance):
    top_articles = sorted(content_performance, key=lambda x: x.engagement, reverse=True)[:5]
    topics = extract_topics(top_articles)
    
    prompt = f"""
    Content Performance Analysis:
    
    Top Articles: {top_articles}
    Topics: {topics}
    
    Welche Research-Bereiche sollte ich priorisieren?
    """
    priorities = [[LLM]].generate(prompt)
    update_intake_keywords(priorities)
```

**Build vs Use:**
- âœ… **Build:** Custom Feedback-Loop (keine Tools fÃ¼r Content â†’ Research Mapping)

---

## Teil 3: Technische Implementierung

### 3.1 Tech Stack

| Layer | Technology | Why |
|-------|-----------|-----|
| **Agent Runtime** | OpenClaw | Florians bestehende Infrastruktur |
| **[[LLM]]** | [[Claude]] Sonnet 4.5, Opus 4.5 | Best-in-class reasoning |
| **Memory (L1-L3)** | Obsidian (Markdown + Graph) | Human-readable, versionable, Florians Vault |
| **Memory (L4)** | Vector DB (Pinecone/Weaviate) | Embedding-basierte Retrieval |
| **Tool Integration** | MCP (Model Context Protocol) | Standard fÃ¼r Agent-Tool-Communication |
| **Cron/Scheduling** | OpenClaw Cron | Bestehende Infrastruktur |
| **APIs** | Substack, LinkedIn, Twitter | Distribution |
| **PDF Extraction** | `pdftotext`, PyPDF2 | Robust |
| **Graph DB (optional)** | Neo4j | FÃ¼r komplexe Memory-Traversal |

### 3.2 Ordnerstruktur (Obsidian)

```
~/Library/Mobile Documents/iCloud~md~obsidian/Documents/System_OS/
â”œâ”€â”€ 00-Principles/              # L1: Permanent Principles
â”œâ”€â”€ 10-Projects/                # Active Work
â”‚   â””â”€â”€ Compound-Machine-Sprints.md
â”œâ”€â”€ 20-Knowledge/               # L2: Domain Knowledge
â”‚   â”œâ”€â”€ AI-Agents/
â”‚   â”œâ”€â”€ VC-Fundraising/
â”‚   â”œâ”€â”€ Content-Creation/
â”‚   â””â”€â”€ Operations/
â”œâ”€â”€ 01-Daily/                   # L3: Episodic (YYYY-MM-DD.md)
â”œâ”€â”€ 60-Resources/
â”‚   â””â”€â”€ Knowledge/
â”‚       â””â”€â”€ Compound-Machine-Architecture.md  # This document
â””â”€â”€ 99-Archive/                 # Old L3 after consolidation
```

### 3.3 Ordnerstruktur (Workspace)

```
~/.openclaw/workspace/
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ top-20-agent-papers.md
â”‚   â”œâ”€â”€ compound-machine-architecture.md  # This document
â”‚   â”œâ”€â”€ compound-machine-sprints.md
â”‚   â””â”€â”€ papers/
â”‚       â”œâ”€â”€ inbox/              # L4: Raw PDFs
â”‚       â”œâ”€â”€ scored/             # Scored + Summarized
â”‚       â””â”€â”€ archive/            # Old papers
â”œâ”€â”€ content/
â”‚   â”œâ”€â”€ ideas/                  # Ideator output
â”‚   â”œâ”€â”€ drafts/                 # Writer output
â”‚   â”œâ”€â”€ published/              # Final articles
â”‚   â””â”€â”€ performance/            # Analytics data
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ YYYY-MM-DD.md           # Daily episodic
â”‚   â””â”€â”€ heartbeat-state.json
â””â”€â”€ skills/                     # Custom agents as skills
    â”œâ”€â”€ scorer/
    â”œâ”€â”€ writer/
    â”œâ”€â”€ critic/
    â””â”€â”€ synthesizer/
```

### 3.4 MCP Integration

**Was:** Model Context Protocol (Paper #B, Recent Breakthroughs)

**Warum:** Standardisierte Tool-Integration. Statt fÃ¼r jedes Tool (Obsidian, Notion, GitHub, Substack) custom Code zu schreiben, nutzen wir MCP-Server.

**Implementierung:**
```json
// MCP Config (mcp.json)
{
  "servers": {
    "obsidian": {
      "type": "filesystem",
      "path": "~/Library/Mobile Documents/iCloud~md~obsidian/Documents/System_OS/"
    },
    "notion": {
      "type": "http",
      "url": "https://api.notion.com/v1/",
      "auth": "bearer"
    },
    "substack": {
      "type": "http",
      "url": "https://api.substack.com/",
      "auth": "api_key"
    }
  }
}
```

**Agents nutzen MCP:**
```python
# Writer Agent publishes to Substack via MCP
def publish_to_substack(article):
    mcp_client.call('substack', 'create_post', {
        'title': article.title,
        'body': article.content,
        'publish': True
    })
```

**Build vs Use:**
- âš ï¸ **Use:** MCP Protocol (industry standard)
- âœ… **Build:** Custom MCP Servers fÃ¼r Obsidian (falls nicht existiert)

---

## Teil 4: Build vs Use Entscheidungen (Zusammenfassung)

| Komponente | Entscheidung | BegrÃ¼ndung |
|-----------|-------------|-----------|
| **PDF Extraction** | âš ï¸ Use (`pdftotext`) | Robust, maintained |
| **[[LLM]]** | âš ï¸ Use (Claude [[API]]) | Best-in-class |
| **Vector DB** | âš ï¸ Use (Pinecone/Weaviate) | Standard, skaliert |
| **Graph DB** | ğŸ¤” Optional (Neo4j) | Nur wenn Memory-Traversal komplex wird |
| **MCP Protocol** | âš ï¸ Use | Industry standard |
| **Obsidian Integration** | âœ… Build (custom scripts) | Florians spezifische Vault-Struktur |
| **Scorer Model** | âœ… Build (custom prompts) | Domain-spezifisch, personalisiert |
| **Writer Agent** | âœ… Build (few-shot Florians Voice) | Voice ist unique |
| **Self-Refine Loop** | âœ… Build | Einfach, custom logic |
| **Memory Consolidation** | âœ… Build | Keine Tools fÃ¼r personalisierte Hierarchie |
| **Calibrator** | âœ… Build | Domain-specific |
| **Synthesizer** | âœ… Build | Research-to-Insight ist custom |
| **Cron Jobs** | âš ï¸ Use (OpenClaw Cron) | Bestehende Infrastruktur |
| **Platform APIs** | âš ï¸ Use (Substack, LinkedIn, Twitter) | Standard |

**Grundregel:** Use fÃ¼r Infrastruktur, Build fÃ¼r Personalisierung.

---

## Teil 5: Architektur-Diagramme

### 5.1 Agent-Kommunikation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Intake      â”‚  Daily Cron (ArXiv, Scholar, Twitter)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scorer      â”‚  Score new papers ([[LLM]]-based)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reader      â”‚  PDF â†’ Summary (if score > 7)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Critic      â”‚  Red Team the paper
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Synthesizer  â”‚  Weekly: Find patterns across papers
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ideator     â”‚  Research â†’ Article Ideas
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Writer      â”‚  Idea â†’ Draft (Florians Voice)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Self-Refine  â”‚  Draft â†’ Critique â†’ Revise (loop)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Repurposer   â”‚  Blog â†’ LinkedIn + Twitter + Carousel
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Distributor  â”‚  Publish via APIs
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tracker      â”‚  Monitor Performance
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calibrator   â”‚  Feedback â†’ Adjust Priorities
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“ (loop back to Intake)
```

### 5.2 Memory-Hierarchie mit Feedback-Loops

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INPUT (Florian)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   L4: RAW       â”‚ â† Papers, Tweets, Emails
                    â”‚   (Inbox)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“ (Daily Cron: Consolidation)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  L3: EPISODIC   â”‚ â† "Today I read X, Y, Z"
                    â”‚  (Daily Notes)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“ (Weekly Cron: Pattern Recognition)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ L2: SEMANTIC    â”‚ â† "Multi-Agent > Monolith"
                    â”‚ (Knowledge)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“ (Monthly Manual: Review)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ L1: PRINCIPLES  â”‚ â† "Simplicity > Complexity"
                    â”‚ (Permanent)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   AGENTS USE    â”‚ â† Agents read L1-L4
                    â”‚   ALL LAYERS    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Teil 6: Was ist SCHWER? (Ehrlichkeit)

### Hard Problems

**1. Voice Consistency (Writer Agent)**
- **Problem:** Florians Voice ist subtil â€” "no-fluff, direkt, ehrlich". Schwer zu replizieren.
- **LÃ¶sung:** Viele Few-Shot Examples, iteratives Tuning, Human-Feedback
- **Risk:** Writer klingt generic

**2. Memory Consolidation Quality**
- **Problem:** Welche Raw Items sind wichtig genug fÃ¼r L3? [[LLM]] kann falsch liegen.
- **LÃ¶sung:** Conservative Promotion (lieber zu viel als zu wenig), Human-Review
- **Risk:** Memory-Bloat oder Information-Loss

**3. Scorer Calibration**
- **Problem:** Florians Relevanz-Score ist subjektiv und zeitabhÃ¤ngig.
- **LÃ¶sung:** Continuous Calibration mit Feedback, Acceptance: Score ist Heuristik, nicht Ground Truth
- **Risk:** Scorer driftet, verpasst wichtige Papers

**4. Self-Improvement Convergence**
- **Problem:** Self-Improvement Loops kÃ¶nnen divergieren (zu aggressiv oder zu konservativ).
- **LÃ¶sung:** Constraints (max 10% change per iteration), Human-Checkpoints
- **Risk:** System wird instabil

**5. Tool Reliability (APIs)**
- **Problem:** Substack [[API]] down, LinkedIn limitiert requests â†’ Distribution fails
- **LÃ¶sung:** Retry-Logic, Fallbacks, Notifications to Florian
- **Risk:** Silent failures

---

## Teil 7: Success Metrics

**Nach 1 Monat:**
- âœ… Research Intake lÃ¤uft tÃ¤glich (10+ Papers/Woche scored)
- âœ… Memory L4 â†’ L3 Consolidation funktioniert (Daily Notes generiert)
- âœ… Erster Auto-Generated Research Brief (Weekly Synthesis)

**Nach 3 Monaten:**
- âœ… Content Pipeline LIVE (1 Artikel/Woche, Research â†’ Draft â†’ Publish)
- âœ… Memory L3 â†’ L2 Consolidation funktioniert (Domain Knowledge wÃ¤chst)
- âœ… Scorer ist kalibriert (Predicted vs Actual < 2 Punkte Differenz)

**Nach 6 Monaten:**
- âœ… Full Loop: Research â†’ Content â†’ Distribution â†’ Feedback â†’ Research
- âœ… Self-Improvement messbar (Scorer accuracy +20%, Writer quality +30%)
- âœ… Florian spart 10h/Woche durch Automation

**Nach 12 Monaten:**
- âœ… Compound Machine hat 500+ L2 Notes (Florians Personal [[AI]] Moat)
- âœ… Content Performance: 5000+ Views/Monat, 100+ Subscribers
- âœ… Florian's "Second Brain" ist operational

---

## Zusammenfassung

**Was wir bauen:** Personal Intelligence Compound Machine â€” ein System, das mit jedem Tag smarter wird, tief personalisiert auf Florian, domain-spezifisch ([[VC]], [[AI]], Content), mit hierarchischem Memory und Self-Improvement.

**Warum wir gewinnen:** Meta/[[Google]] bauen General Intelligence (fÃ¼r alle, fÃ¼r niemanden optimal). Wir bauen Personal Intelligence (fÃ¼r einen, perfekt optimiert). Unser Moat: Deep Context, Compound Learning, Domain Expertise.

**Wie wir bauen:** OpenClaw + Obsidian + [[Claude]] + MCP + Custom Agents. Build fÃ¼r Personalisierung, Use fÃ¼r Infrastruktur.

**Was schwer wird:** Voice Consistency, Memory Quality, Scorer Calibration, Self-Improvement Stability. Wir gehen ehrlich damit um.

**Wann wir gewinnen:** Nach 6 Monaten Compound Learning ist das System besser als ChatGPT fÃ¼r Florians Use Cases. Nach 12 Monaten ist es unschlagbar.

---

**Let's build.**

---

*Florian, das ist der Blueprint. Jetzt Sprint 1 starten.*
