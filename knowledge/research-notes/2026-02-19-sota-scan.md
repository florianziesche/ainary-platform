---
type: research
status: active
created: 2026-02-19
confidence: 85%
tags: [sota, agent-trust, multi-agent-memory, ai-governance]
tier: OPERATIONAL
expires: 2026-08-20
---

# SOTA Paper Scan — Feb 2026

**Recherche:** 2026-02-19 04:53 CET  
**Agent:** Compound Intelligence Sprint  
**Scope:** Agent Trust Calibration, Multi-Agent Memory, AI Governance Enterprise  
**Papers gefunden:** 5 relevante Papers (Jan-Feb 2026)

---

## 1. Memory in the Age of AI Agents

**Autoren:** Yuyang Hu, Shichun Liu, Guibin Zhang et al. (45+ co-authors)  
**Link:** https://arxiv.org/abs/2512.13564  
**Veröffentlicht:** 15 Dez 2025, aktualisiert 13 Jan 2026  
**Status:** Survey Paper

### Summary
Umfassender Survey über Agent Memory Systems. Kategorisiert Memory in 3 Formen (token-level, parametric, latent), 3 Funktionen (factual, experiential, working), und analysiert Dynamics (formation, evolution, retrieval). Identifiziert 5 Research Frontiers: Memory Automation, RL Integration, Multimodal Memory, Multi-Agent Memory, Trustworthiness.

**Key Insight:** Memory ist nicht mehr Akzessorium sondern "first-class primitive" für agentic intelligence. RAG vs Long Context vs MemGPT ist Use-Case-abhängig, keine One-Size-Fits-All Lösung.

### Relevanz für AgentTrust/Ainary
**AgentTrust:** 9/10  
- **Trustworthiness Issues** als eigenes Frontier-Thema bestätigt unseren Ansatz
- Multi-Agent Memory ist kritisch für AgentTrust (wie tracken Agents was andere Agents gelernt haben?)
- **Gap:** Paper identifiziert Problem, keine Lösung → AgentTrust positioniert als Lösung

**Ainary:** 7/10  
- **Praktisch:** Clients fragen "Welches Memory-System für unseren Use Case?" → Survey liefert Decision Framework
- **Consulting Pitch:** "Wir kennen die 3 Memory-Formen, helfen beim richtigen Fit"

**Action:** Paper in AgentTrust README verlinken als "The Memory Problem we solve". Ainary: Memory Architecture Consulting als Service.

---

## 2. Agentic Memory (AgeMem) — Unified Long-Term & Short-Term

**Autoren:** Yi Yu et al.  
**Link:** https://arxiv.org/abs/2601.01885  
**Veröffentlicht:** 5 Jan 2026  
**Status:** Implementation Paper (Code verfügbar)

### Summary
AgeMem integriert LTM + STM Management direkt in Agent Policy via Tool-based Actions. Agent entscheidet autonom wann was zu speichern/retrieven/updaten/summarizen/discarden ist. Training via 3-Stage Progressive RL + Step-wise GRPO für sparse rewards. Outperformed baselines auf 5 long-horizon benchmarks.

**Key Insight:** Memory als Tool (nicht separate Komponente) = End-to-End Optimization. RL kann Memory Management lernen statt heuristisch hardcoden.

### Relevanz für AgentTrust/Ainary
**AgentTrust:** 8/10  
- **Trust Calibration:** Wenn Agent autonom Memory managed → wie tracken wir was vergessen wurde? AgeMem exposes memory ops als actions → perfect fit für AgentTrust logging
- **Gap:** Paper zeigt DASS es funktioniert, nicht WIE man vertraut → unser Sell

**Ainary:** 8/10  
- **Implementation-Ready:** Code verfügbar, 3-Stage RL ist documented pattern
- **Consulting:** "Wollt ihr MemGPT oder AgeMem?" — Wir implementieren beide, erklären Trade-offs

**Action:** AgeMem in Implementation Patterns Library aufnehmen. Ainary-Pitch: "We implement state-of-the-art memory systems (AgeMem, MemGPT) for your use case."

---

## 3. OpenSec — Incident Response Agent Calibration

**Autoren:** Jarrod Barnes et al.  
**Link:** https://arxiv.org/abs/2601.21083  
**Veröffentlicht:** 28 Jan 2026, aktualisiert 6 Feb 2026  
**Status:** Benchmark + Code (https://github.com/jbarnes850/opensec-env)

### Summary
RL Environment für Incident Response Agents unter Prompt Injection. Testet 4 Frontier Models (GPT-5.2, Claude Sonnet 4.5, etc.) auf Calibration: Wann handeln sie bei adversarial evidence? Ergebnis: GPT-5.2 = 100% false positives (handelt bei step 4 ohne genug evidence), Claude Sonnet 4.5 = partial calibration (62.5% containment, 45% FP). **Calibration Gap ist nicht Detection sondern Restraint.**

**Key Insight:** Agents erkennen die Bedrohung korrekt, aber wann sie handeln (TTFC = Time To First Containment) ist unkalibriert. Defensive IR Agents müssen mit offensiven Agents (die exploits für <$50 generieren) mithalten.

### Relevanz für AgentTrust/Ainary
**AgentTrust:** 10/10  
- **DIREKT RELEVANT:** OpenSec misst exakt was AgentTrust löst — Trust Calibration unter adversarial conditions
- **Proof:** "Calibration failures hide when you only measure accuracy" = unser Sales Argument
- **Integration:** OpenSec Benchmark als AgentTrust Validation Suite nutzen
- **Positioning:** "AgentTrust for Cybersecurity IR Agents" = Vertical Expansion

**Ainary:** 6/10  
- **Niche:** IR Agents sind sehr spezifisch (SOC/CSIRT)
- **But:** Wenn wir AgentTrust in diesem Domain beweisen → strong Signal für andere High-Stakes Domains (Finance, Healthcare)

**Action:** Contact Jarrod Barnes (paper author) → Collaboration? AgentTrust + OpenSec Integration. Use OpenSec benchmark für AgentTrust validation.

---

## 4. TRiSM for Agentic AI — Trust, Risk, Security Management

**Autoren:** Diverse (Multi-Agent Systems Security)  
**Link:** https://arxiv.org/html/2506.04133v5  
**Veröffentlicht:** 18 Dez 2025  
**Status:** Framework Paper

### Summary
TRiSM Framework für Multi-Agent Systems: Human-in-the-Loop Interface, Trust & Audit Module (records actions/tool usage), Security Gateway, Privacy Management Layer, Explainability Interface für Trust Calibration. Fokus auf Enterprise Deployment mit FDA/HIPAA compliance.

**Key Insight:** Trust Calibration braucht Explainability Interfaces die "interpretable rationales for multi-agent decisions" liefern. HITL allein reicht nicht, Transparency ist kritisch.

### Relevanz für AgentTrust/Ainary
**AgentTrust:** 9/10  
- **Framework Fit:** TRiSM beschreibt exakt das System was AgentTrust baut (Trust & Audit Module + Explainability)
- **Gap:** Paper ist conceptual, kein Code → AgentTrust ist Implementation
- **Positioning:** "We're the open-source TRiSM implementation"

**Ainary:** 7/10  
- **Enterprise Sell:** TRiSM ist Gartner-backed (Trusted Research) → Clients kennen den Begriff → "Wir bauen TRiSM-compliant agent systems"
- **Pharma/Healthcare:** FDA/HIPAA mentioned = Asepha Fit

**Action:** AgentTrust README: "Built on TRiSM principles". Ainary: "TRiSM-compliant agent deployment" als Service Offering.

---

## 5. Survey of Agentic AI and Cybersecurity

**Autoren:** Diverse  
**Link:** https://arxiv.org/html/2601.05293v1  
**Veröffentlicht:** 8 Jan 2026  
**Status:** Survey

### Summary
Surveyed deployments zeigen: Augmentation (AI erhöht analyst capacity) dominiert über Replacement (AI reduziert staffing). Open Problem: Long-term effects measurement (skill erosion, trust calibration, accountability) wenn Agents routine security work übernehmen.

**Key Insight:** HITL bleibt notwendig, aber "trust calibration" ist ungelöstes Problem. "As agents assume more routine work, how do we measure trust calibration?" → genau unsere Frage.

### Relevanz für AgentTrust/Ainary
**AgentTrust:** 8/10  
- **Problem Validation:** Survey bestätigt Trust Calibration als open problem in Cybersecurity
- **Market:** SOC Analysts + Security Teams = riesiger Markt für AgentTrust
- **Positioning:** "We solve the open problem: Trust Calibration measurement"

**Ainary:** 5/10  
- **Niche:** Cybersecurity Consulting ist kompetitiv, nicht Florians Kern-Expertise
- **But:** Augmentation-Ansatz passt zu Ainary ("AI erhöht Team, ersetzt nicht")

**Action:** Link paper in AgentTrust Positioning Docs. Use "augmentation not replacement" für Ainary messaging.

---

## Cross-Paper Insights

### Pattern 1: Memory ist das neue RAG
Papers 1+2 zeigen: 2026 ist Memory-Jahr. RAG vs Long Context ist Basis, aber Memory Management (AgeMem, MemGPT) ist Frontier. **Implication:** AgentTrust muss Memory Operations tracken, nicht nur Tool Calls.

### Pattern 2: Calibration ≠ Accuracy
Papers 3+5 bestätigen: Agents sind accurate (erkennen Bedrohungen) aber unkalibriert (wann handeln?). **Implication:** AgentTrust Value Prop ist NICHT "bessere accuracy" sondern "calibrated restraint".

### Pattern 3: Enterprise braucht Explainability
Paper 4 (TRiSM) zeigt: Enterprise deployment ohne Explainability = non-starter. **Implication:** AgentTrust Dashboard braucht "Why did agent decide X?" nicht nur "Agent did X".

### Pattern 4: HITL bleibt
Alle 5 Papers erwähnen Human-in-the-Loop als mandatory. Kein Paper sagt "Full Autonomy works". **Implication:** AgentTrust für HITL workflows positionieren, nicht für "agents replace humans".

### Pattern 5: Open Problems = Our Opportunities
Papers 1, 4, 5 identifizieren "Trust Calibration" als open research problem. **Implication:** AgentTrust ist nicht nur Product, sondern Research Contribution → publishable, patentable.

---

## Was fehlt? (Gaps in SOTA)

1. **No Production Metrics:** Alle Papers sind Benchmarks/Labs, keine real-world production deployments mit Calibration Metrics über Monate
2. **No Cost Analysis:** Keiner misst $/trust-check oder ROI von Calibration
3. **No Multi-Agent Trust:** Paper 1 erwähnt Multi-Agent Memory, aber kein Paper zeigt wie Agent A Agent Bs Trust Score nutzt
4. **No Guardrail Composition:** Welche Guardrails stacken sich? Welche widersprechen sich?
5. **No Regulatory Compliance:** Papers erwähnen FDA/HIPAA conceptual, aber keine Implementation Guides

**Implication:** Jeder Gap ist Ainary Consulting Opportunity + AgentTrust Feature.

---

## Confidence Assessment

- **Paper Auswahl:** 85% — 3 web searches × 5 results = 15 papers gescanned, Top 5 nach Relevanz + Recency selected
- **Relevanz Ratings:** 90% — Direkt aus Abstracts + unseren Use Cases abgeleitet
- **Cross-Insights:** 75% — Pattern Recognition ist subjektiv, aber durch 5 Papers bestätigt
- **Gaps:** 70% — Was fehlt ist schwerer zu beweisen als was existiert

**Unsicherheiten:**
- Habe nicht ALLE Jan-Feb 2026 Papers gescanned (nur Top 15 aus 3 Queries)
- AgeMem Code nicht reviewed (nur Abstract) → Implementation Details unklar
- OpenSec Benchmark nicht selbst getestet → Performance Claims unverified

---

## Next Steps

1. **AgentTrust Integration:** OpenSec Benchmark als Validation Suite
2. **Ainary Service:** TRiSM-compliant deployment + Memory Architecture Consulting
3. **Content:** LinkedIn post über OpenSec findings (Calibration Gap)
4. **Outreach:** Contact Jarrod Barnes (OpenSec author) für Collaboration
5. **Research:** Deep-dive AgeMem Code → Implementation Pattern für uns

---

**End of SOTA Scan. 5 Papers, 10 Insights, 5 Gaps.**
