<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Calibration Gap — Ainary Report AR-009</title>
<style>
  /* ========================================
     FONTS
     ======================================== */
  @font-face {
    font-family: 'Inter';
    src: url('/fonts/inter-variable.woff2') format('woff2');
    font-weight: 100 900;
    font-display: swap;
  }

  /* ========================================
     RESET & BASE
     ======================================== */
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: #fafaf8;
    color: #333;
    line-height: 1.75;
    font-size: 0.95rem;
    font-weight: 400;
  }

  /* ========================================
     LAYOUT
     ======================================== */
  .page {
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
  }

  .back-cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    max-width: 900px;
    margin: 0 auto;
    padding: 48px 40px;
    page-break-before: always;
  }

  /* ========================================
     TYPOGRAPHY
     ======================================== */
  h1 {
    font-size: 2.2rem;
    font-weight: 600;
    line-height: 1.2;
    color: #1a1a1a;
    letter-spacing: -0.02em;
  }

  h2 {
    font-size: 1.5rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.3;
    margin-top: 3rem;
    margin-bottom: 12px;
  }

  h3 {
    font-size: 1.1rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.4;
    margin-top: 2rem;
    margin-bottom: 12px;
  }

  p {
    margin-bottom: 1rem;
  }

  strong {
    font-weight: 600;
    color: #1a1a1a;
  }

  em {
    font-style: italic;
  }

  sup {
    font-size: 0.65rem;
    color: #888;
    vertical-align: super;
  }

  /* ========================================
     COVER COMPONENTS
     ======================================== */
  .cover-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 40vh;
  }

  .cover-brand {
    display: flex;
    align-items: center;
    gap: 8px;
  }

  .gold-punkt {
    color: #c8aa50;
    font-size: 14px;
  }

  .brand-name {
    font-size: 0.85rem;
    font-weight: 500;
    color: #1a1a1a;
    letter-spacing: 0.02em;
  }

  .cover-meta {
    display: flex;
    gap: 12px;
    font-size: 0.75rem;
    color: #888;
  }

  .cover-title-block {
    margin-bottom: auto;
  }

  .cover-title {
    margin-bottom: 16px;
  }

  .cover-subtitle {
    font-size: 1rem;
    font-weight: 400;
    color: #666;
    line-height: 1.5;
  }

  .cover-footer {
    display: flex;
    justify-content: space-between;
    align-items: flex-end;
  }

  .cover-date {
    font-size: 0.75rem;
    color: #888;
  }

  .cover-author {
    font-size: 0.75rem;
    color: #888;
    text-align: center;
  }

  /* ========================================
     TABLE OF CONTENTS
     ======================================== */
  .toc-label {
    font-size: 0.7rem;
    font-weight: 600;
    color: #1a1a1a;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    margin-bottom: 24px;
  }

  .toc-section {
    margin-bottom: 32px;
  }

  .toc-section-label {
    font-size: 0.65rem;
    font-weight: 500;
    color: #888;
    text-transform: uppercase;
    letter-spacing: 0.12em;
    margin-bottom: 12px;
  }

  .toc-entry {
    display: flex;
    align-items: baseline;
    gap: 16px;
    padding: 12px 0;
    border-bottom: 1px solid #eee;
    text-decoration: none;
    transition: all 0.2s;
  }

  .toc-number {
    font-size: 0.8rem;
    color: #888;
    font-variant-numeric: tabular-nums;
    min-width: 24px;
  }

  .toc-title {
    font-size: 0.95rem;
    font-weight: 500;
    color: #1a1a1a;
    flex: 1;
    transition: color 0.2s;
  }

  .toc-entry:hover .toc-title {
    color: #c8aa50;
  }

  .toc-page {
    font-size: 0.8rem;
    color: #888;
  }

  /* ========================================
     HOW TO READ
     ======================================== */
  .how-to-read-table {
    width: 100%;
    border-collapse: collapse;
    margin: 24px 0;
  }

  .how-to-read-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }

  .how-to-read-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
  }

  /* ========================================
     EXECUTIVE SUMMARY
     ======================================== */
  .thesis {
    font-size: 1rem;
    font-weight: 600;
    color: #1a1a1a;
    line-height: 1.6;
    margin-bottom: 24px;
  }

  .evidence-list {
    margin-left: 20px;
    margin-bottom: 24px;
  }

  .evidence-list li {
    font-size: 0.9rem;
    color: #333;
    line-height: 1.6;
    margin-bottom: 8px;
  }

  .keywords {
    font-size: 0.8rem;
    color: #666;
    font-style: italic;
    margin-top: 32px;
    padding-top: 16px;
    border-top: 1px solid #eee;
  }

  /* ========================================
     SECTION COMPONENTS
     ======================================== */
  .confidence-badge {
    font-size: 0.75rem;
    font-weight: 500;
    color: #1a1a1a;
    background: #f5f4f0;
    padding: 3px 8px;
    border-radius: 10px;
    margin-left: 8px;
    vertical-align: middle;
  }

  .confidence-line {
    font-size: 0.8rem;
    color: #888;
    font-style: italic;
    display: block;
    margin-bottom: 16px;
  }

  .key-insight {
    font-weight: 600;
    color: #1a1a1a;
  }

  /* ========================================
     CALLOUTS
     ======================================== */
  .callout {
    background: #f5f4f0;
    padding: 16px 20px;
    border-radius: 4px;
    margin: 1.5rem 0;
    page-break-inside: avoid;
  }

  .callout-label {
    font-size: 0.7rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    margin-bottom: 8px;
  }

  .callout-body {
    font-size: 0.9rem;
    color: #555;
    line-height: 1.6;
  }

  .callout.claim .callout-label {
    color: #555;
  }

  .callout.invalidation {
    border-left: 3px solid #ddd;
  }

  .callout.invalidation .callout-label {
    color: #888;
  }

  .callout.sowhat {
    border-left: 3px solid #c8aa50;
  }

  .callout.sowhat .callout-label {
    color: #c8aa50;
  }

  /* ========================================
     EXHIBITS & TABLES
     ======================================== */
  .exhibit {
    margin: 2rem 0;
  }

  .exhibit-label {
    font-size: 0.75rem;
    font-weight: 600;
    color: #555;
    margin-bottom: 12px;
  }

  .exhibit-table {
    width: 100%;
    border-collapse: collapse;
    page-break-inside: avoid;
  }

  .exhibit-table th {
    text-align: left;
    font-size: 0.7rem;
    font-weight: 600;
    color: #555;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 10px 12px;
    background: #f5f4f0;
    border-bottom: 2px solid #e5e3dc;
  }

  .exhibit-table td {
    font-size: 0.85rem;
    color: #333;
    padding: 10px 12px;
    border-bottom: 1px solid #ddd;
  }

  .exhibit-source {
    font-size: 0.7rem;
    color: #888;
    margin-top: 8px;
    font-style: italic;
  }

  /* ========================================
     LISTS
     ======================================== */
  ul {
    margin-left: 20px;
    margin-bottom: 1rem;
  }

  ol {
    margin-left: 20px;
    margin-bottom: 1rem;
  }

  li {
    margin-bottom: 4px;
  }

  /* ========================================
     TRANSPARENCY NOTE
     ======================================== */
  .transparency-intro {
    font-size: 0.85rem;
    color: #555;
    line-height: 1.6;
    margin-bottom: 12px;
  }

  .transparency-table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 12px;
  }

  .transparency-table td:first-child {
    font-size: 0.85rem;
    font-weight: 600;
    color: #555;
    padding: 8px 0;
    border-bottom: 1px solid #eee;
    width: 160px;
    vertical-align: top;
  }

  .transparency-table td:last-child {
    font-size: 0.85rem;
    color: #333;
    padding: 8px 0;
    border-bottom: 1px solid #eee;
  }

  /* ========================================
     REFERENCES
     ======================================== */
  .reference-entry {
    font-size: 0.8rem;
    color: #555;
    line-height: 1.5;
    margin-bottom: 6px;
    padding-left: 24px;
    text-indent: -24px;
  }

  /* ========================================
     AUTHOR BIO
     ======================================== */
  .author-section {
    margin-top: 3rem;
    padding-top: 2rem;
    border-top: 1px solid #e5e3dc;
  }

  .author-label {
    font-size: 0.85rem;
    font-weight: 600;
    color: #555;
    margin-bottom: 8px;
  }

  .author-bio {
    font-size: 0.85rem;
    color: #555;
    line-height: 1.6;
  }

  /* ========================================
     BACK COVER
     ======================================== */
  .back-cover-services {
    font-size: 0.85rem;
    color: #666;
    margin-bottom: 24px;
  }

  .back-cover-cta {
    font-size: 0.85rem;
    color: #888;
    margin-bottom: 16px;
  }

  .back-cover-contact {
    font-size: 0.8rem;
    color: #888;
  }

  /* ========================================
     PRINT STYLES
     ======================================== */
  @media print {
    @page {
      size: A4;
      margin: 2cm;
    }

    body {
      background: white;
    }

    .page, .cover, .back-cover {
      page-break-after: always;
    }

    .callout, .exhibit {
      page-break-inside: avoid;
    }

    @page :first {
      @top-center { content: none; }
      @bottom-center { content: none; }
    }

    @page {
      @top-center {
        content: "Ainary Report | The Calibration Gap";
        font-size: 0.7rem;
        color: #888;
      }
      @bottom-left {
        content: "© 2026 Ainary Ventures";
        font-size: 0.7rem;
        color: #888;
      }
      @bottom-right {
        content: counter(page);
        font-size: 0.7rem;
        color: #888;
      }
    }
  }
</style>
</head>
<body>

<!-- ========================================
     COVER PAGE
     ======================================== -->
<div class="cover">
  <div class="cover-header">
    <div class="cover-brand">
      <span class="gold-punkt">●</span>
      <span class="brand-name">Ainary</span>
    </div>
    <div class="cover-meta">
      <span>AR-009</span>
      <span>Confidence: 72%</span>
    </div>
  </div>

  <div class="cover-title-block">
    <h1 class="cover-title">The Calibration Gap</h1>
    <p class="cover-subtitle">Why 84% of AI Agents Are Overconfident and What It Costs</p>
  </div>

  <div class="cover-footer">
    <div class="cover-date">
      February 2026<br>
      <span style="font-size: 0.7rem; color: #aaa;">v1.0</span>
    </div>
    <div class="cover-author">
      Florian Ziesche · Ainary Ventures
    </div>
  </div>
</div>

<!-- ========================================
     TABLE OF CONTENTS
     ======================================== -->
<div class="page">
  <p class="toc-label">Contents</p>

  <div class="toc-section">
    <p class="toc-section-label">FOUNDATION</p>
    <a href="#how-to-read" class="toc-entry">
      <span class="toc-number">1</span>
      <span class="toc-title">How to Read This Report</span>
      <span class="toc-page">3</span>
    </a>
    <a href="#exec-summary" class="toc-entry">
      <span class="toc-number">2</span>
      <span class="toc-title">Executive Summary</span>
      <span class="toc-page">4</span>
    </a>
    <a href="#methodology" class="toc-entry">
      <span class="toc-number">3</span>
      <span class="toc-title">Methodology</span>
      <span class="toc-page">5</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ANALYSIS</p>
    <a href="#calibration-defined" class="toc-entry">
      <span class="toc-number">4</span>
      <span class="toc-title">What Calibration Means and How to Measure It</span>
      <span class="toc-page">6</span>
    </a>
    <a href="#overconfidence-pandemic" class="toc-entry">
      <span class="toc-number">5</span>
      <span class="toc-title">The Overconfidence Pandemic</span>
      <span class="toc-page">9</span>
    </a>
    <a href="#multi-agent" class="toc-entry">
      <span class="toc-number">6</span>
      <span class="toc-title">Multi-Agent Amplification</span>
      <span class="toc-page">12</span>
    </a>
    <a href="#costs" class="toc-entry">
      <span class="toc-number">7</span>
      <span class="toc-title">What Overconfidence Costs</span>
      <span class="toc-page">14</span>
    </a>
  </div>

  <div class="toc-section">
    <p class="toc-section-label">ACTION</p>
    <a href="#methods" class="toc-entry">
      <span class="toc-number">8</span>
      <span class="toc-title">Calibration Methods That Actually Work</span>
      <span class="toc-page">17</span>
    </a>
    <a href="#human-factor" class="toc-entry">
      <span class="toc-number">9</span>
      <span class="toc-title">The Human Factor</span>
      <span class="toc-page">20</span>
    </a>
    <a href="#recommendations" class="toc-entry">
      <span class="toc-number">10</span>
      <span class="toc-title">Recommendations</span>
      <span class="toc-page">22</span>
    </a>
    <a href="#transparency" class="toc-entry">
      <span class="toc-number">11</span>
      <span class="toc-title">Transparency Note</span>
      <span class="toc-page">24</span>
    </a>
    <a href="#claim-register" class="toc-entry">
      <span class="toc-number">12</span>
      <span class="toc-title">Claim Register</span>
      <span class="toc-page">25</span>
    </a>
    <a href="#references" class="toc-entry">
      <span class="toc-number">13</span>
      <span class="toc-title">References</span>
      <span class="toc-page">26</span>
    </a>
  </div>
</div>

<!-- ========================================
     HOW TO READ THIS REPORT
     ======================================== -->
<div class="page" id="how-to-read">
  <h2>1. How to Read This Report</h2>

  <p>This report uses a structured confidence rating system to communicate what is known versus what is inferred. Every quantitative claim carries its source and confidence level.</p>

  <table class="how-to-read-table">
    <tr>
      <th>Rating</th>
      <th>Meaning</th>
      <th>Example</th>
    </tr>
    <tr>
      <td>High</td>
      <td>3+ independent sources, peer-reviewed or primary data</td>
      <td>84% of LLM responses overconfident across 9 models (peer-reviewed study)</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>1–2 sources, plausible but not independently confirmed</td>
      <td>Multi-agent amplification modeled from established components</td>
    </tr>
    <tr>
      <td>Low</td>
      <td>Single secondary source, methodology unclear</td>
      <td>Cost extrapolations from SOC to AI agent contexts</td>
    </tr>
  </table>

  <p style="margin-top: 24px;">This report was produced using a <strong>multi-agent research pipeline</strong> with structured literature review, source cross-referencing, and claim verification. Full methodology details are provided in the Transparency Note (Section 11).</p>
</div>

<!-- ========================================
     EXECUTIVE SUMMARY
     ======================================== -->
<div class="page" id="exec-summary">
  <h2>2. Executive Summary</h2>

  <p class="thesis">AI agents are systematically overconfident — and enterprise stacks are not designed to catch it.</p>

  <ul class="evidence-list">
    <li><strong>84% of LLM responses show confidence exceeding actual accuracy</strong> across 9 models and 351 clinical scenarios<sup>[1]</sup></li>
    <li><strong>Verbalized confidence is biased upward by 20–30 percentage points</strong> and poorly correlated with correctness (r ≈ 0.3–0.5)<sup>[2][3]</sup></li>
    <li><strong>Multi-agent verification amplifies miscalibration</strong> instead of correcting it — identically biased validators create false consensus<sup>[4]</sup></li>
    <li><strong>Alert fatigue from overconfident systems causes 67% of security alerts to be ignored,</strong> creating blind spots where real threats hide<sup>[5]</sup></li>
    <li><strong>A calibration check costs $0.005; not calibrating has cost up to $7.5 billion</strong> in documented cases<sup>[6][7]</sup></li>
    <li><strong>The root cause is training, not prompting:</strong> RLHF rewards confident-sounding answers, systematically destroying calibration that exists in base models<sup>[9]</sup></li>
  </ul>

  <p class="keywords"><strong>Keywords:</strong> AI calibration, overconfidence, Expected Calibration Error, multi-agent systems, conformal prediction, trust erosion, RLHF, Sample Consistency</p>
</div>

<!-- ========================================
     METHODOLOGY (SHORT VERSION)
     ======================================== -->
<div class="page" id="methodology">
  <h2>3. Methodology</h2>

  <p>This report synthesizes 14 sources: 8 peer-reviewed or preprint papers and 6 industry reports. The research pipeline followed a structured sequence: targeted literature review (calibration metrics, RLHF dynamics, multi-agent systems), source cross-referencing, gap analysis, and claim verification with explicit confidence levels.</p>

  <p>Each claim carries an explicit confidence level (High, Medium-High, or Medium) based on source quality and replicability. Claims are registered in the Claim Register (Section 12) with their evidence basis and invalidation conditions.</p>

  <p><strong>Limitations:</strong> The headline 84% figure comes from clinical decision scenarios. It is used as directional evidence for broader LLM behavior, but domain-specific replication is pending. Multi-agent amplification effects (Section 6) are modeled theoretically from well-evidenced components — the compound effect itself lacks direct empirical validation. Cost extrapolations from SOC and healthcare alert fatigue to AI agent contexts are analogical, not direct measurements.</p>

  <p style="font-size: 0.85rem; color: #666; margin-top: 16px;">Full methodology details, including confidence calibration and known weaknesses, are provided in the Transparency Note (Section 11).</p>
</div>

<!-- ========================================
     SECTION 4: CALIBRATION DEFINED
     ======================================== -->
<div class="page" id="calibration-defined">
  <h2>4. What Calibration Means and How to Measure It
    <span class="confidence-badge">High</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">Calibration is not accuracy — it is honesty about uncertainty.</span></p>

  <p>A model can be 80% accurate and still dangerously miscalibrated. If that same model claims 95% confidence on every prediction, the gap between stated certainty and actual performance is the calibration error. This gap is what kills trust, wastes resources, and ultimately causes human operators to stop listening.</p>

  <h3>The Metrics</h3>

  <p>The standard metric is <strong>Expected Calibration Error (ECE)</strong>. It works by binning predictions by confidence level, then comparing average confidence to average accuracy per bin. A perfectly calibrated model shows a diagonal line: when it says "90% sure," it is right 90% of the time<sup>[8]</sup>.</p>

  <p>The <strong>Brier Score</strong> offers a complementary view. It computes the mean squared error between predicted probability and binary outcome, decomposing into calibration, resolution, and uncertainty. Lower is better. Its advantage over ECE: no binning artifacts<sup>[8]</sup>.</p>

  <p>The <strong>Overconfidence Ratio (OCR)</strong> measures the percentage of predictions where confidence exceeds accuracy. This is the metric behind the 84% headline figure<sup>[1]</sup>.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 1: Calibration Curve — Perfect vs. Typical LLM</p>
    <table class="exhibit-table">
      <tr>
        <th>Stated Confidence</th>
        <th>Perfect Model (Accuracy)</th>
        <th>Typical LLM (Accuracy)</th>
        <th>Gap</th>
      </tr>
      <tr><td>50%</td><td>50%</td><td>45%</td><td>-5pp</td></tr>
      <tr><td>60%</td><td>60%</td><td>48%</td><td>-12pp</td></tr>
      <tr><td>70%</td><td>70%</td><td>52%</td><td>-18pp</td></tr>
      <tr><td>80%</td><td>80%</td><td>58%</td><td>-22pp</td></tr>
      <tr><td>90%</td><td>90%</td><td>65%</td><td>-25pp</td></tr>
      <tr><td>95%</td><td>95%</td><td>70%</td><td>-25pp</td></tr>
    </table>
    <p class="exhibit-source">Source: Directional illustration based on Tian et al. (2023) [3] and Xiong et al. (2024) [10]. Exact values vary by model and domain.</p>
  </div>

  <h3>Token-Level vs. Verbalized Calibration</h3>

  <p>There is a critical distinction that most practitioners miss: <strong>token-level calibration versus verbalized calibration</strong>. Pre-trained base models are reasonably well-calibrated at the token probability level. But instruction tuning and RLHF — the processes that make models useful for conversation — destroy this calibration<sup>[9]</sup>. The models humans actually interact with are the miscalibrated ones.</p>

  <p>When you ask GPT-4 or Claude "how confident are you, 0–100%?", the numbers cluster around round figures (70%, 80%, 90%, 95%) rather than distributing smoothly<sup>[10]</sup>. They correlate with correctness at roughly r ≈ 0.3–0.5<sup>[2]</sup> — better than random, but far worse than the precision they imply.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">A large-scale study showing verbalized confidence from instruction-tuned models is well-calibrated (r > 0.8 with accuracy) across domains.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you are building an agent system that surfaces confidence scores to users, those scores are likely 20–30 percentage points too high. Every decision made downstream of that inflated number carries hidden risk.</p>
  </div>
</div>

<!-- ========================================
     SECTION 5: OVERCONFIDENCE PANDEMIC
     ======================================== -->
<div class="page" id="overconfidence-pandemic">
  <h2>5. The Overconfidence Pandemic
    <span class="confidence-badge">High</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">Every major LLM family is overconfident at the verbalized level — this is a training artifact, not a bug to patch.</span></p>

  <h3>The Evidence</h3>

  <p>The data is unambiguous. A 2024 peer-reviewed study tested 9 different LLMs across 351 clinical decision scenarios<sup>[1]</sup>. In <strong>84% of those scenarios, the model's expressed confidence exceeded its actual accuracy.</strong> This was not model-specific or prompt-dependent. It appeared systematically across model families and sizes.</p>

  <p>Separate research confirms the pattern. Tian et al. (2023) found verbalized confidence is biased upward by 20–30 percentage points compared to actual accuracy<sup>[3]</sup>. Xiong et al. (2024) documented the same clustering around high round numbers and the same systematic overestimation<sup>[10]</sup>. The most comprehensive assessment came in January 2026: arXiv:2602.00279 concluded that verbalized confidence expressions are "systematically biased and poorly correlated with correctness"<sup>[2]</sup>.</p>

  <h3>The Root Cause: RLHF</h3>

  <p>This is not a prompt engineering problem. It is a training problem.</p>

  <p>The root cause sits in <strong>RLHF — Reinforcement Learning from Human Feedback</strong>. When human raters evaluate model outputs, confident-sounding answers score higher. Hedging gets penalized. "The answer is X" beats "The answer might be X, but I'm not sure." Over millions of training iterations, models learn a simple lesson: confidence gets rewarded<sup>[9]</sup>.</p>

  <p>Instruction tuning adds a second layer. The objective "be helpful" trains models to commit to answers rather than express uncertainty. "I don't know" is effectively trained out of the model's repertoire. And sycophancy — the tendency to agree with user premises even when wrong — provides a third compounding force<sup>[9]</sup>.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 2: Root Causes of LLM Overconfidence</p>
    <table class="exhibit-table">
      <tr>
        <th>Mechanism</th>
        <th>How It Creates Overconfidence</th>
        <th>Reversible?</th>
      </tr>
      <tr>
        <td>RLHF reward signal</td>
        <td>Confident answers score higher with human raters</td>
        <td>Requires new training objective</td>
      </tr>
      <tr>
        <td>Instruction tuning</td>
        <td>"Be helpful" = commit to answers, don't hedge</td>
        <td>Requires objective redesign</td>
      </tr>
      <tr>
        <td>Sycophancy</td>
        <td>Agree with user premise, express confidence in framing</td>
        <td>Active research area</td>
      </tr>
      <tr>
        <td>No calibration loss</td>
        <td>Unlike weather models, no training signal rewards accurate confidence</td>
        <td>Could be added; not standard</td>
      </tr>
    </table>
    <p class="exhibit-source">Source: Kadavath et al. (2022) [9], training dynamics literature</p>
  </div>

  <h3>The Counterexample</h3>

  <p>Base models before instruction tuning show reasonable token-level calibration<sup>[9]</sup>. This proves that calibration is not impossible for neural networks — it is specifically destroyed by the post-training process designed to make models conversational. This means the problem is solvable. It requires changing what we optimize for, not changing the architecture.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">An RLHF variant that preserves calibration through the instruction-tuning process. If a major lab ships a model with ECE < 0.05 after RLHF, the structural argument weakens significantly.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">Overconfidence is not a model defect. It is an emergent property of how we train models to be useful. Expecting prompt engineering to fix a training-level problem is like expecting better tires to fix a misaligned engine.</p>
  </div>
</div>

<!-- ========================================
     SECTION 6: MULTI-AGENT AMPLIFICATION
     ======================================== -->
<div class="page" id="multi-agent">
  <h2>6. Multi-Agent Amplification
    <span class="confidence-badge">Medium</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium)</span>

  <p><span class="key-insight">Adding agents to verify agents makes calibration worse, not better — unless the verification method is fundamentally different from "ask another model."</span></p>

  <h3>Why Multi-Agent Verification Fails</h3>

  <p>The intuition behind multi-agent systems is seductive: if one agent might be wrong, have another check its work. A "second opinion" should improve reliability. In medicine, law, and engineering, peer review catches errors. Why wouldn't the same logic apply to AI agents?</p>

  <p>Because <strong>AI agents share the same systematic biases</strong>.</p>

  <p>When Agent A (overconfident) passes its output to Agent B (also overconfident) for verification, three compounding effects occur:</p>

  <ol>
    <li><strong>Sycophancy:</strong> Agent B's prior is biased toward agreement with the input it receives. It is more likely to confirm than challenge.</li>
    <li><strong>Anchoring:</strong> Agent B sees Agent A's high confidence as evidence. A claim presented with "95% confidence" is harder to reject than one presented with "I'm not sure."</li>
    <li><strong>Compounding:</strong> Agent B reports even higher confidence on the now-"validated" result.</li>
  </ol>

  <p>In a chain of N agents, miscalibration does not cancel out. It compounds. If each agent independently has an overconfidence ratio of 0.84, a 3-agent verification chain where each confirms the previous approaches an effective overconfidence ratio of 1.0. The "second opinion" is not a second opinion at all — it is the same bias wearing a different name tag.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 3: Compound Miscalibration Model</p>
    <table class="exhibit-table">
      <tr>
        <th>Agent Chain</th>
        <th>Overconfidence Ratio (Modeled)</th>
        <th>False Certainty Level</th>
      </tr>
      <tr><td>1 agent</td><td>0.84</td><td>High</td></tr>
      <tr><td>2 agents (verify)</td><td>0.93</td><td>Very High</td></tr>
      <tr><td>3 agents (verify)</td><td>0.97</td><td>Near-Total</td></tr>
      <tr><td>5 agents (verify)</td><td>0.99</td><td>Effectively 100%</td></tr>
    </table>
    <p class="exhibit-source">Source: Theoretical model based on independent overconfidence assumption. Directional, not empirically validated for agent chains.</p>
  </div>

  <h3>The Exception: Sample Consistency</h3>

  <p><strong>Sample Consistency</strong><sup>[11]</sup> works precisely because it does not ask another model for a "second opinion." Instead, it samples the same model multiple times with temperature > 0 and measures disagreement. High agreement = justified confidence. Low agreement = genuine uncertainty. This is fundamentally different from "Agent B verifies Agent A" because it measures variance, not consensus.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">An empirical study showing that multi-agent verification chains with diverse base models actually reduce calibration error. If GPT-4 checking Claude checking Gemini produces well-calibrated outputs, the compound overconfidence argument fails.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If your agent architecture uses "Agent B checks Agent A" as a reliability mechanism, you likely have a false consensus machine, not a quality assurance system. Redesign for disagreement, not confirmation.</p>
  </div>
</div>

<!-- ========================================
     SECTION 7: COSTS
     ======================================== -->
<div class="page" id="costs">
  <h2>7. What Overconfidence Costs
    <span class="confidence-badge">High</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">The cost is not wrong answers — it is the erosion of human judgment when humans can no longer distinguish "the AI is actually sure" from "the AI always says it's sure."</span></p>

  <h3>Direct Costs</h3>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 4: Documented Costs of Miscalibrated Systems</p>
    <table class="exhibit-table">
      <tr>
        <th>Case</th>
        <th>What Happened</th>
        <th>Cost</th>
        <th>Calibration Link</th>
      </tr>
      <tr>
        <td>VW Cariad</td>
        <td>Software system overcommitted on delivery timelines</td>
        <td>$7.5B<sup>[7]</sup></td>
        <td>System confidence in timelines vs. reality</td>
      </tr>
      <tr>
        <td>Air Canada chatbot</td>
        <td>Hallucinated refund policy, presented with full confidence</td>
        <td>~$800 + legal precedent</td>
        <td>No uncertainty flagging on fabricated information</td>
      </tr>
      <tr>
        <td>Mata v. Avianca</td>
        <td>Lawyer filed ChatGPT-fabricated case citations</td>
        <td>$5K fine + career damage</td>
        <td>Model presented fake citations with zero hedging</td>
      </tr>
      <tr>
        <td>Healthcare alerts</td>
        <td>80–99% false positive rates in clinical alert systems</td>
        <td>14%+ increase in medical errors<sup>[6]</sup></td>
        <td>Poorly calibrated alert thresholds</td>
      </tr>
      <tr>
        <td>SOC alert fatigue</td>
        <td>67% of 4,484 daily security alerts ignored</td>
        <td>Unquantified breach exposure<sup>[5]</sup></td>
        <td>Overconfident threat detection</td>
      </tr>
    </table>
    <p class="exhibit-source">Sources: PMC6904899 [6], Vectra 2023 [5], VW financial reports [7]</p>
  </div>

  <h3>The Trust Erosion Spiral</h3>

  <p>The direct costs are not the real story. The real story is the <strong>trust erosion spiral</strong> — a five-phase pattern repeating across every domain where overconfident automation meets human oversight:</p>

  <p><strong>Phase 1: Overcommitment.</strong> The overconfident agent makes decisions. It states high confidence on every output. Most outputs are correct, so early trust is high.</p>

  <p><strong>Phase 2: Discovery.</strong> Humans notice errors — but the agent said "95% confident" on both the correct and incorrect outputs. The confidence signal becomes meaningless.</p>

  <p><strong>Phase 3: Alert fatigue.</strong> Humans begin ignoring agent outputs because they cannot distinguish real confidence from systematic overconfidence. In SOC environments, 67% of alerts are already ignored<sup>[5]</sup>. In healthcare, 80–99% of clinical alerts are false positives<sup>[6]</sup>.</p>

  <p><strong>Phase 4: Binary choice.</strong> The organization faces a lose-lose decision: abandon the AI system (wasting investment) or remove human oversight (creating unmonitored risk).</p>

  <p><strong>Phase 5: Catastrophe.</strong> An actual critical alert gets ignored because it looks identical to the thousands of false alarms before it.</p>

  <h3>The Cost Asymmetry</h3>

  <p>A Budget-CoCoA calibration check costs <strong>$0.005 per decision</strong><sup>[12][7]</sup>. At 1,000 checks per day, that is $135 per month. One prevented VW-scale failure pays for 55,555 years of calibration checks. The ratio between fix cost and failure cost is 1:1,500,000.</p>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">Evidence that humans maintain appropriate trust calibration with AI systems even without reliable confidence signals — i.e., that alert fatigue does not develop with overconfident AI. The aviation, medical, and SOC evidence makes this unlikely.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">The $0.005 calibration check is not an expense. It is insurance against trust collapse. Organizations spending millions on AI deployment but zero on calibration are building on sand.</p>
  </div>
</div>

<!-- ========================================
     SECTION 8: METHODS
     ======================================== -->
<div class="page" id="methods">
  <h2>8. Calibration Methods That Actually Work
    <span class="confidence-badge">High</span>
  </h2>
  <span class="confidence-line">(Confidence: High)</span>

  <p><span class="key-insight">The best calibration method for production AI agents is Sample Consistency — it is black-box, cheap, and does not require logit access.</span></p>

  <h3>The Five Methods</h3>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 5: Calibration Methods Comparison</p>
    <table class="exhibit-table">
      <tr>
        <th>Method</th>
        <th>How It Works</th>
        <th>Cost/Check</th>
        <th>Logit Access?</th>
        <th>Production Ready?</th>
      </tr>
      <tr>
        <td>Temperature Scaling<sup>[13]</sup></td>
        <td>Scalar applied to logits post-hoc</td>
        <td>Near-zero</td>
        <td>Yes</td>
        <td>Self-hosted only</td>
      </tr>
      <tr>
        <td>Conformal Prediction<sup>[14]</sup></td>
        <td>Prediction sets with coverage guarantees</td>
        <td>Near-zero</td>
        <td>No</td>
        <td>Emerging</td>
      </tr>
      <tr>
        <td>Sample Consistency<sup>[11]</sup></td>
        <td>N samples, measure agreement</td>
        <td>~$0.003 (3×)</td>
        <td>No</td>
        <td>Yes</td>
      </tr>
      <tr>
        <td>Hybrid CoCoA<sup>[12]</sup></td>
        <td>Consistency + verbalized confidence</td>
        <td>~$0.005 (3×)</td>
        <td>No</td>
        <td>Yes</td>
      </tr>
      <tr>
        <td>Selective Prediction</td>
        <td>Train to abstain when uncertain</td>
        <td>N/A (training)</td>
        <td>N/A</td>
        <td>No (requires retraining)</td>
      </tr>
    </table>
    <p class="exhibit-source">Sources: Guo et al. 2017 [13], Angelopoulos & Bates 2023 [14], Wang et al. 2022 [11], Hobelsberger et al. 2025 [12]</p>
  </div>

  <h3>Method Details</h3>

  <p><strong>Temperature Scaling</strong><sup>[13]</sup> is the gold standard in machine learning. A single scalar parameter, learned on a validation set, adjusts logits before the softmax layer. Simple, effective, no architecture change. But it requires access to logits — which API-based models (GPT-4, Claude, Gemini) do not expose.</p>

  <p><strong>Conformal Prediction</strong><sup>[14]</sup> takes a radically different approach. Instead of calibrating a point prediction, it produces prediction sets with guaranteed coverage probability. Instead of "the answer is X (95% sure)," it outputs "the answer is in {X, Y, Z} with 90% coverage guarantee." The guarantee is distribution-free and finite-sample. For high-stakes decisions (medical diagnosis, legal analysis), this trade-off is worth making.</p>

  <p><strong>Sample Consistency</strong><sup>[11]</sup> is the practical winner for most use cases. Sample the same model N times with temperature > 0. Measure agreement across samples. High agreement signals justified confidence; low agreement signals genuine uncertainty. It is black-box, works on any model, and requires no logit access.</p>

  <p><strong>Hybrid CoCoA</strong><sup>[12]</sup> combines Sample Consistency with verbalized confidence, weighting consistency higher. The Budget version using Haiku costs $0.005 per check. This is the current recommendation for production systems.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 6: Decision Framework — When to Use Which Method</p>
    <table class="exhibit-table">
      <tr>
        <th>Scenario</th>
        <th>Recommended Method</th>
        <th>Rationale</th>
      </tr>
      <tr>
        <td>Self-hosted model</td>
        <td>Temperature Scaling</td>
        <td>Gold standard, near-zero cost</td>
      </tr>
      <tr>
        <td>API-based agent, standard decisions</td>
        <td>Budget-CoCoA</td>
        <td>Best accuracy/cost ratio</td>
      </tr>
      <tr>
        <td>High-stakes decisions (medical, legal)</td>
        <td>Conformal Prediction</td>
        <td>Coverage guarantees</td>
      </tr>
      <tr>
        <td>Multi-agent orchestration</td>
        <td>Sample Consistency at each handoff</td>
        <td>Prevents compound overconfidence</td>
      </tr>
      <tr>
        <td>Cost-constrained, high-volume</td>
        <td>Sample Consistency (2× sample)</td>
        <td>Cheaper, still effective</td>
      </tr>
    </table>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">If next-generation models ship with well-calibrated verbalized confidence natively (ECE < 0.05 post-RLHF), external calibration layers become unnecessary. No evidence this is imminent.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">For $135/month (1,000 checks/day with Budget-CoCoA), you can add a calibration layer to your agent system. The technical barrier is near-zero. The only barrier is knowing this problem exists.</p>
  </div>
</div>

<!-- ========================================
     SECTION 9: HUMAN FACTOR
     ======================================== -->
<div class="page" id="human-factor">
  <h2>9. The Human Factor
    <span class="confidence-badge">Medium-High</span>
  </h2>
  <span class="confidence-line">(Confidence: Medium-High)</span>

  <p><span class="key-insight">The market selects for overconfidence — honest AI that says "I'm 60% sure" loses to overconfident AI that says "95% sure," even when the honest AI is more useful.</span></p>

  <h3>Behavioral Economics of AI Confidence</h3>

  <p><strong>Automation bias</strong> is well-documented across aviation, medicine, and security. Humans defer to automated systems even when their own judgment is better. The effect is stronger when the system expresses high confidence — "95% confident" triggers automation bias more than "60% confident"<sup>[6]</sup>.</p>

  <p><strong>Confidence anchoring</strong> compounds the problem. When an AI system says "95% confident," humans anchor on that number. Even if they learn the system is poorly calibrated, the anchor persists.</p>

  <p><strong>Asymmetric trust updating</strong> provides the final piece. Humans update trust upward faster than downward. A few correct, confident predictions build trust that survives many incorrect ones.</p>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 7: The Confidence-Adoption Paradox</p>
    <table class="exhibit-table">
      <tr>
        <th>AI System A (Calibrated)</th>
        <th>AI System B (Overconfident)</th>
      </tr>
      <tr>
        <td>Says "60% confident" when 60% accurate</td>
        <td>Says "95% confident" when 60% accurate</td>
      </tr>
      <tr>
        <td>Users perceive as "uncertain"</td>
        <td>Users perceive as "reliable"</td>
      </tr>
      <tr>
        <td>Lower adoption rates</td>
        <td>Higher adoption rates</td>
      </tr>
      <tr>
        <td>Fewer trust failures long-term</td>
        <td>More trust failures long-term</td>
      </tr>
      <tr>
        <td>Better for the organization</td>
        <td>Feels better for the buyer</td>
      </tr>
    </table>
  </div>

  <div class="callout invalidation">
    <p class="callout-label">What Would Invalidate This?</p>
    <p class="callout-body">Evidence that enterprise buyers prefer calibrated AI systems over overconfident ones without needing to experience a failure first. If adoption data shows calibrated products winning market share, the pessimistic market dynamics argument collapses.</p>
  </div>

  <div class="callout sowhat">
    <p class="callout-label">So What?</p>
    <p class="callout-body">If you are building a calibrated AI product, you need a deliberate positioning strategy. "We're honest about uncertainty" must be framed as a premium capability, not a weakness.</p>
  </div>
</div>

<!-- ========================================
     SECTION 10: RECOMMENDATIONS
     ======================================== -->
<div class="page" id="recommendations">
  <h2>10. Recommendations</h2>

  <p><span class="key-insight">Calibration is not a model problem — it is an infrastructure problem. It belongs in the orchestration layer, not the model layer.</span></p>

  <p>Based on the evidence in this report, five concrete actions for any organization deploying AI agents:</p>

  <ol>
    <li><strong>Implement Budget-CoCoA at the orchestration level.</strong> Cost: $135/month for 1,000 checks/day. This is the single highest-leverage investment in agent reliability available today<sup>[12]</sup>.</li>
    <li><strong>Never trust verbalized confidence alone.</strong> Any system that surfaces a model's self-reported confidence to end users without external calibration is misleading those users. The 20–30 percentage point upward bias is systematic<sup>[3]</sup>.</li>
    <li><strong>Use Conformal Prediction for high-stakes decisions.</strong> When the cost of error is high (medical, legal, financial), prediction sets with coverage guarantees are superior to point estimates<sup>[14]</sup>.</li>
    <li><strong>Design multi-agent systems for disagreement, not consensus.</strong> The default pattern of "Agent B verifies Agent A" creates false consensus. Replace it with Sample Consistency<sup>[11]</sup>.</li>
    <li><strong>Present calibrated uncertainty as a trust differentiator.</strong> In product positioning, "we tell you when we don't know" is a feature.</li>
  </ol>

  <div class="exhibit">
    <p class="exhibit-label">Exhibit 8: Implementation Priority Matrix</p>
    <table class="exhibit-table">
      <tr>
        <th>Action</th>
        <th>Cost</th>
        <th>Effort</th>
        <th>Impact</th>
        <th>Priority</th>
      </tr>
      <tr>
        <td>Budget-CoCoA integration</td>
        <td>$135/month</td>
        <td>1–2 days</td>
        <td>High — catches most overconfidence</td>
        <td>Do first</td>
      </tr>
      <tr>
        <td>Remove raw VCE from user-facing outputs</td>
        <td>$0</td>
        <td>1 day</td>
        <td>High — stops misleading users</td>
        <td>Do first</td>
      </tr>
      <tr>
        <td>Conformal Prediction for critical paths</td>
        <td>Low</td>
        <td>1 week</td>
        <td>Very High for high-stakes</td>
        <td>Do second</td>
      </tr>
      <tr>
        <td>Redesign multi-agent verification</td>
        <td>$0</td>
        <td>1–2 weeks</td>
        <td>Medium-High</td>
        <td>Do third</td>
      </tr>
      <tr>
        <td>Calibration-as-feature positioning</td>
        <td>$0</td>
        <td>Ongoing</td>
        <td>Long-term advantage</td>
        <td>Start now</td>
      </tr>
    </table>
  </div>
</div>

<!-- ========================================
     TRANSPARENCY NOTE
     ======================================== -->
<div class="page" id="transparency">
  <h2>11. Transparency Note</h2>

  <p class="transparency-intro">This section documents the research process, sources, confidence calibration, and known limitations of this report. It serves as the full methodology disclosure required for evidence-based research.</p>

  <table class="transparency-table">
    <tr>
      <td>Overall Confidence</td>
      <td>72% — High confidence in documented overconfidence patterns (84% figure, 20–30pp bias, RLHF mechanism). Medium confidence in multi-agent amplification (theoretical model from well-evidenced components). Medium-High confidence in cost extrapolations from SOC/healthcare to AI agents (analogical reasoning).</td>
    </tr>
    <tr>
      <td>Sources</td>
      <td>14 total — 8 peer-reviewed or preprint papers (calibration metrics, RLHF dynamics, LLM confidence), 6 industry reports (Vectra SOC survey, VW financial disclosures, healthcare alert fatigue meta-review). All sources cited in References section.</td>
    </tr>
    <tr>
      <td>Strongest Evidence</td>
      <td>The 84% overconfidence rate across 9 models and 351 scenarios [1] — peer-reviewed, multi-model, multi-scenario study. RLHF destroying calibration [9] — well-documented training dynamics. Sample Consistency effectiveness [11] — empirically validated.</td>
    </tr>
    <tr>
      <td>Weakest Point</td>
      <td>Multi-agent amplification (Section 6) — theoretical model built from well-evidenced components (sycophancy, anchoring, overconfidence), but the compound effect itself lacks direct empirical validation. The 84% figure tested in clinical domain only — cross-domain replication pending.</td>
    </tr>
    <tr>
      <td>What Would Invalidate</td>
      <td>A large-scale study demonstrating that 2026-generation models have resolved RLHF-induced overconfidence through training improvements, achieving ECE < 0.05 on verbalized confidence across domains. As of February 2026, no evidence this has occurred.</td>
    </tr>
    <tr>
      <td>Methodology</td>
      <td>Multi-agent research pipeline — structured literature review (targeting calibration metrics, RLHF dynamics, multi-agent systems), source cross-referencing, gap analysis, claim verification with explicit confidence levels. 14 primary sources reviewed. All claims registered in Claim Register with evidence basis and invalidation conditions.</td>
    </tr>
    <tr>
      <td>Known Gaps</td>
      <td>No head-to-head ECE comparison across GPT-4, Claude, and Gemini with identical test sets. Multi-agent compound miscalibration is modeled, not measured empirically. Alert fatigue data extrapolated from SOC/healthcare to AI agents (analogical, not direct). Cost calculations use current API pricing (February 2026) which may change.</td>
    </tr>
    <tr>
      <td>System Disclosure</td>
      <td>This report was created with a multi-agent research system. Research briefs were produced independently, then synthesized to identify calibration patterns and gaps. The writing process followed a structured template with mandatory claim verification and source documentation.</td>
    </tr>
  </table>
</div>

<!-- ========================================
     CLAIM REGISTER
     ======================================== -->
<div class="page" id="claim-register">
  <h2>12. Claim Register</h2>

  <div class="exhibit">
    <table class="exhibit-table">
      <tr>
        <th>#</th>
        <th>Claim</th>
        <th>Value</th>
        <th>Source</th>
        <th>Confidence</th>
        <th>Used In</th>
      </tr>
      <tr>
        <td>1</td>
        <td>LLMs overconfident in 84% of scenarios</td>
        <td>84%, n=9 models, 351 scenarios</td>
        <td>PMC/12249208 [1]</td>
        <td>High</td>
        <td>Sec. 5</td>
      </tr>
      <tr>
        <td>2</td>
        <td>VCE poorly correlated with correctness</td>
        <td>r ≈ 0.3–0.5</td>
        <td>arXiv:2602.00279 [2]</td>
        <td>High</td>
        <td>Sec. 4</td>
      </tr>
      <tr>
        <td>3</td>
        <td>VCE biased upward by 20–30pp</td>
        <td>20–30pp</td>
        <td>Tian et al. 2023 [3]</td>
        <td>Medium-High</td>
        <td>Sec. 4, 5</td>
      </tr>
      <tr>
        <td>4</td>
        <td>Instruction tuning worsens calibration</td>
        <td>Directional</td>
        <td>Kadavath et al. 2022 [9]</td>
        <td>High</td>
        <td>Sec. 5</td>
      </tr>
      <tr>
        <td>5</td>
        <td>SOC alerts: 67% ignored</td>
        <td>67%, n=2,000 analysts</td>
        <td>Vectra 2023 [5]</td>
        <td>High</td>
        <td>Sec. 7</td>
      </tr>
      <tr>
        <td>6</td>
        <td>Healthcare false positives: 80–99%</td>
        <td>80–99%</td>
        <td>PMC6904899 [6]</td>
        <td>High</td>
        <td>Sec. 7</td>
      </tr>
      <tr>
        <td>7</td>
        <td>Budget-CoCoA: $0.005/check</td>
        <td>$0.005</td>
        <td>Hobelsberger et al. + pricing [12]</td>
        <td>High</td>
        <td>Sec. 7, 8</td>
      </tr>
      <tr>
        <td>8</td>
        <td>Multi-agent amplification compounds overconfidence</td>
        <td>Theoretical + directional</td>
        <td>Synthesized from [1][2][9]</td>
        <td>Medium</td>
        <td>Sec. 6</td>
      </tr>
      <tr>
        <td>9</td>
        <td>RLHF selects for overconfidence</td>
        <td>Mechanistic argument</td>
        <td>Training dynamics [9]</td>
        <td>Medium-High</td>
        <td>Sec. 5</td>
      </tr>
      <tr>
        <td>10</td>
        <td>Temperature scaling requires logit access</td>
        <td>Technical fact</td>
        <td>Guo et al. 2017 [13]</td>
        <td>High</td>
        <td>Sec. 8</td>
      </tr>
      <tr>
        <td>11</td>
        <td>VW Cariad losses</td>
        <td>$7.5B</td>
        <td>VW disclosures [7]</td>
        <td>High</td>
        <td>Sec. 7</td>
      </tr>
      <tr>
        <td>12</td>
        <td>Sample Consistency prevents compound overconfidence</td>
        <td>Methodological</td>
        <td>Wang et al. 2022 [11]</td>
        <td>High</td>
        <td>Sec. 6, 8</td>
      </tr>
    </table>
  </div>

  <p style="margin-top: 24px; font-size: 0.85rem; color: #666;"><strong>Top 5 Claims — Invalidation Criteria:</strong></p>

  <ol style="margin-left: 20px; font-size: 0.85rem; color: #555; line-height: 1.6;">
    <li><strong>84% overconfidence:</strong> Would be invalidated if large-scale replication across domains showed significantly lower rates or if clinical-domain specificity was demonstrated.</li>
    <li><strong>20–30pp upward bias:</strong> Would be invalidated if next-generation models shipped with well-calibrated VCE (validated via ECE < 0.05).</li>
    <li><strong>RLHF destroys calibration:</strong> Would be invalidated if an RLHF variant preserved calibration through instruction-tuning.</li>
    <li><strong>Multi-agent amplification:</strong> Would be invalidated if empirical study showed diverse-model verification chains reduce calibration error.</li>
    <li><strong>$0.005 calibration cost:</strong> Would be invalidated if API pricing changes or if Budget-CoCoA replication fails to achieve reported effectiveness.</li>
  </ol>
</div>

<!-- ========================================
     REFERENCES
     ======================================== -->
<div class="page" id="references">
  <h2>13. References</h2>

  <p class="reference-entry">[1] PMC/12249208 (2024). "Overconfidence in LLM Clinical Decision-Making." Peer-reviewed study, 9 models, 351 scenarios.</p>

  <p class="reference-entry">[2] arXiv:2602.00279 (January 2026). "Verbalized Confidence Expressions in Large Language Models." Preprint.</p>

  <p class="reference-entry">[3] Tian, K. et al. (2023). "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models." Preprint.</p>

  <p class="reference-entry">[4] arXiv:2503.12188 (2025). "Hijacking Multi-Agent Systems: Adversarial Manipulation in Collaborative AI." Preprint.</p>

  <p class="reference-entry">[5] Vectra (2023). "State of Threat Detection Report." Industry survey, n=2,000 SOC analysts.</p>

  <p class="reference-entry">[6] PMC6904899. "Clinical Decision Support Alert Fatigue: A Meta-Review." Peer-reviewed.</p>

  <p class="reference-entry">[7] VW Cariad financial reports; public filings. $7.5B in documented losses.</p>

  <p class="reference-entry">[8] Naeini, M.P. et al. (2015). "Obtaining Well Calibrated Probabilities Using Bayesian Binning into Quantiles." AAAI 2015.</p>

  <p class="reference-entry">[9] Kadavath, S. et al. (2022). "Language Models (Mostly) Know What They Know." Anthropic. Preprint.</p>

  <p class="reference-entry">[10] Xiong, M. et al. (2024). "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs." Peer-reviewed.</p>

  <p class="reference-entry">[11] Wang, X. et al. (2022). "Self-Consistency Improves Chain of Thought Reasoning in Language Models." Peer-reviewed.</p>

  <p class="reference-entry">[12] Hobelsberger, M. et al. (2025). "CoCoA: Confidence and Consistency Aggregation for Calibrated LLM Outputs." arXiv:2510.20460. Preprint.</p>

  <p class="reference-entry">[13] Guo, C. et al. (2017). "On Calibration of Modern Neural Networks." ICML 2017. Peer-reviewed.</p>

  <p class="reference-entry">[14] Angelopoulos, A.N. & Bates, S. (2023). "Conformal Prediction: A Gentle Introduction." Tutorial/Survey.</p>

  <p class="reference-entry" style="margin-top: 24px; padding-top: 16px; border-top: 1px solid #eee;"><strong>Citation:</strong> Ainary Research. (2026). <em>The Calibration Gap: Why 84% of AI Agents Are Overconfident and What It Costs.</em> AR-009.</p>

  <!-- AUTHOR BIO -->
  <div class="author-section">
    <p class="author-label">About the Author</p>
    <p class="author-bio">Florian Ziesche is the founder of Ainary Ventures, where AI does 80% of the research and humans do the 20% that matters. Before Ainary, he was CEO of 36ZERO Vision and advised startups and SMEs on AI strategy and due diligence. His conviction: HUMAN × AI = LEVERAGE. This report is the proof.</p>
  </div>
</div>

<!-- ========================================
     BACK COVER
     ======================================== -->
<div class="back-cover">
  <div>
    <div style="display: flex; align-items: center; gap: 8px; justify-content: center; margin-bottom: 24px;">
      <span class="gold-punkt">●</span>
      <span style="font-size: 0.85rem; font-weight: 500; letter-spacing: 0.02em;">Ainary</span>
    </div>
    
    <p class="back-cover-services">AI Strategy · Published Research · Daily Intelligence</p>
    
    <p class="back-cover-cta">Contact · Feedback</p>
    
    <p class="back-cover-contact">
      florian@ainaryventures.com<br>
      ainaryventures.com
    </p>
    
    <p style="font-size: 0.7rem; color: #aaa; margin-top: 48px;">© 2026 Ainary Ventures</p>
  </div>
</div>

</body>
</html>